{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "7b076bb5",
   "metadata": {},
   "outputs": [],
   "source": "# Lab 5, Module 2: Sentence Embeddings & Semantic Search\n\n**Estimated time:** 25 minutes\n\n---\n\n## From Words to Sentences: The Next Level\n\nIn Module 0, you embedded individual **words** using co-occurrence patterns.  \nIn Module 1, you explored **word-level relationships** using GloVe vectors.\n\n**Now let's scale up again.**\n\nIn this module, you'll embed **entire sentences** and build a semantic search engine. This is how modern AI systems like ChatGPT, Google, and RAG (Retrieval-Augmented Generation) systems find relevant information.\n\n### What You'll Learn\n\n1. **Sentence embeddings** - How to represent whole sentences as vectors\n2. **Semantic similarity** - Measuring how close two sentences are in meaning\n3. **Semantic search** - Finding documents by meaning, not keywords\n4. **Real-world applications** - How this powers modern AI systems\n\n### The Model: all-MiniLM-L6-v2\n\nYou'll use a **SentenceTransformer** model called **all-MiniLM-L6-v2**:\n- Trained specifically for sentence embeddings (not just words!)\n- Produces **384-dimensional vectors** (vs GloVe's 50 dimensions)\n- Small and fast, but powerful for semantic similarity tasks\n- Used in production systems for document retrieval\n\n**The key difference from word embeddings:** This model understands entire sentences as unified concepts, not just bags of words.\n\nLet's get started!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0116fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  Module 2 â€” Setup & Why LLMs Need Embeddings\n",
    "#  DATA 1010 â€“ Artificial Intelligence in Action\n",
    "# ============================================================\n",
    "\n",
    "# This cell:\n",
    "#   â€¢ Installs and loads a free, open-source embedding model\n",
    "#   â€¢ Asks for a group code (like Labs 1â€“4)\n",
    "#   â€¢ Generates a test embedding for a sample sentence\n",
    "#   â€¢ Shows the embedding dimensionality\n",
    "#   â€¢ Confirms the Colab environment is working\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Install requirements\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    !pip install -q sentence-transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Group Code\n",
    "# -----------------------------\n",
    "group_code = input(\"Enter your group code (an integer): \")\n",
    "print(f\"Group code set to: {group_code}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load the embedding model\n",
    "# -----------------------------\n",
    "print(\"\\nLoading embedding model (all-MiniLM-L6-v2)...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Generate a test embedding\n",
    "# -----------------------------\n",
    "test_sentence = \"Astronomy is the study of stars and galaxies.\"\n",
    "\n",
    "print(\"\\nGenerating a test embedding...\")\n",
    "embedding = model.encode(test_sentence)\n",
    "\n",
    "print(\"\\nSample sentence:\")\n",
    "print(\"   \", test_sentence)\n",
    "print(\"\\nEmbedding generated!\")\n",
    "print(f\"Embedding vector length: {len(embedding)} dimensions\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Finished\n",
    "# -----------------------------\n",
    "print(\"\\nSetup complete. You are ready for Module 2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95053f",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n#  Module 2 â€” Activity 1: Sentence Embeddings Visualization\n#  DATA 1010 â€“ Artificial Intelligence in Action\n# ============================================================\n\n# This cell:\n#   â€¢ Defines a small set of sentences\n#   â€¢ Uses the embedding model to encode them\n#   â€¢ Reduces embeddings to 2D using PCA\n#   â€¢ Plots the sentences as points in a 2D \"meaning space\"\n\n\n\n# -----------------------------\n# 1. Define the sentences\n# -----------------------------\nsentences = [\n    \"The cat sat on the mat.\",\n    \"Cats are great pets.\",\n    \"Stars fuse hydrogen into helium.\",\n    \"Galaxies contain billions of stars.\",\n    \"Neural networks learn patterns.\"\n]\n\nprint(\"Sentences to embed:\\n\")\nfor i, s in enumerate(sentences, start=1):\n    print(f\"{i}. {s}\")\nprint(\"\\nEncoding sentences into embeddings...\")\n\n# -----------------------------\n# 2. Encode sentences\n# -----------------------------\n# Uses the SentenceTransformer model loaded in setup\nembeddings = model.encode(sentences)\nembeddings = np.array(embeddings)\n\nprint(\"Done!\")\nprint(f\"Embedding array shape: {embeddings.shape}  (sentences x dimensions)\")\nprint(f\"\\nNotice: Each sentence becomes a {embeddings.shape[1]}-dimensional vector!\")\nprint(\"(Compare to GloVe's 50 dimensions for individual words)\")\n\n# -----------------------------\n# 3. Reduce to 2D with PCA\n# -----------------------------\npca = PCA(n_components=2)\nembeddings_2d = pca.fit_transform(embeddings)\n\nprint(\"\\nPCA complete. Showing 2D coordinates for each sentence:\")\nfor i, (x, y) in enumerate(embeddings_2d, start=1):\n    print(f\"{i}. ({x:.3f}, {y:.3f})\")\n\n# -----------------------------\n# 4. Plot the 2D embedding space\n# -----------------------------\nplt.figure(figsize=(6, 6))\nplt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n\nfor i, (x, y) in enumerate(embeddings_2d):\n    label = f\"{i+1}\"\n    plt.text(x + 0.02, y + 0.02, label, fontsize=12)\n\nplt.title(\"2D PCA Projection of Sentence Embeddings\")\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.axhline(0, linewidth=0.5, color='gray', alpha=0.5)\nplt.axvline(0, linewidth=0.5, color='gray', alpha=0.5)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Each point is one sentence. The numbers match the list above.\")\nprint(\"=\"*50)\nprint(\"\\nObserve:\")\nprint(\"  â€¢ Which sentences are closest together?\")\nprint(\"  â€¢ Do the clusters match your intuition about meaning?\")\nprint(\"  â€¢ Why might some sentences sit apart from the others?\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "e47vkf4zfv",
   "source": "---\n\n## Activity 1: Visualizing Sentence Embeddings\n\nNow that the model is loaded, let's embed a few sentences and see how they cluster in meaning space.\n\n### ðŸ“ Question 16 (Prediction)\n\nBefore running the code below, look at these 5 sentences:\n\n1. \"The cat sat on the mat.\"\n2. \"Cats are great pets.\"\n3. \"Stars fuse hydrogen into helium.\"\n4. \"Galaxies contain billions of stars.\"\n5. \"Neural networks learn patterns.\"\n\n**Q16.** Which two sentences do you predict will be closest together in the embedding space? Why?\n\n*Think about: Which sentences have the most similar meanings?*\n\n**Write your prediction in the answer sheet, then run the code to check!**\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e25e31",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n#  Module 2 â€” Activity 2: Cosine Similarity Exploration\n#  DATA 1010 â€“ Artificial Intelligence in Action\n# ============================================================\n\n# This cell:\n#   â€¢ Lets you choose any two sentences from Activity 1\n#   â€¢ Computes cosine similarity + distance\n#   â€¢ Highlights the two chosen points on the PCA plot\n#   â€¢ Helps you compare intuition vs measurable similarity\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport ipywidgets as widgets\n\n# ---------------------------------------\n# 1. Dropdown menus for sentence selection\n# ---------------------------------------\nsentence_options = [f\"{i+1}. {sentences[i]}\" for i in range(len(sentences))]\n\ndropdown1 = widgets.Dropdown(\n    options=sentence_options,\n    description='Sentence 1:',\n    style={'description_width': 'initial'},\n    value=sentence_options[0]\n)\n\ndropdown2 = widgets.Dropdown(\n    options=sentence_options,\n    description='Sentence 2:',\n    style={'description_width': 'initial'},\n    value=sentence_options[1]\n)\n\ndisplay(dropdown1)\ndisplay(dropdown2)\n\n# ---------------------------------------\n# 2. Button to run the comparison\n# ---------------------------------------\nbutton = widgets.Button(description=\"Compute Similarity\", button_style='info')\noutput = widgets.Output()\ndisplay(button, output)\n\n# ---------------------------------------\n# 3. Callback function for button press\n# ---------------------------------------\ndef on_button_clicked(b):\n    with output:\n        output.clear_output()\n\n        # Extract indices\n        idx1 = int(dropdown1.value.split(\".\")[0]) - 1\n        idx2 = int(dropdown2.value.split(\".\")[0]) - 1\n        \n        emb1 = embeddings[idx1].reshape(1, -1)\n        emb2 = embeddings[idx2].reshape(1, -1)\n        \n        sim = cosine_similarity(emb1, emb2)[0][0]\n        dist = 1 - sim\n        \n        print(\"===============================================\")\n        print(\"Sentence 1:\")\n        print(\"  \", sentences[idx1], \"\\n\")\n        print(\"Sentence 2:\")\n        print(\"  \", sentences[idx2])\n        print(\"===============================================\\n\")\n        \n        print(f\"Cosine Similarity: {sim:.4f}\")\n        print(f\"Cosine Distance:   {dist:.4f}\\n\")\n        \n        # Interpretation\n        if sim > 0.8:\n            print(\"Interpretation: Very similar meaning\")\n        elif sim > 0.6:\n            print(\"Interpretation: Related meanings\")\n        elif sim > 0.4:\n            print(\"Interpretation: Somewhat related\")\n        else:\n            print(\"Interpretation: Different meanings\")\n        \n        # ---------------------------------------\n        # Visualize the two points on the PCA scatterplot\n        # ---------------------------------------\n        plt.figure(figsize=(6, 6))\n        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], color='gray', alpha=0.6)\n        \n        # Highlight selected points\n        plt.scatter(embeddings_2d[idx1, 0], embeddings_2d[idx1, 1], color='red', s=120, label=\"Sentence 1\", zorder=5)\n        plt.scatter(embeddings_2d[idx2, 0], embeddings_2d[idx2, 1], color='blue', s=120, label=\"Sentence 2\", zorder=5)\n        \n        # Labels for all points\n        for i, (x, y) in enumerate(embeddings_2d):\n            plt.text(x + 0.02, y + 0.02, str(i+1), fontsize=10)\n        \n        plt.title(\"PCA Visualization of Selected Sentences\")\n        plt.xlabel(\"PCA Component 1\")\n        plt.ylabel(\"PCA Component 2\")\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\nbutton.on_click(on_button_clicked)\n\nprint(\"\\nSelect two sentences and click 'Compute Similarity' to compare them!\")"
  },
  {
   "cell_type": "markdown",
   "id": "nlujv653qf9",
   "source": "---\n\n## ðŸ“ Questions 17-18 (Observation & Analysis)\n\n**Q17.** After viewing the PCA visualization above, which sentences clustered together? Was your prediction from Q16 correct?\n\n**Q18.** Why is \"Neural networks learn patterns\" isolated from the other sentences in the PCA plot?\n\n*Think about: What topics do the other sentences share? What makes the neural networks sentence different?*\n\n*Record your answers in the answer sheet.*\n\n---\n\n## Activity 2: Exploring Cosine Similarity\n\nNow let's measure exactly how similar different sentence pairs are using cosine similarity.\n\n**Interpretation guide for cosine similarity:**\n- **> 0.8:** Very similar meaning\n- **0.6 - 0.8:** Related meanings\n- **0.4 - 0.6:** Somewhat related\n- **< 0.4:** Different meanings\n\nUse the dropdown menus below to compare any two sentences!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b75c0",
   "metadata": {},
   "outputs": [],
   "source": "#@title ### ðŸ”Ž Activity 3: Semantic Search Mini-Application\n\n# # Module 3 â€” Semantic Search Mini-Application\n# In this activity, you'll build a tiny **semantic search engine** using the same\n# embedding model from earlier in the lab.\n#\n# Instead of searching by keywords, you'll search by **meaning**:\n#\n# 1. Embed a small corpus of mixed-topic sentences.\n# 2. Embed a **user query** (in natural language).\n# 3. Compute cosine similarities between the query and all corpus sentences.\n# 4. Show the **top 3 most similar** sentences.\n# 5. Visualize the query in the same 2D PCA space as the corpus.\n#\n# Try queries like:\n# - \"objects that orbit the sun\"\n# - \"examples of nuclear fusion\"\n# - \"animals people keep at home\"\n# - \"how machines learn patterns\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ------------------------------------------------\n# 1. Define a small mixed-topic corpus\n# ------------------------------------------------\ncorpus = [\n    # Astronomy-related\n    \"The Earth orbits the Sun once every year.\",\n    \"Stars fuse hydrogen into helium in their cores.\",\n    \"Galaxies contain billions of stars.\",\n    \"The Moon causes tides in the oceans.\",\n    \"Telescopes help astronomers observe distant galaxies.\",\n\n    # Pets / animals\n    \"Cats are popular pets that like to nap.\",\n    \"Dogs are loyal animals that enjoy walks.\",\n    \"Many people keep fish in aquariums at home.\",\n    \"Birds can be trained to mimic human speech.\",\n    \"Hamsters often run on wheels in their cages.\",\n\n    # Machine learning / AI\n    \"Neural networks learn patterns from data.\",\n    \"Machine learning models can recognize images.\",\n    \"Large language models generate human-like text.\",\n    \"Training a model requires lots of examples.\",\n    \"Embeddings represent meaning as high-dimensional vectors.\",\n\n    # Miscellaneous\n    \"Cooking at home can be fun and relaxing.\",\n    \"Music concerts bring people together.\",\n    \"Running is a good form of exercise.\",\n    \"Libraries are quiet places to read and study.\",\n    \"Video games can be played with friends online.\"\n]\n\nprint(\"Our corpus has\", len(corpus), \"sentences.\")\nprint(\"Topics: Astronomy, Pets/Animals, Machine Learning, Miscellaneous\\n\")\n\n# ------------------------------------------------\n# 2. Embed the corpus\n# ------------------------------------------------\nprint(\"Encoding corpus sentences into embeddings...\")\ncorpus_embeddings = model.encode(corpus)\ncorpus_embeddings = np.array(corpus_embeddings)\nprint(\"Done. Shape:\", corpus_embeddings.shape, \"(sentences x dimensions)\")\n\n# ------------------------------------------------\n# 3. Fit PCA on the corpus for 2D visualization\n# ------------------------------------------------\npca_corpus = PCA(n_components=2)\ncorpus_2d = pca_corpus.fit_transform(corpus_embeddings)\n\n# ------------------------------------------------\n# 4. Get a user query and embed it\n# ------------------------------------------------\nprint(\"\\nEnter a natural-language query to search the corpus by meaning.\")\nprint(\"Example queries:\")\nprint(\"  â€¢ objects that orbit the sun\")\nprint(\"  â€¢ examples of nuclear fusion\")\nprint(\"  â€¢ animals people keep at home\")\nprint(\"  â€¢ how machines learn patterns\\n\")\n\nquery = input(\"Type your query here: \").strip()\nif not query:\n    query = \"objects that orbit the sun\"\n    print(f\"(No input detected. Using default query: '{query}')\")\n\nprint(\"\\nEmbedding your query...\")\nquery_embedding = model.encode([query])\nquery_2d = pca_corpus.transform(query_embedding)  # project into same PCA space\n\n# ------------------------------------------------\n# 5. Compute cosine similarity and get top 3 results\n# ------------------------------------------------\nsims = cosine_similarity(query_embedding, corpus_embeddings)[0]  # shape: (N,)\ntop_indices = np.argsort(sims)[::-1][:3]  # indices of top 3\n\nprint(\"\\n\" + \"=\"*60)\nprint(\" SEMANTIC SEARCH RESULTS\")\nprint(\"=\"*60)\nprint(\"\\nYour query:\")\nprint(\"  \", query, \"\\n\")\nprint(\"Top 3 most similar sentences in the corpus:\\n\")\n\nfor rank, idx in enumerate(top_indices, start=1):\n    print(f\"{rank}. [Corpus sentence {idx}]  (similarity = {sims[idx]:.4f})\")\n    print(\"   \", corpus[idx], \"\\n\")\n\n# ------------------------------------------------\n# 6. Visualize corpus + query in PCA space\n# ------------------------------------------------\nplt.figure(figsize=(8, 8))\n\n# Plot corpus points\nplt.scatter(corpus_2d[:, 0], corpus_2d[:, 1], alpha=0.5, s=50, label=\"Corpus sentences\")\n\n# Highlight top 3\nplt.scatter(corpus_2d[top_indices, 0], corpus_2d[top_indices, 1],\n            color=\"orange\", s=150, label=\"Top 3 matches\", zorder=4)\n\n# Plot query\nplt.scatter(query_2d[0, 0], query_2d[0, 1], color=\"red\", s=200, marker='*', \n            label=\"Your Query\", zorder=5)\n\n# Label corpus points (indices)\nfor i, (x, y) in enumerate(corpus_2d):\n    plt.text(x + 0.01, y + 0.01, str(i), fontsize=8, alpha=0.7)\n\nplt.title(\"Semantic Search: Corpus + Query in 2D PCA Space\")\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.axhline(0, linewidth=0.5, color='gray', alpha=0.3)\nplt.axvline(0, linewidth=0.5, color='gray', alpha=0.3)\nplt.grid(True, alpha=0.3)\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Visualization Guide:\")\nprint(\"  â€¢ Gray dots = corpus sentences (labeled by index)\")\nprint(\"  â€¢ Orange dots = your top 3 matches\")\nprint(\"  â€¢ Red star = your query\")\nprint(\"=\"*60)\nprint(\"\\nNotice how the query (red star) is close to the orange matches!\")\nprint(\"This is semantic search in action: finding meaning, not keywords.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "8odfsxsit6e",
   "source": "---\n\n## ðŸ“ Questions 21-24 (Application & Synthesis)\n\n**Q21.** In the semantic search activity above, what query did you test? What were the top 3 results?\n\n*Write down your query and the 3 sentences it returned.*\n\n**Q22.** Do the top 3 results make sense for your query? Would keyword search have found these sentences?\n\n*Think about: Did your query use the exact same words as the results, or did it find them by meaning?*\n\n**Q23.** How is semantic search different from searching with Ctrl+F (keyword search)? Give a specific example from your results.\n\n**Q24.** Reflecting on all three modules: How do embeddings allow AI systems like ChatGPT to \"understand\" the meaning of text rather than just matching keywords?\n\n*Think about the journey:*\n- *Module 0: Co-occurrence captures context*\n- *Module 1: Word relationships preserved as geometry*\n- *Module 2: Sentences embedded in meaning space*\n- *How does this all come together for LLMs?*\n\n*Record your answers in the answer sheet.*\n\n---\n\n## âœ… Module 2 Complete! Lab 5 Finished!\n\nCongratulations! You've completed the full journey through embeddings:\n\n### What You've Learned\n\n**Module 0:**\n- Built a tiny embedding system from scratch\n- Learned how co-occurrence creates meaning vectors\n- Visualized word clusters with PCA\n\n**Module 1:**\n- Explored professional GloVe embeddings (6 billion tokens!)\n- Discovered dimensions capture abstract concepts\n- Used vector arithmetic to solve analogies\n\n**Module 2:**\n- Embedded entire sentences (not just words!)\n- Measured semantic similarity with cosine distance\n- Built a semantic search engine by meaning\n\n### The Big Picture: Why This Matters\n\n**Embeddings are the foundation of modern AI:**\n\n1. **Search & Retrieval** - Google, Bing, and specialized search use semantic embeddings\n2. **RAG Systems** - ChatGPT and Claude use embeddings to find relevant context from documents\n3. **Recommendation** - Netflix, Spotify, Amazon use embeddings to find similar content\n4. **Translation** - Translating between languages by mapping to shared embedding space\n5. **Classification** - Spam detection, sentiment analysis, topic categorization\n6. **Clustering** - Grouping similar documents, detecting duplicates\n7. **Question Answering** - Finding answers in knowledge bases\n\n**The key insight:** By representing language as geometry, AI systems can compute with meaning instead of just matching text patterns.\n\n### Optional: Module 3 - RAG in Practice\n\nIf your instructor assigns it, **Module 3** explores Retrieval-Augmented Generation (RAG) using lmnotebook. You'll see how semantic search integrates with LLMs to create systems that can answer questions using retrieved context.\n\n### What's Next?\n\nYou now understand the representation layer that powers modern NLP. Future topics might include:\n- **Attention mechanisms** - How transformers weigh different parts of text\n- **Transfer learning** - Fine-tuning pre-trained models for specific tasks\n- **Multimodal embeddings** - Representing images, audio, and text in the same space\n\n**Excellent work!** You've gone from basic co-occurrence matrices to understanding the geometry of meaning that powers billion-parameter AI systems.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "wj1w6ymre3",
   "source": "---\n\n## ðŸ“ Questions 19-20 (Experimentation)\n\nUse the dropdown tool above to answer these questions:\n\n**Q19.** Compare \"The cat sat on the mat.\" with \"Cats are great pets.\" What is their cosine similarity? Are they similar or different?\n\n**Q20.** Now compare \"Stars fuse hydrogen into helium.\" with \"Galaxies contain billions of stars.\" What is their cosine similarity? Is it higher or lower than the cat sentences? Why?\n\n*Hint: Think about what the sentences have in commonâ€”specific topics vs general relatedness.*\n\n*Record your answers (including the similarity values) in the answer sheet.*\n\n---\n\n## Activity 3: Semantic Search Mini-Application\n\nNow for the exciting part! You'll build a tiny semantic search engine that finds documents by **meaning**, not keywords.\n\n**How it works:**\n1. Embed a corpus of 20 mixed-topic sentences\n2. Embed your natural language query\n3. Compute cosine similarities between query and all corpus sentences\n4. Return the top 3 most similar sentences\n5. Visualize everything in 2D PCA space\n\n**Why this matters:**\n- This is how **Google Search** improved beyond keyword matching\n- This is the foundation of **RAG (Retrieval-Augmented Generation)** systems\n- This is how **ChatGPT** and **Claude** find relevant context from documents\n- This powers **recommendation systems** (Netflix, Spotify, Amazon)\n\nLet's try it!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}