{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 - Module 3: Revise & Compare (Stage 3)\n",
    "\n",
    "**Time:** ~20-25 minutes\n",
    "\n",
    "## Stage 3: Revision and Improvement\n",
    "\n",
    "In Module 2, you identified the 2 weakest criteria in the AI's output. Now you'll test whether AI can **improve when given explicit feedback**.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Use rubric feedback to guide AI revision\n",
    "- Evaluate whether explicit criteria help AI improve\n",
    "- Identify trade-offs in revision (gains vs. new problems)\n",
    "- Compare original vs. revised quality systematically\n",
    "- Understand AI as a revision tool (not an authority)\n",
    "\n",
    "### What You'll Do\n",
    "\n",
    "1. Review the 2 weakest criteria from Module 2\n",
    "2. Create a targeted revision prompt\n",
    "3. Get the AI to revise its work\n",
    "4. Score the revised version using the same rubric\n",
    "5. Compare: Did it improve? Where? Did new problems appear?\n",
    "6. Make an overall judgment: original or revised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML, Markdown\n",
    "\n",
    "print(\"âœ“ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all previous data\n",
    "group_code = int(input(\"Enter your group code: \"))\n",
    "\n",
    "try:\n",
    "    with open(f'lab11_group_{group_code}_scenario.json', 'r') as f:\n",
    "        scenario = json.load(f)\n",
    "    \n",
    "    with open(f'lab11_group_{group_code}_module1.json', 'r') as f:\n",
    "        module1_data = json.load(f)\n",
    "    \n",
    "    with open(f'lab11_group_{group_code}_module2.json', 'r') as f:\n",
    "        module2_data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded all data for Group {group_code}\")\n",
    "    print(f\"\\nPrompt Family: {scenario['family']}\")\n",
    "    print(f\"Rubric Used: {module2_data['selected_rubric']}\")\n",
    "    print(f\"\\nWeakest Criteria Identified:\")\n",
    "    print(f\"  1. {module2_data['weakest_criteria'][0]}\")\n",
    "    print(f\"  2. {module2_data['weakest_criteria'][1]}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nâŒ ERROR: Could not find required files for group {group_code}.\")\n",
    "    print(\"Please run Modules 0-2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Original Output and Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ORIGINAL AI OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(module1_data['ai_output'])\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYOUR ORIGINAL SCORES:\")\n",
    "print(\"=\"*70)\n",
    "for criterion, score in module2_data['human_scores'].items():\n",
    "    marker = \"âš ï¸ WEAK\" if criterion in module2_data['weakest_criteria'] else \"\"\n",
    "    print(f\"  {criterion}: {score}/5 {marker}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Revision Prompt\n",
    "\n",
    "You'll ask the AI to revise its work, focusing specifically on the 2 weakest criteria.\n",
    "\n",
    "### Effective Revision Prompts:\n",
    "\n",
    "**Good approach:**\n",
    "- Be specific about what to improve\n",
    "- Reference the rubric criteria by name\n",
    "- Give concrete examples of what's missing\n",
    "\n",
    "**Less effective:**\n",
    "- \"Make it better\"\n",
    "- \"Fix the problems\"\n",
    "- Vague, non-actionable feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generate suggested revision prompt\n",
    "def generate_revision_prompt_template():\n",
    "    weak_1 = module2_data['weakest_criteria'][0]\n",
    "    weak_2 = module2_data['weakest_criteria'][1]\n",
    "    \n",
    "    template = f\"\"\"Please revise your previous response to improve these two specific areas:\n",
    "\n",
    "1. **{weak_1}**: [Explain specifically what needs improvement based on the rubric]\n",
    "\n",
    "2. **{weak_2}**: [Explain specifically what needs improvement based on the rubric]\n",
    "\n",
    "Keep the same length (~same word count) and maintain the strengths of your original response, but focus on addressing these two weaknesses.\n",
    "\n",
    "Provide the complete revised version.\"\"\"\n",
    "    \n",
    "    return template\n",
    "\n",
    "# Display template\n",
    "template_button = widgets.Button(\n",
    "    description='ðŸ“ Show Template',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "template_output = widgets.Output()\n",
    "\n",
    "def on_template_clicked(b):\n",
    "    with template_output:\n",
    "        clear_output()\n",
    "        print(\"SUGGESTED REVISION PROMPT TEMPLATE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"(Fill in the [brackets] with specific feedback)\")\n",
    "        print()\n",
    "        print(generate_revision_prompt_template())\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "\n",
    "template_button.on_click(on_template_clicked)\n",
    "\n",
    "display(template_button)\n",
    "display(template_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget to write custom revision prompt\n",
    "revision_prompt = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Write your revision prompt here. Be specific about what needs improvement.',\n",
    "    description='Revision Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='200px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"\\nWRITE YOUR REVISION PROMPT\")\n",
    "print(\"=\"*70)\n",
    "print(\"This is what you'll give to the AI to request a revision.\")\n",
    "print()\n",
    "display(revision_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get the Revised Version from AI\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Go back to your AI conversation\n",
    "2. Copy your revision prompt from above\n",
    "3. Paste it into the AI\n",
    "4. Wait for the complete revised response\n",
    "5. Copy the ENTIRE revised output\n",
    "6. Paste it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget to record revised output\n",
    "revised_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Paste the complete REVISED output from the AI here...',\n",
    "    description='Revised Output:',\n",
    "    layout=widgets.Layout(width='100%', height='300px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "ai_change_summary = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Did the AI explain what it changed? Record that here.',\n",
    "    description='AI Changes:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"RECORD REVISED OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "display(revised_output)\n",
    "display(ai_change_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Score the Revised Version\n",
    "\n",
    "Now evaluate the **revised** output using the **same rubric** you used in Module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rubric definition\n",
    "RUBRICS = {\n",
    "    'General Communication': {\n",
    "        'criteria': [\n",
    "            {'name': 'Clarity'},\n",
    "            {'name': 'Audience Fit'},\n",
    "            {'name': 'Structure'},\n",
    "            {'name': 'Specificity'},\n",
    "            {'name': 'Accuracy'}\n",
    "        ]\n",
    "    },\n",
    "    'Persuasion/Campaign': {\n",
    "        'criteria': [\n",
    "            {'name': 'Message Focus'},\n",
    "            {'name': 'Evidence/Support'},\n",
    "            {'name': 'Call to Action'},\n",
    "            {'name': 'Tone Fit'},\n",
    "            {'name': 'Ethics'}\n",
    "        ]\n",
    "    },\n",
    "    'Creative/Narrative': {\n",
    "        'criteria': [\n",
    "            {'name': 'Engagement'},\n",
    "            {'name': 'Coherence'},\n",
    "            {'name': 'Creativity'},\n",
    "            {'name': 'Constraint Adherence'}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create scoring widgets for revised version\n",
    "selected_rubric = RUBRICS[module2_data['selected_rubric']]\n",
    "revised_scores = {}\n",
    "revised_justifications = {}\n",
    "\n",
    "print(\"SCORE THE REVISED VERSION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Using the same rubric: {module2_data['selected_rubric']}\")\n",
    "print()\n",
    "\n",
    "for criterion in selected_rubric['criteria']:\n",
    "    name = criterion['name']\n",
    "    original_score = module2_data['human_scores'][name]\n",
    "    \n",
    "    score_widget = widgets.Dropdown(\n",
    "        options=[1, 2, 3, 4, 5],\n",
    "        description=f\"{name}:\",\n",
    "        style={'description_width': '150px'},\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "    \n",
    "    justification_widget = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder=f'Why this score? Did it improve from {original_score}?',\n",
    "        layout=widgets.Layout(width='100%', height='60px')\n",
    "    )\n",
    "    \n",
    "    revised_scores[name] = score_widget\n",
    "    revised_justifications[name] = justification_widget\n",
    "    \n",
    "    marker = \"âš ï¸ TARGET\" if name in module2_data['weakest_criteria'] else \"\"\n",
    "    print(f\"Original score: {original_score}/5 {marker}\")\n",
    "    display(score_widget)\n",
    "    display(justification_widget)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Original vs. Revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison\n",
    "compare_button = widgets.Button(\n",
    "    description='ðŸ“Š Compare Versions',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "compare_output = widgets.Output()\n",
    "\n",
    "def on_compare_clicked(b):\n",
    "    with compare_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Build comparison table\n",
    "        comparison_data = []\n",
    "        total_original = 0\n",
    "        total_revised = 0\n",
    "        improvements = []\n",
    "        regressions = []\n",
    "        \n",
    "        for criterion in selected_rubric['criteria']:\n",
    "            name = criterion['name']\n",
    "            original_score = module2_data['human_scores'][name]\n",
    "            revised_score = revised_scores[name].value\n",
    "            change = revised_score - original_score\n",
    "            \n",
    "            total_original += original_score\n",
    "            total_revised += revised_score\n",
    "            \n",
    "            if change > 0:\n",
    "                improvements.append((name, original_score, revised_score, change))\n",
    "                change_str = f\"âœ“ +{change}\"\n",
    "            elif change < 0:\n",
    "                regressions.append((name, original_score, revised_score, change))\n",
    "                change_str = f\"âœ— {change}\"\n",
    "            else:\n",
    "                change_str = \"â†’ Same\"\n",
    "            \n",
    "            target = \"ðŸŽ¯\" if name in module2_data['weakest_criteria'] else \"\"\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Criterion': name,\n",
    "                'Original': original_score,\n",
    "                'Revised': revised_score,\n",
    "                'Change': change_str,\n",
    "                'Target': target\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"ORIGINAL VS. REVISED COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        print(df.to_string(index=False))\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total Original Score: {total_original}/{len(selected_rubric['criteria']) * 5}\")\n",
    "        print(f\"Total Revised Score:  {total_revised}/{len(selected_rubric['criteria']) * 5}\")\n",
    "        print(f\"Overall Change:       {total_revised - total_original:+d}\")\n",
    "        print()\n",
    "        \n",
    "        if improvements:\n",
    "            print(\"IMPROVEMENTS:\")\n",
    "            for name, orig, rev, change in improvements:\n",
    "                target_marker = \" (TARGET)\" if name in module2_data['weakest_criteria'] else \"\"\n",
    "                print(f\"  âœ“ {name}: {orig} â†’ {rev} (+{change}){target_marker}\")\n",
    "            print()\n",
    "        \n",
    "        if regressions:\n",
    "            print(\"REGRESSIONS (Got worse):\")\n",
    "            for name, orig, rev, change in regressions:\n",
    "                print(f\"  âœ— {name}: {orig} â†’ {rev} ({change})\")\n",
    "            print()\n",
    "        \n",
    "        if not improvements and not regressions:\n",
    "            print(\"No changes in any criterion.\")\n",
    "            print()\n",
    "        \n",
    "        # Check if targeted weaknesses improved\n",
    "        weak_1 = module2_data['weakest_criteria'][0]\n",
    "        weak_2 = module2_data['weakest_criteria'][1]\n",
    "        weak_1_change = revised_scores[weak_1].value - module2_data['human_scores'][weak_1]\n",
    "        weak_2_change = revised_scores[weak_2].value - module2_data['human_scores'][weak_2]\n",
    "        \n",
    "        print(\"TARGETED IMPROVEMENTS:\")\n",
    "        print(f\"  {weak_1}: {weak_1_change:+d} {'âœ“' if weak_1_change > 0 else 'âœ—' if weak_1_change < 0 else 'â†’'}\")\n",
    "        print(f\"  {weak_2}: {weak_2_change:+d} {'âœ“' if weak_2_change > 0 else 'âœ—' if weak_2_change < 0 else 'â†’'}\")\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "\n",
    "compare_button.on_click(on_compare_clicked)\n",
    "\n",
    "display(compare_button)\n",
    "display(compare_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analysis Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis widgets\n",
    "new_problems = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Did revision introduce any NEW problems? Be specific.',\n",
    "    description='New Problems:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "overall_judgment = widgets.RadioButtons(\n",
    "    options=['Original is better', 'Revised is better', 'About the same', 'Mixed (some better, some worse)'],\n",
    "    description='Overall:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "judgment_reasoning = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Why did you choose this judgment? Consider all criteria, not just the targeted ones.',\n",
    "    description='Reasoning:',\n",
    "    layout=widgets.Layout(width='100%', height='120px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "further_revision = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='If you could revise again, what would you focus on?',\n",
    "    description='Next Revision:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"\\nANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "display(new_problems)\n",
    "display(overall_judgment)\n",
    "display(judgment_reasoning)\n",
    "display(further_revision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 (Optional): Ask AI to Score the Revision\n",
    "\n",
    "If time permits, you can ask the AI to score its **revised** version and see if its self-evaluation improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: AI self-eval of revision\n",
    "ai_revised_self_eval = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='(Optional) Paste AI\\'s self-evaluation of the REVISED version here',\n",
    "    description='AI Rev. Eval:',\n",
    "    layout=widgets.Layout(width='100%', height='150px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "ai_eval_comparison = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='(Optional) Was AI more/less accurate in evaluating the revision?',\n",
    "    description='Comparison:',\n",
    "    layout=widgets.Layout(width='100%', height='80px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"\\nOPTIONAL: AI SELF-EVALUATION OF REVISION\")\n",
    "print(\"=\"*70)\n",
    "display(ai_revised_self_eval)\n",
    "display(ai_eval_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save button\n",
    "save_module3_button = widgets.Button(\n",
    "    description='ðŸ’¾ Save Module 3',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "save_output = widgets.Output()\n",
    "\n",
    "def on_save_module3_clicked(b):\n",
    "    with save_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Validate\n",
    "        if not revision_prompt.value:\n",
    "            print(\"âŒ Please write a revision prompt\")\n",
    "            return\n",
    "        if not revised_output.value or len(revised_output.value.strip()) < 50:\n",
    "            print(\"âŒ Please paste the revised output\")\n",
    "            return\n",
    "        if not overall_judgment.value:\n",
    "            print(\"âŒ Please make an overall judgment\")\n",
    "            return\n",
    "        \n",
    "        # Compile data\n",
    "        revised_scores_data = {name: widget.value for name, widget in revised_scores.items()}\n",
    "        revised_justifications_data = {name: widget.value for name, widget in revised_justifications.items()}\n",
    "        \n",
    "        module3_data = {\n",
    "            'group_code': group_code,\n",
    "            'revision_prompt': revision_prompt.value,\n",
    "            'revised_output': revised_output.value,\n",
    "            'ai_change_summary': ai_change_summary.value,\n",
    "            'revised_scores': revised_scores_data,\n",
    "            'revised_justifications': revised_justifications_data,\n",
    "            'new_problems': new_problems.value,\n",
    "            'overall_judgment': overall_judgment.value,\n",
    "            'judgment_reasoning': judgment_reasoning.value,\n",
    "            'further_revision_ideas': further_revision.value,\n",
    "            'ai_revised_self_eval': ai_revised_self_eval.value,\n",
    "            'ai_eval_comparison': ai_eval_comparison.value\n",
    "        }\n",
    "        \n",
    "        # Save\n",
    "        with open(f'lab11_group_{group_code}_module3.json', 'w') as f:\n",
    "            json.dump(module3_data, f, indent=2)\n",
    "        \n",
    "        print(\"âœ“ Module 3 data saved successfully!\")\n",
    "        print(f\"\\nOverall Judgment: {overall_judgment.value}\")\n",
    "        print(f\"\\nYou're ready for Module 4 (Reflection & Synthesis)!\")\n",
    "\n",
    "save_module3_button.on_click(on_save_module3_clicked)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVE YOUR WORK\")\n",
    "print(\"=\"*70)\n",
    "display(save_module3_button)\n",
    "display(save_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3 Questions\n",
    "\n",
    "Answer these on your Lab 11 Answer Sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: Revision Prompt\n",
    "\n",
    "What were the 2 weakest criteria? What revision prompt did you use? Was it specific and actionable?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: Improvement Analysis\n",
    "\n",
    "Did the revised version improve on the targeted criteria? Show the comparison (original score â†’ revised score for each targeted criterion).\n",
    "\n",
    "*(Answer on your answer sheet with comparison table)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: New Problems\n",
    "\n",
    "Did the revision introduce any NEW problems? Did improving one area make another area worse?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: Overall Judgment\n",
    "\n",
    "Overall, is the revised version better than the original? Why or why not? Consider ALL criteria, not just the targeted ones.\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16 (Optional): AI Self-Evaluation Accuracy\n",
    "\n",
    "If you asked AI to score its revision, was it more or less accurate than its original self-evaluation? Did it learn from the feedback?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17: Further Revision\n",
    "\n",
    "If you had to revise AGAIN, what would you focus on? What's still weak?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Accomplished\n",
    "\n",
    "âœ“ Identified the 2 weakest criteria from Module 2\n",
    "\n",
    "âœ“ Created a targeted revision prompt\n",
    "\n",
    "âœ“ Got AI to revise its work\n",
    "\n",
    "âœ“ Scored the revised version using the same rubric\n",
    "\n",
    "âœ“ Compared original vs. revised systematically\n",
    "\n",
    "âœ“ Identified improvements, regressions, and trade-offs\n",
    "\n",
    "âœ“ Made an overall judgment about quality\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "You've likely discovered:\n",
    "\n",
    "1. **Explicit feedback helps**: When you give clear, rubric-based guidance, AI can often improve\n",
    "\n",
    "2. **Trade-offs exist**: Improving one aspect sometimes makes another worse\n",
    "\n",
    "3. **Iteration has limits**: AI revision is useful but not perfect\n",
    "\n",
    "4. **Human judgment remains essential**: You, not the AI, must decide if the revision is actually better\n",
    "\n",
    "### The Bigger Picture\n",
    "\n",
    "AI is a **collaborator**, not an **authority**:\n",
    "- âœ“ Good at generating drafts\n",
    "- âœ“ Good at revising when given specific feedback\n",
    "- âœ— Poor at evaluating its own quality\n",
    "- âœ— Cannot reliably decide what's \"good\"\n",
    "\n",
    "**Humans must provide the judgment, standards, and oversight.**\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 4 (Reflection)**, you will:\n",
    "- Synthesize your findings from all 3 stages\n",
    "- Read case studies about AI evaluation failures\n",
    "- Articulate when AI is a good collaborator vs. when humans must lead\n",
    "- Develop principles for responsible human-AI collaboration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
