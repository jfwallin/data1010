{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 4, Module 3: Real-World Classification - Penguin Species\n\n## From XOR to Real Animals: Multiclass Neural Networks\n\n**Learning Objectives:**\n- Apply neural networks to a real dataset with measurements you can picture\n- Understand multiclass classification (3 species, not just binary)\n- Compare linear baseline vs. hidden layer model\n- See how model complexity affects performance\n- Connect high-level Keras/TensorFlow to earlier hand-built gradient descent\n\n**Estimated time:** 15-20 minutes\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: Introduction - From Abstract to Real\n\n**Remember from earlier modules:**\n- Module 0-2: You worked with XOR - an abstract pattern problem\n- You manually adjusted 9 weights to separate data points\n- You saw gradient descent automate this process\n- You experimented with momentum and learning rates\n\n**Today's shift to reality:**\n- Real dataset: Palmer Penguins (not abstract patterns)\n- Real task: Identify species from body measurements\n- Real tools: TensorFlow/Keras (industry-standard ML library)\n- Real expectations: ~95% accuracy is excellent (not 100%)\n\n### The Palmer Penguins Dataset\n\nThe Palmer Penguins dataset contains body measurements of 333 penguins from 3 different species observed on islands near Antarctica:\n- **Adelie** (class 0)\n- **Chinstrap** (class 1)\n- **Gentoo** (class 2)\n\nEach penguin has 4 numeric features:\n1. Bill length (mm) â€” how long the beak is\n2. Bill depth (mm) â€” how tall/thick the beak is\n3. Flipper length (mm) â€” how long the wing-flipper is\n4. Body mass (g) â€” how much the penguin weighs\n\n**The Task:** Given these 4 measurements, predict which species the penguin belongs to.\n\n**Connection to XOR:** Like XOR had 2 inputs (xâ‚, xâ‚‚) and 2 classes, Penguins has 4 inputs and 3 classes. The principles are the same, but now we're solving a real problem!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Setup and Package Check\n",
    "\n",
    "First, let's make sure we have all the packages we need. This cell will work on Google Colab or your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Package check and installation\nimport sys\n\nprint(\"Checking packages...\")\nprint(f\"Python version: {sys.version}\")\n\n# Check/install required packages\nrequired_packages = [\n    ('numpy', 'numpy'),\n    ('matplotlib', 'matplotlib'),\n    ('seaborn', 'seaborn'),\n    ('sklearn', 'scikit-learn'),\n    ('tensorflow', 'tensorflow')\n]\n\nmissing_packages = []\nfor module_name, package_name in required_packages:\n    try:\n        __import__(module_name)\n        print(f\"\\u2713 {module_name} is installed\")\n    except ImportError:\n        print(f\"\\u2717 {module_name} not found\")\n        missing_packages.append(package_name)\n\nif missing_packages:\n    print(f\"\\nInstalling missing packages: {', '.join(missing_packages)}\")\n    import subprocess\n    for package in missing_packages:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n    print(\"\\u2713 All packages installed!\")\nelse:\n    print(\"\\n\\u2713 All required packages are already installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import all the libraries we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(\"\\u2713 All imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Load and Explore the Dataset\n",
    "\n",
    "Let's load the Iris dataset and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Palmer Penguins dataset from seaborn\npenguins = sns.load_dataset('penguins')\n\n# Drop rows with missing values\npenguins = penguins.dropna(subset=['bill_length_mm', 'bill_depth_mm',\n                                    'flipper_length_mm', 'body_mass_g'])\n\n# Extract numeric features and labels\nfeature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\nfriendly_names = ['Bill Length (mm)', 'Bill Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nX = penguins[feature_names].values\n\n# Encode species as integers: Adelie=0, Chinstrap=1, Gentoo=2\nle = LabelEncoder()\ny = le.fit_transform(penguins['species'])\nspecies_names = le.classes_  # ['Adelie', 'Chinstrap', 'Gentoo']\n\nprint(\"=\"*70)\nprint(\"PALMER PENGUINS DATASET OVERVIEW\")\nprint(\"=\"*70)\nprint(f\"\\nDataset shape:\")\nprint(f\"  Features (X): {X.shape} - {X.shape[0]} penguins, {X.shape[1]} measurements each\")\nprint(f\"  Labels (y):   {y.shape} - {y.shape[0]} species labels\")\n\nprint(f\"\\nFeature names:\")\nfor i, (col, friendly) in enumerate(zip(feature_names, friendly_names)):\n    print(f\"  {i+1}. {friendly}\")\n\nprint(f\"\\nTarget classes:\")\nfor i, name in enumerate(species_names):\n    count = np.sum(y == i)\n    print(f\"  Class {i}: {name:15s} - {count} samples\")\n\nprint(f\"\\nSample data (first 3 penguins):\")\nprint(f\"  {'Bill L':>7s}  {'Bill D':>7s}  {'Flip L':>7s}  {'Mass':>7s}  Species\")\nfor i in range(3):\n    print(f\"  {X[i][0]:7.1f}  {X[i][1]:7.1f}  {X[i][2]:7.1f}  {X[i][3]:7.0f}  {species_names[y[i]]}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualize: Can We See the Species?\n\nLet's plot two of the features to see if the species are visually separable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create scatter plot using flipper length vs body mass\nfig, ax = plt.subplots(figsize=(10, 7), dpi=100)\n\n# Plot each class with different color\ncolors = ['blue', 'red', 'green']\nfor i, (name, color) in enumerate(zip(species_names, colors)):\n    mask = (y == i)\n    ax.scatter(X[mask, 2], X[mask, 3],\n              c=color, label=name, s=100, alpha=0.7,\n              edgecolors='k', linewidths=1.5)\n\nax.set_xlabel('Flipper Length (mm)', fontsize=13, fontweight='bold')\nax.set_ylabel('Body Mass (g)', fontsize=13, fontweight='bold')\nax.set_title('Palmer Penguins: Flipper Length vs Body Mass by Species',\n            fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper left')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"   - Gentoo (green) is clearly separated â€” large flippers and heavy\")\nprint(\"   - Adelie (blue) and Chinstrap (red) overlap somewhat\")\nprint(\"   - This is like XOR, but with 3 classes instead of 2!\")\nprint(\"   - A neural network should be able to learn these boundaries\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 4: Train/Test Split and Feature Scaling\n\n**Why split the data?**\n- **Training set (80%):** Used to teach the network\n- **Test set (20%):** Used to evaluate how well it learned (never seen during training)\n\n**Why scale features?**\n- Features have very different units and ranges (e.g., flipper: 172-231 mm, mass: 2700-6300 g)\n- Neural networks learn better when all features are on similar scales\n- StandardScaler transforms each feature to mean=0, std=1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (80%) and test (20%) sets\n",
    "# stratify=y ensures each class is proportionally represented in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training set:   {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set:       {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features to mean=0, std=1\n",
    "# IMPORTANT: Fit scaler on training data only, then apply to both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling:\")\n",
    "print(f\"  Before scaling - mean: {X_train[:, 0].mean():.2f}, std: {X_train[:, 0].std():.2f}\")\n",
    "print(f\"  After scaling  - mean: {X_train_scaled[:, 0].mean():.2f}, std: {X_train_scaled[:, 0].std():.2f}\")\n",
    "print(\"\\nâœ“ Data prepared for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 5: Baseline Linear Model (No Hidden Layer)\n\n**Connection to Module 2:**\n- Remember the XOR network with NO hidden neurons? It couldn't learn XOR.\n- A linear model draws straight boundaries in feature space\n- Let's see how well this works for Penguins!\n\n**Model architecture:**\n- Input: 4 features\n- Output: 3 units (one per class) with softmax activation\n- NO hidden layers - just a direct linear transformation\n\nThis is like logistic regression, but for 3 classes instead of 2."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build linear model (no hidden layer)\nlinear_model = Sequential([\n    Dense(3, activation='softmax', input_dim=4, name='output')\n], name='Linear_Model')\n\n# Compile model\n# - Adam optimizer: like momentum from Module 2, but smarter!\n# - sparse_categorical_crossentropy: loss function for multiclass classification\n# - accuracy: % of correct predictions\nlinear_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Show model architecture\nprint(\"=\"*70)\nprint(\"LINEAR MODEL ARCHITECTURE\")\nprint(\"=\"*70)\nlinear_model.summary()\nprint(\"\\nThis model has NO hidden layers - just 4 inputs \\u2192 3 outputs\")\nprint(\"   Total trainable parameters: (4 features \\u00d7 3 classes) + 3 biases = 15\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Linear Model\n",
    "\n",
    "**What's happening during training:**\n",
    "- The `.fit()` method does gradient descent automatically!\n",
    "- It computes gradients (backpropagation) just like you saw in Module 2\n",
    "- Adam optimizer updates weights using momentum and adaptive learning rates\n",
    "- All this happens behind the scenes - no manual weight adjustment needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training linear model...\\n\")\n",
    "\n",
    "history_linear = linear_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    verbose=0  # Suppress detailed output\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on test set\ntest_loss, test_accuracy = linear_model.evaluate(X_test_scaled, y_test, verbose=0)\n\nprint(\"=\"*70)\nprint(\"LINEAR MODEL RESULTS\")\nprint(\"=\"*70)\nprint(f\"Test Accuracy:  {test_accuracy:.1%}\")\nprint(f\"Test Loss:      {test_loss:.4f}\")\nprint(\"=\"*70)\n\nif test_accuracy >= 0.95:\n    print(\"\\nExcellent! Linear model works well on Penguins.\")\n    print(\"   Why? The classes are mostly linearly separable.\")\nelif test_accuracy >= 0.90:\n    print(\"\\nGood performance, but not perfect.\")\n    print(\"  The overlapping classes are hard to separate with straight boundaries.\")\nelse:\n    print(\"\\nLinear model struggles with this data.\")\n    print(\"  We'll need hidden layers to improve!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training and validation accuracy\ntry:\n    history_linear\nexcept NameError:\n    raise RuntimeError(\n        \"history_linear is not defined. Please run the 'Train the Linear Model' \"\n        \"cell above (Section 5) before running this cell.\"\n    )\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n\n# Accuracy plot\nax1.plot(history_linear.history['accuracy'], label='Training', linewidth=2)\nax1.plot(history_linear.history['val_accuracy'], label='Validation', linewidth=2)\nax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\nax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\nax1.set_title('Linear Model: Accuracy over Training', fontsize=13, fontweight='bold')\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# Loss plot\nax2.plot(history_linear.history['loss'], label='Training', linewidth=2)\nax2.plot(history_linear.history['val_loss'], label='Validation', linewidth=2)\nax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\nax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\nax2.set_title('Linear Model: Loss over Training', fontsize=13, fontweight='bold')\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nWhat to look for:\")\nprint(\"   - Accuracy increases quickly in early epochs\")\nprint(\"   - Training and validation curves are close (no overfitting)\")\nprint(\"   - Loss decreases and stabilizes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 6: Model with Hidden Layer\n\n**Connection to Module 1:**\n- Remember the 2-2-1 network for XOR? It had 2 hidden neurons.\n- Hidden layers let the network learn **curved boundaries** (not just straight lines)\n- More hidden neurons = more flexibility (but also more to learn)\n\n**Experiment:** Try different numbers of hidden units and see how it affects performance!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Adjustable parameter - TRY CHANGING THIS!\nhidden_units = 8  # Try: 2, 4, 8, 16, 32, 64\n\n# Build model with one hidden layer\nhidden_model = Sequential([\n    Dense(hidden_units, activation='relu', input_dim=4, name='hidden'),\n    Dense(3, activation='softmax', name='output')\n], name='Hidden_Layer_Model')\n\n# Compile with same settings as linear model\nhidden_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"=\"*70)\nprint(f\"MODEL WITH {hidden_units} HIDDEN UNITS\")\nprint(\"=\"*70)\nhidden_model.summary()\nprint(f\"\\nThis model has a hidden layer with {hidden_units} neurons\")\nprint(\"   Each hidden neuron creates a new 'dimension' (like x\\u2083 = x\\u2081 \\u00d7 x\\u2082 for XOR)\")\nprint(\"   ReLU activation allows curved boundaries (like tanh in Module 1)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Hidden Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(f\"Training model with {hidden_units} hidden units...\\n\")\n",
    "\n",
    "history_hidden = hidden_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on test set\ntest_loss_hidden, test_accuracy_hidden = hidden_model.evaluate(X_test_scaled, y_test, verbose=0)\n\nprint(\"=\"*70)\nprint(\"COMPARISON: LINEAR vs HIDDEN LAYER MODEL\")\nprint(\"=\"*70)\nprint(f\"Linear Model (no hidden layer):    {test_accuracy:.1%}\")\nprint(f\"Hidden Layer Model ({hidden_units:2d} units):     {test_accuracy_hidden:.1%}\")\nprint(\"=\"*70)\n\nimprovement = test_accuracy_hidden - test_accuracy\nif improvement > 0.02:\n    print(f\"\\nHidden layer helps! Improvement: +{improvement:.1%}\")\nelif improvement > 0:\n    print(f\"\\nSlight improvement: +{improvement:.1%}\")\nelse:\n    print(f\"\\nNo improvement. Linear model was already good enough!\")\n    print(\"   This suggests Penguins data is mostly linearly separable.\")\n\nprint(f\"\\nTry changing 'hidden_units' above and re-running to experiment!\")\nprint(\"   Does more neurons always help? Where do you see diminishing returns?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Hidden Layer Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for hidden layer model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history_hidden.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history_hidden.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Hidden Layer Model ({hidden_units} units): Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history_hidden.history['loss'], label='Training', linewidth=2)\n",
    "ax2.plot(history_hidden.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Hidden Layer Model ({hidden_units} units): Loss', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 8: Understanding Variability - ML is an Experiment!\n\n### Why Run Multiple Experiments?\n\n**You may have noticed:** Running the same model twice can give different results!\n\n**Why does this happen?**\n- Neural networks start with **random initial weights**\n- Different starting points can lead to different final solutions\n- This is called the **stochastic nature** of machine learning\n\n**Connection to Module 2:**\n- Remember when you tried different random starting points for XOR?\n- Some converged faster, some to slightly different loss values\n- Same thing happens here - but Keras handles it automatically!\n\n### The Scientific Approach\n\n**In real ML research and practice:**\n- You NEVER report results from a single run\n- You run the experiment multiple times (5, 10, or even 100 times)\n- You report the **mean (average)** and **standard deviation (spread)**\n- This tells you: \"How reliable is this model?\"\n\n**Let's do this properly!** Run multiple experiments and analyze the results statistically.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Experiment: Linear Model with Multiple Runs\n\nLet's train the linear model **5 times** and see how much the results vary.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear model multiple times to understand variability\n",
    "num_runs = 5\n",
    "num_epochs = 70\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"RUNNING LINEAR MODEL {num_runs} TIMES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWhy? To see how much results vary due to random initialization!\\n\")\n",
    "\n",
    "# Store results from each run\n",
    "linear_test_accuracies = []\n",
    "linear_test_losses = []\n",
    "linear_histories = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run+1}/{num_runs}...\", end=\" \")\n",
    "    \n",
    "    # IMPORTANT: Create a NEW model each time!\n",
    "    # This ensures fresh random initialization\n",
    "    linear_model_run = Sequential([\n",
    "        Dense(3, activation='softmax', input_dim=4, name='output')\n",
    "    ], name=f'Linear_Model_Run_{run+1}')\n",
    "    \n",
    "    linear_model_run.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = linear_model_run.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = linear_model_run.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    linear_test_accuracies.append(test_accuracy)\n",
    "    linear_test_losses.append(test_loss)\n",
    "    linear_histories.append(history)\n",
    "    \n",
    "    print(f\"Accuracy: {test_accuracy:.1%}, Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY - LINEAR MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:  {np.mean(linear_test_accuracies):.1%} Â± {np.std(linear_test_accuracies):.1%}\")\n",
    "print(f\"Test Loss:      {np.mean(linear_test_losses):.4f} Â± {np.std(linear_test_losses):.4f}\")\n",
    "print(f\"\\nMin Accuracy:   {np.min(linear_test_accuracies):.1%}\")\n",
    "print(f\"Max Accuracy:   {np.max(linear_test_accuracies):.1%}\")\n",
    "print(f\"Range:          {np.max(linear_test_accuracies) - np.min(linear_test_accuracies):.1%}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ’¡ What does this tell us?\")\n",
    "if np.std(linear_test_accuracies) < 0.05:\n",
    "    print(\"   - Low standard deviation â†’ Results are CONSISTENT\")\n",
    "    print(\"   - The model is stable across different initializations\")\n",
    "else:\n",
    "    print(\"   - Higher standard deviation â†’ Results VARY\")\n",
    "    print(\"   - Random initialization matters more for this model\")\n",
    "print(f\"   - We can report: {np.mean(linear_test_accuracies):.1%} Â± {np.std(linear_test_accuracies):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Runs Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves from all runs on the same plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n",
    "\n",
    "# Accuracy plot - all runs\n",
    "for i, history in enumerate(linear_histories):\n",
    "    ax1.plot(history.history['val_accuracy'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Linear Model: {num_runs} Runs - Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot - all runs\n",
    "for i, history in enumerate(linear_histories):\n",
    "    ax2.plot(history.history['val_loss'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Linear Model: {num_runs} Runs - Validation Loss', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ What to observe:\")\n",
    "print(\"   - Do all runs converge to similar accuracy?\")\n",
    "print(\"   - Do some runs converge faster than others?\")\n",
    "print(\"   - Is there a spread in final performance?\")\n",
    "print(\"   - This variability is NORMAL and expected in ML!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Hidden Layer Model with Multiple Runs\n",
    "\n",
    "Now let's do the same for the hidden layer model. Do models with more parameters vary more or less?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable parameter - change this to experiment!\n",
    "hidden_units_experiment = 16  # Try: 8, 16, 32\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"RUNNING HIDDEN LAYER MODEL ({hidden_units_experiment} units) {num_runs} TIMES\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Store results from each run\n",
    "hidden_test_accuracies = []\n",
    "hidden_test_losses = []\n",
    "hidden_histories = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run+1}/{num_runs}...\", end=\" \")\n",
    "    \n",
    "    # Create a NEW model each time for fresh initialization\n",
    "    hidden_model_run = Sequential([\n",
    "        Dense(hidden_units_experiment, activation='relu', input_dim=4, name='hidden'),\n",
    "        Dense(3, activation='softmax', name='output')\n",
    "    ], name=f'Hidden_Model_Run_{run+1}')\n",
    "    \n",
    "    hidden_model_run.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = hidden_model_run.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = hidden_model_run.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    hidden_test_accuracies.append(test_accuracy)\n",
    "    hidden_test_losses.append(test_loss)\n",
    "    hidden_histories.append(history)\n",
    "    \n",
    "    print(f\"Accuracy: {test_accuracy:.1%}, Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"STATISTICAL SUMMARY - HIDDEN LAYER MODEL ({hidden_units_experiment} units)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:  {np.mean(hidden_test_accuracies):.1%} Â± {np.std(hidden_test_accuracies):.1%}\")\n",
    "print(f\"Test Loss:      {np.mean(hidden_test_losses):.4f} Â± {np.std(hidden_test_losses):.4f}\")\n",
    "print(f\"\\nMin Accuracy:   {np.min(hidden_test_accuracies):.1%}\")\n",
    "print(f\"Max Accuracy:   {np.max(hidden_test_accuracies):.1%}\")\n",
    "print(f\"Range:          {np.max(hidden_test_accuracies) - np.min(hidden_test_accuracies):.1%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Hidden Layer Model Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves from all runs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n",
    "\n",
    "# Accuracy plot - all runs\n",
    "for i, history in enumerate(hidden_histories):\n",
    "    ax1.plot(history.history['val_accuracy'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Hidden Layer Model ({hidden_units_experiment}U): {num_runs} Runs - Validation Accuracy', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot - all runs\n",
    "for i, history in enumerate(hidden_histories):\n",
    "    ax2.plot(history.history['val_loss'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Hidden Layer Model ({hidden_units_experiment}U): {num_runs} Runs - Validation Loss', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comparison: Linear vs Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL COMPARISON: LINEAR vs HIDDEN LAYER (STATISTICAL)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLinear Model (no hidden layer):\")\n",
    "print(f\"  Mean Accuracy:  {np.mean(linear_test_accuracies):.1%} Â± {np.std(linear_test_accuracies):.1%}\")\n",
    "print(f\"  Mean Loss:      {np.mean(linear_test_losses):.4f} Â± {np.std(linear_test_losses):.4f}\")\n",
    "\n",
    "print(f\"\\nHidden Layer Model ({hidden_units_experiment} units):\")\n",
    "print(f\"  Mean Accuracy:  {np.mean(hidden_test_accuracies):.1%} Â± {np.std(hidden_test_accuracies):.1%}\")\n",
    "print(f\"  Mean Loss:      {np.mean(hidden_test_losses):.4f} Â± {np.std(hidden_test_losses):.4f}\")\n",
    "\n",
    "mean_improvement = np.mean(hidden_test_accuracies) - np.mean(linear_test_accuracies)\n",
    "print(f\"\\nMean Improvement: {mean_improvement:+.1%}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create box plot to visualize distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "\n",
    "box_data = [linear_test_accuracies, hidden_test_accuracies]\n",
    "box_labels = ['Linear Model\\n(0 hidden layers)', f'Hidden Layer Model\\n({hidden_units_experiment} units)']\n",
    "\n",
    "bp = ax.boxplot(box_data, labels=box_labels, patch_artist=True,\n",
    "                showmeans=True, meanline=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Test Accuracy Distribution Across {num_runs} Runs', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add individual points\n",
    "for i, data in enumerate(box_data, 1):\n",
    "    x = np.random.normal(i, 0.04, size=len(data))\n",
    "    ax.scatter(x, data, alpha=0.6, s=60, c='red', edgecolors='black', linewidths=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Understanding the box plot:\")\n",
    "print(\"   - Box shows the middle 50% of results (interquartile range)\")\n",
    "print(\"   - Line in box is the median\")\n",
    "print(\"   - Green dashed line is the mean\")\n",
    "print(\"   - Red dots are individual run results\")\n",
    "print(\"   - Whiskers show the full range (min to max)\")\n",
    "print(\"\\n   â†’ This is how you should report ML results in research!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Insights from Multiple Runs\n\n**What you should have learned:**\n\n1. **ML is stochastic** - Same code, different results each time!\n   - Random weight initialization matters\n   - Results vary from run to run\n\n2. **Reporting means Â± std is standard practice**\n   - Never trust a single run\n   - Always run experiments multiple times\n   - Report: \"Accuracy: 93.2% Â± 2.1%\" not \"Accuracy: 95.0%\"\n\n3. **Variability can tell you about model stability**\n   - Low std â†’ Consistent, reliable model\n   - High std â†’ Sensitive to initialization, may need more tuning\n\n4. **Statistical comparison is more robust**\n   - Comparing single runs: \"95% vs 93%\" (unreliable!)\n   - Comparing distributions: \"94.5Â±1.2% vs 92.8Â±2.3%\" (reliable!)\n\n**Connection to Module 2:**\n- Remember testing multiple random starting points for XOR?\n- You saw some converged faster, some to different local minima\n- **Same concept here**, but now you're quantifying it statistically!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 9: Connection to Earlier Labs\n\nLet's explicitly connect what we just did to the gradient descent work from Modules 0-2!\n\n### What Just Happened Behind the Scenes?\n\n**From Module 2 - Training with Gradient Descent:**\n\nRemember when you saw:\n```python\n# Manual gradient descent (Module 2)\nfor epoch in range(max_epochs):\n    grads = compute_gradients(network, X, y)\n    weights = weights - learning_rate * grads\n```\n\n**Keras `.fit()` does this automatically:**\n- Computes gradients using backpropagation (same math you saw in Module 2!)\n- Updates weights using Adam optimizer (like momentum, but smarter)\n- Loops through epochs and batches\n- Tracks training/validation metrics\n\n### Adam Optimizer = Gradient Descent with Momentum + More\n\n**From Module 2, you saw:**\n```python\nvelocity = beta * velocity + (1-beta) * gradient  # Momentum\nweights = weights - learning_rate * velocity\n```\n\n**Adam does:**\n- Momentum (like you saw)\n- Adaptive learning rates (different rate for each weight)\n- Bias correction\n- All automatically tuned!\n\n### Multi-Start Initialization\n\n**From Module 2:**\n- You manually tried multiple random starting points\n- Picked the one with lowest initial loss\n\n**Keras does this automatically:**\n- Uses smart initialization (Xavier/He initialization)\n- Weights start in a good range for the activation function\n- No need to manually restart!\n\n### The Big Picture\n\n```\nModule 0: Learned that lifting to higher dimensions helps (xâ‚ƒ = xâ‚ Ã— xâ‚‚)\n         â†’ Hidden layers do this automatically!\n\nModule 1: Manually adjusted 9 weights to solve XOR\n         â†’ .fit() does this automatically with gradient descent!\n\nModule 2: Saw gradient descent with momentum converge automatically\n         â†’ Adam optimizer is even better!\n\nModule 3: Applied these same principles to REAL data (penguin species)\n         â†’ Same math, real problems, industrial-strength tools!\n```\n\n**The fundamental concepts haven't changed - we're just using powerful tools that automate the process!**\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 10: Key Takeaways\n\n### 1. Real-World ML Works!\n- Neural networks can classify real data (penguin species) with high accuracy\n- 95%+ accuracy on Penguins is excellent (not 100% - that would be suspicious!)\n- The principles from XOR apply to real problems\n\n### 2. Linear Models Can Be Sufficient\n- Penguins data is mostly linearly separable â†’ linear model works well\n- Hidden layers help, but the improvement may be modest\n- Not every problem needs a deep network!\n\n### 3. Hidden Layers Add Flexibility\n- Hidden neurons create new \"dimensions\" (like xâ‚ƒ = xâ‚ Ã— xâ‚‚ for XOR)\n- More units = more flexibility, but diminishing returns\n- There's a sweet spot - too few underfits, too many may overfit\n\n### 4. Keras/TensorFlow Automates Gradient Descent\n- `.fit()` does backpropagation and weight updates automatically\n- Adam optimizer is like momentum (Module 2) but better\n- All the math you learned still applies - it's just hidden!\n\n### 5. Real Data Has Realistic Accuracy\n- Perfect accuracy (100%) is rare and may indicate overfitting\n- 90-98% is typical for well-separated classes\n- Some errors are expected due to noise and overlap\n\n\n### 6. ML is Stochastic - Always Run Multiple Experiments!\n- Random initialization causes result variability\n- Professional ML practice: run multiple times, report mean Â± std\n- Low variability = stable, reliable model\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reflection Questions\n\nAnswer these on your answer sheet:\n\n**Q1.** Did the linear model (no hidden layer) achieve high accuracy on Penguins? Why or why not?\n\n**Q2.** How much did adding a hidden layer improve accuracy? Was the improvement large or small?\n\n**Q3.** Experiment with different values of `hidden_units` (2, 4, 8, 16, 32, 64). At what point do you see diminishing returns?\n\n**Q4.** Compare this to Module 2 XOR training:\n   - What's similar? (Hint: gradient descent, weight updates)\n   - What's different? (Hint: manual vs. automatic, dataset)\n\n**Q5.** Why is 95% accuracy considered \"excellent\" while 100% might be suspicious?\n\n**Q6.** Looking at the flipper-vs-mass scatter plot, why do you think Adelie and Chinstrap are harder to separate than Gentoo?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. **Experiment** with `hidden_units` above - try 2, 8, 16, 32, 64\n2. **Record** your results: how does test accuracy change?\n3. **Answer** the reflection questions on your answer sheet\n4. **Return to the LMS** and continue to Module 4\n\nIn **Module 4**, you'll work with a medical diagnosis dataset (breast cancer detection) with 30 features instead of 4. You'll experiment with model architecture (number of layers and units) and learn about confusion matrices for medical applications.\n\n---\n\n**Great job! You've successfully applied neural networks to real-world data!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}