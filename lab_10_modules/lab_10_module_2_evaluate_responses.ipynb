{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 - Module 2: Evaluating AI Responses\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "In this module, you'll verify the actual accuracy of the AI responses you collected in Module 1.\n",
    "\n",
    "For each prompt, you'll:\n",
    "- Verify the AI's claims using Google, Wikipedia, or other sources\n",
    "- Record the actual accuracy level\n",
    "- Identify specific error types (if any)\n",
    "- Document hallucinations or fabrications\n",
    "\n",
    "This is where you'll discover whether the AI's confidence matched its correctness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML, Markdown\n",
    "\n",
    "print(\"✓ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Your Group Code\n",
    "\n",
    "Use the **same group code** from Modules 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_code = int(input(\"Enter your group code: \"))\n",
    "\n",
    "# Load prompts and predictions\n",
    "prompts_filename = f\"lab10_group_{group_code}_prompts.csv\"\n",
    "predictions_filename = f\"lab10_group_{group_code}_predictions.csv\"\n",
    "\n",
    "try:\n",
    "    prompts_df = pd.read_csv(prompts_filename)\n",
    "    predictions_df = pd.read_csv(predictions_filename)\n",
    "    \n",
    "    # Merge data\n",
    "    data_df = prompts_df.merge(predictions_df, on=['prompt_id', 'category'])\n",
    "    \n",
    "    print(f\"✓ Loaded {len(data_df)} prompts with predictions\")\n",
    "    print(f\"✓ Group Code: {group_code}\")\n",
    "    print()\n",
    "    print(\"Reminder of what you recorded in Module 1:\")\n",
    "    display(data_df[['prompt_id', 'category_display', 'ai_confidence', 'student_prediction']].head())\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ ERROR: Could not find required files\")\n",
    "    print(f\"Missing: {e.filename}\")\n",
    "    print(\"\\nMake sure you:\")\n",
    "    print(\"1. Ran Module 0 (to generate prompts)\")\n",
    "    print(\"2. Ran Module 1 (to collect predictions)\")\n",
    "    print(\"3. Are using the same group code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Instructions\n",
    "\n",
    "### How to Verify Each Response\n",
    "\n",
    "For each of the 8 prompts, you'll need to verify whether the AI's response was actually accurate.\n",
    "\n",
    "### Verification Strategies\n",
    "\n",
    "**For factual claims:**\n",
    "- Search Google for authoritative sources\n",
    "- Check Wikipedia (but verify with additional sources)\n",
    "- Look for government data, academic sources, or reputable news\n",
    "\n",
    "**For citations and sources:**\n",
    "- Search for the exact paper/book title in quotes\n",
    "- Check if the authors exist and work in that field\n",
    "- Verify the journal or publisher is real\n",
    "- Try Google Scholar for academic papers\n",
    "- **Red flag:** If you can't find it anywhere, it's likely fabricated\n",
    "\n",
    "**For calculations:**\n",
    "- Use a calculator or spreadsheet\n",
    "- Double-check the mathematical reasoning\n",
    "- Verify units and reasonableness\n",
    "\n",
    "**For logic problems:**\n",
    "- Work through the problem yourself\n",
    "- Check if the AI's reasoning makes sense step-by-step\n",
    "- Look for common trick question patterns\n",
    "\n",
    "**For recent events:**\n",
    "- Remember that AI has a training cutoff\n",
    "- Check if the AI acknowledged this limitation\n",
    "- Verify dates and current information\n",
    "\n",
    "### What Counts as a Hallucination?\n",
    "\n",
    "A hallucination is when the AI invents specific, false information:\n",
    "- Fabricated citations (papers, authors, journals that don't exist)\n",
    "- Invented statistics or numbers\n",
    "- Made-up historical facts or dates\n",
    "- False quotes attributed to real people\n",
    "\n",
    "**Not hallucinations:**\n",
    "- Vague or general statements\n",
    "- Correct refusals (\"I don't have access to real-time data\")\n",
    "- Acknowledged limitations\n",
    "\n",
    "### Time Budget\n",
    "\n",
    "Spend about **2 minutes per prompt**:\n",
    "- 1 minute: Search and verify claims\n",
    "- 1 minute: Record accuracy and error types\n",
    "\n",
    "**Total: ~16 minutes for all 8 prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Interface\n",
    "\n",
    "Run this cell to start verifying each prompt's accuracy.\n",
    "\n",
    "You'll see each prompt with its AI response characteristics, and you'll record the actual accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data storage\n",
    "evaluations_data = []\n",
    "\n",
    "# Accuracy options\n",
    "accuracy_options = [\n",
    "    'Select...',\n",
    "    'Fully accurate - no errors found',\n",
    "    'Mostly accurate - minor errors or imprecision',\n",
    "    'Partially accurate - significant errors or omissions',\n",
    "    'Inaccurate - major errors or hallucinations',\n",
    "    'Refused or unable to answer'\n",
    "]\n",
    "\n",
    "# Error type options\n",
    "error_types_options = [\n",
    "    'No errors',\n",
    "    'Factual error (wrong information)',\n",
    "    'Hallucinated citation/source',\n",
    "    'Logic error in reasoning',\n",
    "    'Outdated information',\n",
    "    'Overgeneralization',\n",
    "    'Incomplete answer',\n",
    "    'Misunderstood question'\n",
    "]\n",
    "\n",
    "def create_evaluation_interface(idx, row):\n",
    "    \"\"\"Create widgets for evaluating a single prompt.\"\"\"\n",
    "    \n",
    "    # Display prompt information and previous recording\n",
    "    print(\"=\"*70)\n",
    "    print(f\"PROMPT #{row['prompt_id']} of {len(data_df)}\")\n",
    "    print(f\"Category: {row['category_display']}\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"THE PROMPT:\")\n",
    "    print(\"-\"*70)\n",
    "    print(row['prompt_text'])\n",
    "    print(\"-\"*70)\n",
    "    print()\n",
    "    print(\"WHAT YOU RECORDED IN MODULE 1:\")\n",
    "    print(f\"  AI Confidence: {row['ai_confidence']}\")\n",
    "    print(f\"  Your Prediction: {row['student_prediction']}\")\n",
    "    if pd.notna(row['notes']) and row['notes'].strip():\n",
    "        print(f\"  Notes: {row['notes']}\")\n",
    "    print()\n",
    "    print(\"NOW: Verify the actual accuracy of the AI's response\")\n",
    "    print(\"Use Google, Wikipedia, or other sources to check the facts.\")\n",
    "    print()\n",
    "    \n",
    "    # Create widgets\n",
    "    accuracy_widget = widgets.Dropdown(\n",
    "        options=accuracy_options,\n",
    "        value='Select...',\n",
    "        description='Actual Accuracy:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '650px'}\n",
    "    )\n",
    "    \n",
    "    error_types_widget = widgets.SelectMultiple(\n",
    "        options=error_types_options,\n",
    "        value=[],\n",
    "        description='Error Types:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '650px', 'height': '150px'}\n",
    "    )\n",
    "    \n",
    "    error_details_widget = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Describe specific errors, hallucinations, or issues you found. For hallucinations, be specific (e.g., \"Invented citation: claimed paper by Smith et al. in Nature 2023, but no such paper exists\").', \n",
    "        description='Error Details:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '650px', 'height': '100px'}\n",
    "    )\n",
    "    \n",
    "    save_button = widgets.Button(\n",
    "        description=f'Save and Continue to Prompt #{row[\"prompt_id\"] + 1}' if idx < len(data_df) - 1 else 'Save Final Evaluation',\n",
    "        button_style='success',\n",
    "        layout={'width': '300px'}\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_save(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Validate inputs\n",
    "            if accuracy_widget.value == 'Select...':\n",
    "                print(\"⚠️ Please select an accuracy level before saving.\")\n",
    "                return\n",
    "            \n",
    "            # Save data\n",
    "            evaluations_data.append({\n",
    "                'prompt_id': row['prompt_id'],\n",
    "                'category': row['category'],\n",
    "                'actual_accuracy': accuracy_widget.value,\n",
    "                'error_types': '; '.join(error_types_widget.value) if error_types_widget.value else 'None',\n",
    "                'error_details': error_details_widget.value\n",
    "            })\n",
    "            \n",
    "            print(f\"✓ Prompt #{row['prompt_id']} evaluated!\")\n",
    "            print(f\"Progress: {len(evaluations_data)}/{len(data_df)} prompts completed\")\n",
    "            \n",
    "            # Show if prediction was correct\n",
    "            was_accurate = accuracy_widget.value in ['Fully accurate - no errors found', 'Mostly accurate - minor errors or imprecision']\n",
    "            was_confident = 'No caveats' in row['ai_confidence']\n",
    "            \n",
    "            if was_confident and not was_accurate:\n",
    "                print(\"⚠️ Overconfidence detected! AI was confident but inaccurate.\")\n",
    "            elif not was_confident and was_accurate:\n",
    "                print(\"ℹ️ Underconfidence: AI was cautious but actually accurate.\")\n",
    "            elif was_confident and was_accurate:\n",
    "                print(\"✓ Well-calibrated: AI was confident AND accurate.\")\n",
    "            \n",
    "            if idx < len(data_df) - 1:\n",
    "                print(f\"\\n→ Scroll down for Prompt #{row['prompt_id'] + 1}\")\n",
    "            else:\n",
    "                print(\"\\n✓ All evaluations completed!\")\n",
    "                print(\"→ Scroll down to save your data.\")\n",
    "    \n",
    "    save_button.on_click(on_save)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(accuracy_widget)\n",
    "    display(Markdown(\"*Hold Ctrl (Windows) or Cmd (Mac) to select multiple error types:*\"))\n",
    "    display(error_types_widget)\n",
    "    display(error_details_widget)\n",
    "    display(save_button)\n",
    "    display(output)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "# Display interface for each prompt\n",
    "for idx, row in data_df.iterrows():\n",
    "    create_evaluation_interface(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your Evaluations\n",
    "\n",
    "After completing all 8 evaluations above, run this cell to save your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(evaluations_data) < len(data_df):\n",
    "    print(f\"⚠️ WARNING: You've only completed {len(evaluations_data)}/{len(data_df)} evaluations.\")\n",
    "    print(\"Please complete all prompts above before saving.\")\n",
    "else:\n",
    "    # Create DataFrame\n",
    "    evaluations_df = pd.DataFrame(evaluations_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    evaluations_filename = f\"lab10_group_{group_code}_evaluations.csv\"\n",
    "    evaluations_df.to_csv(evaluations_filename, index=False)\n",
    "    \n",
    "    # Merge all data for analysis\n",
    "    complete_df = data_df.merge(evaluations_df, on=['prompt_id', 'category'])\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    total_prompts = len(complete_df)\n",
    "    accurate_prompts = len(complete_df[complete_df['actual_accuracy'].str.contains('accurate', case=False, na=False)])\n",
    "    confident_prompts = len(complete_df[complete_df['ai_confidence'].str.contains('No caveats', case=False, na=False)])\n",
    "    overconfident = len(complete_df[\n",
    "        complete_df['ai_confidence'].str.contains('No caveats', case=False, na=False) & \n",
    "        complete_df['actual_accuracy'].str.contains('Inaccurate|Partially accurate', case=False, na=False)\n",
    "    ])\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"=\"*60)\n",
    "    print(\"✓ Evaluations Saved Successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"File: {evaluations_filename}\")\n",
    "    print(f\"Prompts evaluated: {total_prompts}\")\n",
    "    print()\n",
    "    print(\"ACCURACY SUMMARY:\")\n",
    "    print(f\"  Accurate responses: {accurate_prompts}/{total_prompts} ({accurate_prompts/total_prompts*100:.1f}%)\")\n",
    "    print(f\"  Confident responses: {confident_prompts}/{total_prompts}\")\n",
    "    print(f\"  Overconfident responses: {overconfident}/{confident_prompts if confident_prompts > 0 else 1}\")\n",
    "    if confident_prompts > 0:\n",
    "        print(f\"  Overconfidence rate: {overconfident/confident_prompts*100:.1f}%\")\n",
    "    print()\n",
    "    print(\"Distribution of Actual Accuracy:\")\n",
    "    print(evaluations_df['actual_accuracy'].value_counts())\n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    print(\"Next Steps:\")\n",
    "    print(\"1. Answer Q8-Q12 on your lab handout\")\n",
    "    print(\"2. Continue to Module 3 for visualizations and analysis\")\n",
    "    print(\"3. Use the same group code in Module 3!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Your Complete Data\n",
    "\n",
    "Optional: View the merged dataset with predictions and evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(evaluations_data) == len(data_df):\n",
    "    complete_df = data_df.merge(pd.DataFrame(evaluations_data), on=['prompt_id', 'category'])\n",
    "    display_cols = ['prompt_id', 'category_display', 'ai_confidence', 'actual_accuracy', 'error_types']\n",
    "    display(HTML(complete_df[display_cols].to_html(index=False, classes='table table-striped')))\n",
    "else:\n",
    "    print(\"Complete all evaluations and save data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Module 2\n",
    "\n",
    "Answer these questions on your lab handout using the data you just collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Verification Method for Prompt #1\n",
    "\n",
    "For Prompt #1, was the AI's response actually accurate? How did you verify this? (What sources did you use?)\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Overconfidence Example\n",
    "\n",
    "Identify ONE prompt where the AI was confident but made errors. What prompt was it? What went wrong?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Uncertainty and Accuracy\n",
    "\n",
    "Identify ONE prompt where the AI expressed uncertainty (caveats or refusal). Was the AI actually accurate or inaccurate on that prompt?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Hallucination Example\n",
    "\n",
    "Did you find any \"hallucinated\" information—specific false details the AI presented as fact? Give a concrete example.\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: Category with Most Errors\n",
    "\n",
    "Which category of prompts led to the most errors in your group's data? Why do you think this category is difficult for AI?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q8-Q12** on your lab handout\n",
    "2. **Remember your group code**\n",
    "3. **Continue to Module 3** where you'll visualize patterns of overconfidence\n",
    "4. **Use the same group code** in Module 3\n",
    "\n",
    "In Module 3, you'll see the big picture - how often confidence matched accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
