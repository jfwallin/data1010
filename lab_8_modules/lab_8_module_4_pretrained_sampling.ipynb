{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8, Module 4: Professional Diffusion Models\n",
    "\n",
    "**Estimated time:** 12-15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: From Toy to Professional**\n",
    "\n",
    "In Modules 2-3, you trained a toy model (~100k parameters) that generates 16Ã—16 grayscale digits.\n",
    "\n",
    "The results were recognizable but blurry. Why?\n",
    "- **Small model:** Only 100,000 parameters\n",
    "- **Limited data:** 24,000 MNIST digits\n",
    "- **Low resolution:** 16Ã—16 pixels\n",
    "- **Short training:** 2-3 minutes\n",
    "\n",
    "Now you'll see what happens when we **scale up**!\n",
    "\n",
    "In this module, you'll:\n",
    "- Use a **pre-trained professional model** (~30 million parameters)\n",
    "- Generate **32Ã—32 RGB images** (full color!)\n",
    "- Compare **inference steps** (25 vs 50 vs 100)\n",
    "- Understand the **quality-speed trade-off**\n",
    "- Learn how to bridge to **text-to-image models** (DALL-E, Stable Diffusion)\n",
    "\n",
    "**Most importantly:** You'll see that professional models use the **EXACT SAME algorithm** you implemented in Modules 2-3!\n",
    "\n",
    "The difference is just **scale**: more data, bigger model, longer training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ **A Note About PyTorch**\n",
    "\n",
    "In Modules 1-3, we used **Keras/TensorFlow** to build our toy diffusion model.\n",
    "\n",
    "In Module 4, we'll use **PyTorch** insteadâ€”another popular deep learning framework.\n",
    "\n",
    "**Why the switch?**\n",
    "- The Hugging Face `diffusers` library (industry standard) works best with PyTorch\n",
    "- PyTorch and TensorFlow are just different tools for the same job (like Word vs. Google Docs)\n",
    "- Both frameworks implement the same concepts: tensors, gradients, neural networks\n",
    "\n",
    "**Don't worry!** You don't need to understand PyTorch syntax. We're using it like a black boxâ€”the same diffusion concepts from Modules 1-3 still apply!\n",
    "\n",
    "**The key point:** The algorithm (DDPM reverse diffusion) is identical. Only the syntax changes slightly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ **Setup: Install and Import Libraries**\n",
    "\n",
    "First, let's install the `diffusers` library and import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the diffusers library (takes ~30 seconds)\n",
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from diffusers import DDPMPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(\"âœ… Libraries imported!\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nðŸ–¥ï¸  Using device: {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"\\nâš ï¸  Warning: Running on CPU will be slower (~2-3 minutes per generation)\")\n",
    "    print(\"    Consider enabling GPU in Runtime > Change runtime type > T4 GPU\")\n",
    "else:\n",
    "    print(\"\\nðŸš€ GPU detected! Generation will be fast (~30-60 seconds per batch).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¥ **Load Pre-Trained Model**\n",
    "\n",
    "We'll load a professional diffusion model trained on **CIFAR-10**:\n",
    "- **Dataset:** 60,000 images of 10 object categories\n",
    "- **Categories:** airplane, car, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "- **Resolution:** 32Ã—32 RGB (color images)\n",
    "- **Parameters:** ~30 million (300Ã— more than our toy model!)\n",
    "- **Training time:** Days/weeks on multiple GPUs\n",
    "\n",
    "This model was trained by Google and is available on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained CIFAR-10 diffusion model (~200 MB download)\n",
    "print(\"ðŸ“¥ Downloading model (this may take 1-2 minutes)...\\n\")\n",
    "pipeline = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "print(\"\\nâœ… Model loaded successfully!\")\n",
    "print(f\"\\nðŸ“Š Model details:\")\n",
    "print(f\"   - Training dataset: CIFAR-10 (60,000 images)\")\n",
    "print(f\"   - Image size: 32Ã—32 pixels (RGB color)\")\n",
    "print(f\"   - Object classes: 10 (airplane, car, bird, cat, deer, dog, frog, horse, ship, truck)\")\n",
    "print(f\"   - Parameters: ~30 million (vs. ~100,000 in our toy model!)\")\n",
    "print(f\"\\nðŸ’¡ This model uses the SAME DDPM algorithm you implemented in Module 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¨ **Generate Images from Noise**\n",
    "\n",
    "Let's generate a batch of 16 images from pure random noise!\n",
    "\n",
    "**What's happening behind the scenes:**\n",
    "1. Start with 16 random noise tensors (32Ã—32Ã—3 each)\n",
    "2. Run 50 denoising steps (same reverse diffusion you implemented!)\n",
    "3. Each step: predict noise, remove it, add tiny random noise\n",
    "4. Result: 16 generated images of random objects\n",
    "\n",
    "This takes ~30-60 seconds with GPU, 2-3 minutes with CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of 16 images\n",
    "print(\"ðŸŽ¨ Generating 16 images from pure noise...\")\n",
    "print(\"   (This takes ~30-60 seconds with GPU, 2-3 minutes with CPU)\\n\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate images using 50 denoising steps\n",
    "output = pipeline(\n",
    "    batch_size=16,\n",
    "    num_inference_steps=50,\n",
    "    output_type=\"pil\"\n",
    ")\n",
    "images = output.images\n",
    "\n",
    "print(\"\\nâœ… Generation complete!\")\n",
    "print(f\"   Generated {len(images)} images\")\n",
    "print(f\"   Each image: 32Ã—32 pixels, RGB color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¸ **Visualize Generated Images**\n",
    "\n",
    "Let's see what the model created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display in 4Ã—4 grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "fig.suptitle(\"Professional Diffusion Model: CIFAR-10 Generation\\n(50 denoising steps)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[idx])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Sample {idx+1}\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Observations:\")\n",
    "print(\"   - Images are 32Ã—32 RGB (color)\")\n",
    "print(\"   - Objects are recognizable (cars, planes, animals)\")\n",
    "print(\"   - Quality is MUCH better than our 16Ã—16 toy model\")\n",
    "print(\"   - This model trained on 60,000 images for days/weeks\")\n",
    "print(\"\\nâœ“ Same algorithm, dramatically better results due to scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ **Question Q21 (Observation)**\n",
    "\n",
    "### **Q21. Look at the 16 generated images. How many distinct object categories can you identify?**\n",
    "\n",
    "*(Hint: CIFAR-10 has 10 classes: airplane, car, bird, cat, deer, dog, frog, horse, ship, truck)*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ **Experiment: Step Count Comparison**\n",
    "\n",
    "In Module 3, you saw that more denoising steps generally improve quality.\n",
    "\n",
    "Let's test this with the professional model!\n",
    "\n",
    "**Question:** Does 100 steps produce noticeably better results than 25 steps?\n",
    "\n",
    "**Trade-off:**\n",
    "- **Fewer steps:** Faster generation, possibly lower quality\n",
    "- **More steps:** Slower generation, possibly better quality\n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different numbers of denoising steps\n",
    "step_counts = [25, 50, 100]\n",
    "comparison_images = []\n",
    "\n",
    "print(\"ðŸ”¬ Experiment: How do step counts affect quality?\\n\")\n",
    "\n",
    "for steps in step_counts:\n",
    "    print(f\"Generating with {steps} steps...\", end='\\r')\n",
    "    torch.manual_seed(42)  # Same seed for fair comparison\n",
    "    output = pipeline(\n",
    "        batch_size=1,\n",
    "        num_inference_steps=steps,\n",
    "        output_type=\"pil\"\n",
    "    )\n",
    "    comparison_images.append(output.images[0])\n",
    "    print(f\"   âœ“ Done ({steps} steps)\")\n",
    "\n",
    "print(\"\\nâœ… Comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š **Visualize Step Comparison**\n",
    "\n",
    "Let's see the quality differences side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.suptitle(\"Quality vs. Speed Trade-off: Inference Steps\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, (steps, img) in enumerate(zip(step_counts, comparison_images)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"{steps} steps\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "    # Color-coded borders\n",
    "    if steps == 25:\n",
    "        color = 'orange'\n",
    "        label = 'Faster, lower quality'\n",
    "    elif steps == 50:\n",
    "        color = 'green'\n",
    "        label = 'Balanced'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "        label = 'Slower, best quality'\n",
    "\n",
    "    for spine in axes[idx].spines.values():\n",
    "        spine.set_edgecolor(color)\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    axes[idx].text(0.5, -0.15, label, transform=axes[idx].transAxes,\n",
    "                   ha='center', fontsize=9, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"   - More steps = higher quality but slower generation\")\n",
    "print(\"   - 25 steps: Fast but slightly noisier (~10-15 seconds)\")\n",
    "print(\"   - 50 steps: Good balance (~30-60 seconds)\")\n",
    "print(\"   - 100 steps: Best quality but slowest (~1-2 minutes)\")\n",
    "print(\"   - DALL-E/Stable Diffusion typically use 20-50 steps\")\n",
    "print(\"\\nâœ“ Diminishing returns: 50â†’100 steps improves less than 25â†’50 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ **Questions Q22-Q23 (Analysis)**\n",
    "\n",
    "### **Q22. Compare the images generated with 25, 50, and 100 steps. Describe the quality differences you observe. Is the improvement from 50â†’100 steps as noticeable as 25â†’50 steps?**\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q23. Why might DALL-E and Stable Diffusion use 20-50 steps instead of 100+ steps? What's the trade-off?**\n",
    "\n",
    "*(Hint: Consider both user experience and computational cost)*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š **Comparison: Toy Model vs. Professional Model**\n",
    "\n",
    "Let's compare what you built in Modules 2-3 to this pre-trained professional model:\n",
    "\n",
    "| Aspect | Module 3 Toy Model | Module 4 Professional |\n",
    "|--------|-------------------|----------------------|\n",
    "| **Training Data** | 24,000 MNIST digits | 60,000 CIFAR-10 images |\n",
    "| **Image Size** | 16Ã—16 grayscale | 32Ã—32 RGB color |\n",
    "| **Channels** | 1 (grayscale) | 3 (red, green, blue) |\n",
    "| **Classes** | 4 digits (0-3) | 10 object categories |\n",
    "| **Parameters** | ~100,000 | ~30 million |\n",
    "| **Training Time** | 2-3 minutes (Colab T4) | Days/weeks (multiple GPUs) |\n",
    "| **Quality** | Blurry but recognizable | Sharp, detailed objects |\n",
    "| **Algorithm** | DDPM reverse diffusion | **SAME ALGORITHM!** |\n",
    "\n",
    "### **The Key Takeaway:**\n",
    "- âœ… You implemented the **EXACT SAME algorithm** as professional models\n",
    "- âœ… The difference is **scale**: more data, bigger model, longer training\n",
    "- âœ… The core diffusion process (forward noise, reverse denoising) is **identical**!\n",
    "\n",
    "**This is incredibly powerful:**\n",
    "- You understand the foundation of DALL-E, Midjourney, and Stable Diffusion\n",
    "- The algorithm doesn't changeâ€”only the scale increases\n",
    "- Professional models just have billions of parameters instead of millions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ **Question Q24 (Synthesis)**\n",
    "\n",
    "### **Q24. Looking at the comparison table, what specific factors contribute most to the quality difference between your toy model and the professional CIFAR-10 model?**\n",
    "\n",
    "*(Consider: parameters, training data, resolution, training timeâ€”which matter most?)*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ‰ **From CIFAR-10 to DALL-E**\n",
    "\n",
    "### **What We've Seen So Far**\n",
    "\n",
    "**Module 3 & 4: Unconditional Generation**\n",
    "- **Input:** Pure random noise\n",
    "- **Output:** Random image from learned distribution\n",
    "- **No control** over what gets generated\n",
    "\n",
    "**Example:** CIFAR-10 model generates random objects (could be a car, a bird, a cat, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### **What DALL-E and Stable Diffusion Add**\n",
    "\n",
    "**Text-to-Image: Conditional Generation**\n",
    "- **Input:** Text prompt (\"a cat wearing an astronaut suit on Mars\")\n",
    "- **Output:** Image matching the description\n",
    "- **Full creative control**\n",
    "\n",
    "---\n",
    "\n",
    "### **How Text Conditioning Works (Simplified)**\n",
    "\n",
    "```\n",
    "1. Text Encoder: \"a red sports car\" â†’ embedding vector (list of numbers)\n",
    "2. Diffusion Model: Uses text embedding to guide denoising\n",
    "   - At each step, the model asks: \"Does this look like 'a red sports car'?\"\n",
    "   - Adjusts denoising to match the text description\n",
    "3. Output: Image of a red sports car\n",
    "```\n",
    "\n",
    "**Three-Tier Explanation:**\n",
    "\n",
    "**Plain English:**  \n",
    "The model learns to ask \"What object am I making?\" at each denoising step, using the text description as a guide.\n",
    "\n",
    "**Symbols:**  \n",
    "x_{t-1} = Denoise(x_t, t, text_embedding)\n",
    "\n",
    "**Pseudocode:**\n",
    "```python\n",
    "for t in reversed(range(T)):\n",
    "    noise_prediction = model(noisy_image, timestep=t, text_embedding=text_embed)\n",
    "    noisy_image = denoise_one_step(noisy_image, noise_prediction)\n",
    "```\n",
    "\n",
    "**The key addition:** The denoising model receives **two inputs** instead of one:\n",
    "1. Noisy image (x_t)\n",
    "2. **Text embedding** (what we want to generate)\n",
    "\n",
    "The model learns: \"Remove noise in a way that makes this look like [text description]\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Scaling Up: DALL-E vs. Our Models**\n",
    "\n",
    "| Model | Parameters | Training Data | Capabilities |\n",
    "|-------|-----------|---------------|--------------||\n",
    "| **Our Toy Model (Module 3)** | 100K | 24K MNIST digits | Generates blurry 0-3 digits |\n",
    "| **CIFAR-10 (Module 4)** | 30M | 60K small objects | Generates 32Ã—32 objects |\n",
    "| **Stable Diffusion** | 890M | 2B+ text-image pairs | Generates 512Ã—512 images from text |\n",
    "| **DALL-E 3** | ~10B+ | Billions of images | Photorealistic images from complex prompts |\n",
    "\n",
    "**Same algorithm, different scale!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ **Real-World Applications**\n",
    "\n",
    "Diffusion models power many cutting-edge AI systems:\n",
    "\n",
    "### **Creative Applications**\n",
    "- **DALL-E 3** (OpenAI): Text-to-image generation with unprecedented quality\n",
    "- **Midjourney**: Artistic image generation favored by designers and artists\n",
    "- **Stable Diffusion**: Open-source text-to-image (anyone can run it!)\n",
    "- **Adobe Firefly**: Creative tools integrated into Photoshop and Illustrator\n",
    "\n",
    "### **Beyond Images**\n",
    "- **Video generation:** Runway, Pika Labs, Sora (text-to-video)\n",
    "- **Medical imaging:** Enhancing MRI/CT scans, generating synthetic training data\n",
    "- **Drug discovery:** Generating novel molecular structures for pharmaceuticals\n",
    "- **Audio:** Music generation, voice synthesis, sound effects\n",
    "- **3D models:** Generating 3D objects and scenes from text descriptions\n",
    "\n",
    "### **Why Diffusion Models Matter**\n",
    "- **High quality:** Better than previous generative methods (GANs, VAEs)\n",
    "- **Versatile:** Work for images, video, audio, 3D, molecules, and more\n",
    "- **Controllable:** Text conditioning provides precise creative control\n",
    "- **Iterative:** Step-by-step refinement produces detailed, high-quality results\n",
    "- **Stable training:** Easier to train than GANs, more reliable convergence\n",
    "\n",
    "### **The Diffusion Revolution**\n",
    "- **2020:** DDPM paper introduces the algorithm you learned\n",
    "- **2022:** DALL-E 2 and Stable Diffusion launch publicly\n",
    "- **2023:** Billions of images generated, mainstream adoption\n",
    "- **2024:** Video generation (Sora), 3D generation, and beyond\n",
    "\n",
    "**You now understand the algorithm powering this revolution!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ **Question Q25 (Synthesis & Reflection)**\n",
    "\n",
    "### **Q25. In your own words, explain the complete diffusion pipeline: How does a model like DALL-E go from a text prompt ('a cat in a spacesuit') to a final image? Reference concepts from Modules 0-4.**\n",
    "\n",
    "*(Think about: text encoding, noise initialization, reverse diffusion, text conditioning)*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… **Module 4 Complete!**\n",
    "\n",
    "### **What You Learned**\n",
    "\n",
    "In this module, you:\n",
    "- âœ… Used a professional pre-trained diffusion model (30M parameters)\n",
    "- âœ… Generated high-quality 32Ã—32 RGB images\n",
    "- âœ… Compared inference steps (25 vs 50 vs 100)\n",
    "- âœ… Saw the quality improvement from scale\n",
    "- âœ… Learned how text-to-image conditioning works\n",
    "- âœ… Connected to DALL-E, Stable Diffusion, and real-world applications\n",
    "\n",
    "### **Key Insights**\n",
    "\n",
    "> **The algorithm you implemented in Modules 2-3 is the SAME as DALL-E and Stable Diffusion!**\n",
    ">\n",
    "> The difference is scale:\n",
    "> - More training data (billions vs. thousands)\n",
    "> - Bigger models (billions vs. millions of parameters)\n",
    "> - Longer training (weeks/months vs. minutes)\n",
    "> - Text conditioning (guides generation toward descriptions)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ **Lab 8 Complete!**\n",
    "\n",
    "Congratulations! You've completed all 5 modules of Lab 8!\n",
    "\n",
    "### **Your Journey:**\n",
    "\n",
    "1. **Module 0:** Learned what diffusion models are (conceptual foundation)\n",
    "2. **Module 1:** Saw how forward diffusion destroys information progressively\n",
    "3. **Module 2:** Trained your own denoiser using a simplified U-Net\n",
    "4. **Module 3:** Implemented reverse diffusion to generate images from noise\n",
    "5. **Module 4:** Used a professional model and bridged to DALL-E\n",
    "\n",
    "### **What You Now Understand:**\n",
    "\n",
    "- âœ… **Forward diffusion:** Gradually adding noise destroys information\n",
    "- âœ… **Denoising networks:** U-Net architectures predict noise at each timestep\n",
    "- âœ… **Reverse diffusion:** Iterative denoising creates images from noise\n",
    "- âœ… **Noise schedules:** Control the rate of information destruction/reconstruction\n",
    "- âœ… **Stochasticity:** Random noise additions create variety in generations\n",
    "- âœ… **Scaling:** More data + bigger models = dramatically better quality\n",
    "- âœ… **Text conditioning:** Guides generation toward specific concepts\n",
    "- âœ… **Real-world impact:** Powers DALL-E, Midjourney, Stable Diffusion, and more\n",
    "\n",
    "### **Connection to Lab 7 (CNNs):**\n",
    "\n",
    "```\n",
    "Lab 7 (CNNs):      Image â†’ Features â†’ Classification\n",
    "                   (Analysis: \"What is this?\")\n",
    "\n",
    "Lab 8 (Diffusion): Noise â†’ Features â†’ Image\n",
    "                   (Synthesis: \"Create this!\")\n",
    "```\n",
    "\n",
    "**Both use convolutional architecturesâ€”one for understanding, one for creating!**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ **Final Thought**\n",
    "\n",
    "**From noise to images, you've mastered the technology behind DALL-E!** ðŸŽ¨\n",
    "\n",
    "You started with pure random noise and learned to gradually sculpt it into recognizable imagesâ€”the same process used by the most advanced AI image generators in the world.\n",
    "\n",
    "The only difference between your toy model and DALL-E is **scale**. You have the **knowledge**. With more computational resources, more data, and more training time, you could build the next generation of creative AI tools!\n",
    "\n",
    "**What's next?**\n",
    "- Explore Stable Diffusion and try generating your own images\n",
    "- Learn about other generative models (GANs, VAEs)\n",
    "- Study advanced diffusion techniques (latent diffusion, classifier-free guidance)\n",
    "- Build your own creative AI applications\n",
    "\n",
    "**Thank you for completing Lab 8!** ðŸš€\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
