{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 \u2013 Module 3: Building a Perceptron\n",
    "\n",
    "**Time:** ~5 minutes\n",
    "\n",
    "---\n",
    "\n",
    "> **KEY IDEA**  \n",
    "> A perceptron is the single building block of every neural network.\n",
    "> It is tiny and simple on its own \u2014 but millions of them, connected in layers, power systems like ChatGPT and image recognition.  \n",
    ">\n",
    "> Before answering, look closely at the two-step diagram below.\n",
    "> Make sure you can trace what happens to a number as it moves through the perceptron from input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Two-Step Diagram\n",
    "\n",
    "A perceptron does exactly two things, one after the other:\n",
    "\n",
    "```\n",
    "         STEP 1                          STEP 2\n",
    "     Weighted Sum                     Activation\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502                        \u2502   \u2502                        \u2502\n",
    "  \u2502  z = w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2502 \u2500\u25b6 \u2502  output = activation(z) \u2502 \u2500\u25b6 answer\n",
    "  \u2502                        \u2502   \u2502                        \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "  x\u2081, x\u2082  =  the two inputs (coordinates of a data point)\n",
    "  w\u2081, w\u2082  =  weights   (how much each input matters)\n",
    "  b       =  bias      (shifts the dividing line left or right)\n",
    "  z       =  a single number that says \"which side of the line is this point on?\"\n",
    "```\n",
    "\n",
    "**Step 1** is just multiplication and addition \u2014 the same kind of line equation you used in Lab 1.  \n",
    "**Step 2** is the activation function from Modules 1\u20132. It introduces the *bend* that lets the perceptron handle more than just straight lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Run this cell to create a dataset and the interactive perceptron tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FloatSlider, Dropdown, interact\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def step_fn(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Two datasets: one diagonal, one vertical\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "\n",
    "# Dataset A \u2013 Diagonal separation\n",
    "Xa = np.vstack([\n",
    "    np.random.randn(n, 2) * 0.5 + [-1.5, -1.5],\n",
    "    np.random.randn(n, 2) * 0.5 + [ 1.5,  1.5]\n",
    "])\n",
    "ya = np.hstack([np.zeros(n), np.ones(n)])\n",
    "\n",
    "# Dataset B \u2013 Vertical separation (left vs right)\n",
    "Xb = np.vstack([\n",
    "    np.random.randn(n, 2) * 0.6 + [-1.8, 0],\n",
    "    np.random.randn(n, 2) * 0.6 + [ 1.8, 0]\n",
    "])\n",
    "yb = np.hstack([np.zeros(n), np.ones(n)])\n",
    "\n",
    "datasets = {\n",
    "    'Diagonal': (Xa, ya),\n",
    "    'Left vs Right': (Xb, yb),\n",
    "}\n",
    "\n",
    "print('Setup complete! Two datasets ready.')\n",
    "print('  Diagonal      \\u2014 blue lower-left, red upper-right')\n",
    "print('  Left vs Right \\u2014 blue on the left, red on the right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Perceptron\n",
    "\n",
    "Adjust the **weights** (w\\u2081, w\\u2082) and **bias** (b) to move the green decision boundary so it separates blue from red.  \n",
    "The background color shows which region the perceptron predicts as each class.\n",
    "\n",
    "**Experiments to try:**\n",
    "1. Start with the **Diagonal** dataset. Try to reach 100\u2009%.\n",
    "2. Switch to **Left vs Right**. Which weight matters more now?\n",
    "3. Change **only the bias** (leave weights fixed) and watch the boundary slide.\n",
    "4. Change **only a weight** (leave bias fixed) and watch the boundary tilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_viz(dataset_name, w1, w2, b, activation_name):\n",
    "    X, y_true = datasets[dataset_name]\n",
    "    act = {'Sigmoid': sigmoid, 'ReLU': relu, 'Step': step_fn}[activation_name]\n",
    "    threshold = 0.5 if activation_name == 'Sigmoid' else 0.5\n",
    "\n",
    "    # Decision surface on a grid\n",
    "    g = np.linspace(-4, 4, 200)\n",
    "    G1, G2 = np.meshgrid(g, g)\n",
    "    Z_grid = act(w1 * G1 + w2 * G2 + b)\n",
    "    pred_grid = (Z_grid > threshold).astype(int)\n",
    "\n",
    "    # Classify data points\n",
    "    z_data = act(w1 * X[:, 0] + w2 * X[:, 1] + b)\n",
    "    pred_data = (z_data > threshold).astype(int)\n",
    "    correct = pred_data == y_true\n",
    "    acc = correct.mean() * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), dpi=100)\n",
    "    ax.contourf(G1, G2, pred_grid, levels=[-0.5, 0.5, 1.5],\n",
    "                colors=['#cce0ff', '#ffcccc'], alpha=0.35)\n",
    "    ax.contour(G1, G2, pred_grid, levels=[0.5], colors=['green'], linewidths=3)\n",
    "\n",
    "    # Correct points\n",
    "    ax.scatter(X[correct & (y_true == 0), 0], X[correct & (y_true == 0), 1],\n",
    "              c='blue', s=80, alpha=0.7, edgecolors='k', lw=1.2, label='Class 0')\n",
    "    ax.scatter(X[correct & (y_true == 1), 0], X[correct & (y_true == 1), 1],\n",
    "              c='red',  s=80, alpha=0.7, edgecolors='k', lw=1.2, label='Class 1')\n",
    "    # Misclassified\n",
    "    if not correct.all():\n",
    "        ax.scatter(X[~correct, 0], X[~correct, 1],\n",
    "                  c='yellow', s=130, marker='X', edgecolors='red', lw=3, label='Wrong')\n",
    "\n",
    "    ax.set_title(f'{dataset_name}  |  {activation_name}  |  Accuracy: {acc:.0f}%',\n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('x\\u2081'); ax.set_ylabel('x\\u2082')\n",
    "    ax.legend(fontsize=10, loc='upper left')\n",
    "    ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\n",
    "    ax.set_aspect('equal'); ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(f'Perceptron:  z = {w1:.1f} x\\u2081 + {w2:.1f} x\\u2082 + {b:.1f}  \\u2192  {activation_name}(z)')\n",
    "    if acc == 100:\n",
    "        print('Perfect! Try the other dataset or a different activation.')\n",
    "\n",
    "interact(\n",
    "    perceptron_viz,\n",
    "    dataset_name=Dropdown(options=list(datasets.keys()), value='Diagonal',\n",
    "                          description='Dataset:'),\n",
    "    w1=FloatSlider(min=-3, max=3, step=0.1, value=1.0, description='Weight w\\u2081:',\n",
    "                   continuous_update=False),\n",
    "    w2=FloatSlider(min=-3, max=3, step=0.1, value=1.0, description='Weight w\\u2082:',\n",
    "                   continuous_update=False),\n",
    "    b=FloatSlider(min=-5, max=5, step=0.1, value=0.0,  description='Bias b:',\n",
    "                  continuous_update=False),\n",
    "    activation_name=Dropdown(options=['Sigmoid', 'ReLU', 'Step'], value='Sigmoid',\n",
    "                             description='Activation:')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What the Parameters Control\n",
    "\n",
    "| Parameter | What it does |\n",
    "|-----------|-------------|\n",
    "| **Weights** (w\\u2081, w\\u2082) | Tilt / rotate the decision boundary. Making w\\u2081 large and w\\u2082 small creates a mostly vertical boundary (x\\u2081 matters more). Equal weights make a diagonal boundary. |\n",
    "| **Bias** (b) | Slide the boundary left/right without changing its angle. Positive bias pushes it one way, negative pushes the other. |\n",
    "| **Activation** | For these simple datasets, all activations give the same accuracy \u2014 because the data is already separable by a straight line. The activation matters more when things get complicated (Module 4). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\u2011Sheet Questions (Q10 \u2013 Q12)\n",
    "\n",
    "**Q10.** Look at the two-step diagram in Section 1. Without using any math, describe each step in plain language \u2014 what goes in, what happens, and what comes out?\n",
    "\n",
    "**Q11.** Try adjusting **only the weights** while keeping the bias fixed. What changes about the decision boundary? Now try adjusting **only the bias**. What changes? Describe the difference between what each one controls.\n",
    "\n",
    "**Q12.** You\u2019ve now seen activation functions bend space (Module 1) and a perceptron combine weights, bias, and an activation function (this module). Where exactly in the perceptron does the \u201cbending\u201d happen \u2014 Step 1 or Step 2? Why does that matter for what kinds of patterns the perceptron can separate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next:** Continue to **Module 4** to test the perceptron on the *impossible* patterns from Module 0 and discover its limits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}