{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Module 3: Learning Rate Exploration\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the critical role of learning rate in GD\n",
    "- Predict and observe: slow convergence, fast convergence, oscillation, divergence\n",
    "- Develop intuition for choosing learning rates\n",
    "- Recognize learning rate as a crucial hyperparameter\n",
    "\n",
    "**Time:** ~20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "This module focuses entirely on **learning rate** - the most important hyperparameter in gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Learning Rate Matters\n",
    "\n",
    "The learning rate controls the **step size** in gradient descent:\n",
    "\n",
    "```\n",
    "new = old - learning_rate × gradient\n",
    "```\n",
    "\n",
    "### The Goldilocks Problem:\n",
    "\n",
    "- **Too small:** Convergence is painfully slow (wastes computation)\n",
    "- **Too large:** Oscillation or divergence (never reaches minimum)\n",
    "- **Just right:** Fast and stable convergence\n",
    "\n",
    "Finding the \"just right\" learning rate is an art and science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: A Simple Test Function\n",
    "\n",
    "We'll use a simple 1D quadratic function for clear visualization:\n",
    "\n",
    "```\n",
    "f(x) = 0.5 × x²\n",
    "```\n",
    "\n",
    "Minimum at x = 0, where f(0) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatText, Button, Output, VBox, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Simple quadratic function\n",
    "def test_function(x):\n",
    "    \"\"\"f(x) = 0.5 * x^2\"\"\"\n",
    "    return 0.5 * x**2\n",
    "\n",
    "def compute_gradient(x, func, h=1e-5):\n",
    "    \"\"\"Numerical gradient\"\"\"\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "def run_gd_with_lr(x_start, learning_rate, max_steps=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Run gradient descent and return full history.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'x', 'f', 'grad', 'converged', 'n_steps', 'status'\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'x': [x_start],\n",
    "        'f': [test_function(x_start)],\n",
    "        'grad': [],\n",
    "        'converged': False,\n",
    "        'n_steps': 0,\n",
    "        'status': 'unknown'\n",
    "    }\n",
    "    \n",
    "    x_current = x_start\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        grad = compute_gradient(x_current, test_function)\n",
    "        history['grad'].append(grad)\n",
    "        \n",
    "        # GD update\n",
    "        x_new = x_current - learning_rate * grad\n",
    "        f_new = test_function(x_new)\n",
    "        \n",
    "        history['x'].append(x_new)\n",
    "        history['f'].append(f_new)\n",
    "        history['n_steps'] = step + 1\n",
    "        \n",
    "        # Check for divergence\n",
    "        if abs(x_new) > 100 or abs(f_new) > 1000:\n",
    "            history['status'] = 'diverged'\n",
    "            break\n",
    "        \n",
    "        # Check for convergence\n",
    "        if abs(f_new - history['f'][-2]) < tol:\n",
    "            history['converged'] = True\n",
    "            history['status'] = 'converged'\n",
    "            break\n",
    "        \n",
    "        x_current = x_new\n",
    "    \n",
    "    # Determine status if not already set\n",
    "    if history['status'] == 'unknown':\n",
    "        if history['converged']:\n",
    "            history['status'] = 'converged'\n",
    "        elif len(history['x']) >= 10:\n",
    "            # Check for oscillation: if last few steps bounce around\n",
    "            recent_x = history['x'][-10:]\n",
    "            if max(recent_x) - min(recent_x) > 0.1 * abs(x_start):\n",
    "                history['status'] = 'oscillating'\n",
    "            else:\n",
    "                history['status'] = 'slow_convergence'\n",
    "        else:\n",
    "            history['status'] = 'in_progress'\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"✓ Test function and GD utilities ready\")\n",
    "print(f\"\\nTest function: f(x) = 0.5 × x²\")\n",
    "print(f\"Minimum at x = 0, f(0) = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Prediction Questions (Answer BEFORE running experiments)\n\n**Q10 (PREDICTION):** Starting from x = 10.0, predict what will happen with:\n\n1. **LR = 0.001** (very small):\n   - Will it converge in 100 steps?\n   - Roughly how many steps would it need?\n   - Will the path be smooth?\n\n2. **LR = 0.1** (moderate):\n   - Will it converge quickly?\n   - How many steps approximately?\n   - Will it be stable?\n\n3. **LR = 0.8** (large):\n   - Will it converge?\n   - Will it oscillate (bounce back and forth)?\n   - Will it diverge (explode)?\n\n4. **LR = 3.0** (very large):\n   - Will it converge at all?\n   - What do you expect to happen?\n   - How quickly will it diverge?\n\nWrite your predictions on the answer sheet before continuing!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Four Learning Rates: Side-by-Side Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Starting point\nx_start = 10.0\n\n# Four learning rates to compare\nlearning_rates = [0.001, 0.1, 0.8, 3.0]\nlr_labels = ['0.001 (too small)', '0.1 (just right)', '0.8 (too large)', '3.0 (way too large)']\ncolors = ['blue', 'green', 'orange', 'red']\n\n# Run GD for each learning rate\nresults = {}\nfor lr in learning_rates:\n    results[lr] = run_gd_with_lr(x_start, lr, max_steps=100)\n\nprint(\"Running gradient descent with four learning rates...\")\nprint(\"=\"*80)\nprint(f\"Starting point: x = {x_start}, f(x) = {test_function(x_start):.2f}\\n\")\n\nfor lr, label in zip(learning_rates, lr_labels):\n    res = results[lr]\n    print(f\"Learning Rate = {label}:\")\n    print(f\"  Status: {res['status']}\")\n    print(f\"  Steps taken: {res['n_steps']}\")\n    print(f\"  Final x: {res['x'][-1]:.6f}\")\n    print(f\"  Final f(x): {res['f'][-1]:.6f}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Visualization: Four Scenarios"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create four subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 10), dpi=100)\naxes = axes.flatten()\n\n# Plot function curve\nx_plot = np.linspace(-12, 12, 300)\nf_plot = test_function(x_plot)\n\nfor idx, (lr, label, color) in enumerate(zip(learning_rates, lr_labels, colors)):\n    ax = axes[idx]\n    res = results[lr]\n    \n    # Plot function\n    ax.plot(x_plot, f_plot, 'b-', linewidth=2, alpha=0.3, label='f(x) = 0.5x²')\n    \n    # Plot GD path\n    x_hist = np.array(res['x'])\n    f_hist = np.array(res['f'])\n    \n    # Color gradient for path\n    for i in range(len(x_hist)):\n        marker = 'o' if i == 0 else ('*' if i == len(x_hist)-1 else 'o')\n        size = 150 if i == 0 or i == len(x_hist)-1 else 60\n        alpha = 1.0 if i == 0 or i == len(x_hist)-1 else 0.5\n        \n        ax.scatter(x_hist[i], f_hist[i], c=color, marker=marker, \n                  s=size, alpha=alpha, edgecolors='black', linewidths=1.5, zorder=3)\n    \n    # Add arrows\n    arrow_freq = max(1, len(x_hist) // 10)\n    for i in range(0, min(len(x_hist)-1, 30), arrow_freq):\n        ax.annotate('', xy=(x_hist[i+1], f_hist[i+1]), \n                   xytext=(x_hist[i], f_hist[i]),\n                   arrowprops=dict(arrowstyle='->', color=color, lw=1.5, alpha=0.6))\n    \n    ax.set_xlabel('x', fontsize=11)\n    ax.set_ylabel('f(x)', fontsize=11)\n    ax.set_title(f'LR = {label}\\n{res[\"status\"]}', fontsize=11, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(-12, 12)\n    \n    # Adjust ylim based on divergence\n    if res['status'] == 'diverged':\n        ax.set_ylim(-5, min(200, max(f_hist) + 20))\n    else:\n        ax.set_ylim(-5, min(60, max(f_hist) + 5))\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convergence Curves: Loss Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n\n# Left: Linear scale\nfor lr, label, color in zip(learning_rates, lr_labels, colors):\n    res = results[lr]\n    ax1.plot(range(len(res['f'])), res['f'], 'o-', \n            color=color, label=f'LR = {label}', linewidth=2, markersize=4, alpha=0.8)\n\nax1.axhline(y=0, color='black', linestyle='--', linewidth=2, alpha=0.3, label='Minimum')\nax1.set_xlabel('Step', fontsize=11)\nax1.set_ylabel('f(x)', fontsize=11)\nax1.set_title('Convergence: Linear Scale', fontsize=12, fontweight='bold')\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\nmax_f_val = max([max(results[lr]['f']) for lr in learning_rates])\nax1.set_ylim(-2, min(200, max_f_val + 10))\n\n# Right: Log scale (better for seeing convergence)\nfor lr, label, color in zip(learning_rates, lr_labels, colors):\n    res = results[lr]\n    # Add small epsilon to avoid log(0)\n    f_vals = np.array(res['f']) + 1e-10\n    ax2.semilogy(range(len(f_vals)), f_vals, 'o-',\n                color=color, label=f'LR = {label}', linewidth=2, markersize=4, alpha=0.8)\n\nax2.set_xlabel('Step', fontsize=11)\nax2.set_ylabel('f(x) [log scale]', fontsize=11)\nax2.set_title('Convergence: Log Scale', fontsize=12, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3, which='both')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNotice:\")\nprint(\"  • LR = 0.001: Very slow descent (wastes computation)\")\nprint(\"  • LR = 0.1: Smooth, fast convergence (optimal!)\")\nprint(\"  • LR = 0.8: May oscillate or diverge (unstable)\")\nprint(\"  • LR = 3.0: Rapid divergence - completely unstable!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for lr, label in zip(learning_rates, lr_labels):\n",
    "    res = results[lr]\n",
    "    \n",
    "    # Compute steps to reach f(x) < 0.01 (near minimum)\n",
    "    steps_to_threshold = None\n",
    "    for i, f_val in enumerate(res['f']):\n",
    "        if f_val < 0.01:\n",
    "            steps_to_threshold = i\n",
    "            break\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Learning Rate': label,\n",
    "        'Total Steps': res['n_steps'],\n",
    "        'Steps to f < 0.01': steps_to_threshold if steps_to_threshold else 'N/A',\n",
    "        'Final x': f\"{res['x'][-1]:.6f}\",\n",
    "        'Final f(x)': f\"{res['f'][-1]:.6f}\",\n",
    "        'Status': res['status'],\n",
    "        'Converged': 'Yes' if res['converged'] else 'No'\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "display(df_comparison)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Small LR (0.001): Stable but slow - needs many iterations\")\n",
    "print(\"2. Moderate LR (0.1): Fast and stable - 'Goldilocks' zone\")\n",
    "print(\"3. Large LR (0.8): Risky - may oscillate or diverge\")\n",
    "print(\"\\nTrade-off: Speed vs. Stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive: Find Your Own Learning Rate\n",
    "\n",
    "Now it's your turn! Experiment with different learning rates to find:\n",
    "1. The **largest LR that still converges**\n",
    "2. The **smallest LR that converges in < 50 steps**\n",
    "3. An LR that causes **dramatic divergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exploration widget\n",
    "lr_custom_input = FloatText(description=\"Learning rate:\", value=0.1, step=0.05)\n",
    "run_custom_button = Button(description=\"Run GD\", button_style='success')\n",
    "output_custom = Output()\n",
    "\n",
    "def on_run_custom(b):\n",
    "    lr_custom = lr_custom_input.value\n",
    "    \n",
    "    with output_custom:\n",
    "        output_custom.clear_output(wait=True)\n",
    "        \n",
    "        # Run GD\n",
    "        res = run_gd_with_lr(x_start, lr_custom, max_steps=100)\n",
    "        \n",
    "        # Plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n",
    "        \n",
    "        # Left: Path on function\n",
    "        ax1.plot(x_plot, f_plot, 'b-', linewidth=2, alpha=0.3, label='f(x) = 0.5x²')\n",
    "        \n",
    "        x_hist = np.array(res['x'])\n",
    "        f_hist = np.array(res['f'])\n",
    "        \n",
    "        colors_grad = plt.cm.viridis(np.linspace(0, 1, len(x_hist)))\n",
    "        for i in range(len(x_hist)):\n",
    "            marker = 'o' if i == 0 else ('*' if i == len(x_hist)-1 else 'o')\n",
    "            size = 150 if i == 0 or i == len(x_hist)-1 else 60\n",
    "            ax1.scatter(x_hist[i], f_hist[i], c=[colors_grad[i]], marker=marker,\n",
    "                       s=size, edgecolors='black', linewidths=1.5, zorder=3)\n",
    "        \n",
    "        ax1.set_xlabel('x', fontsize=11)\n",
    "        ax1.set_ylabel('f(x)', fontsize=11)\n",
    "        ax1.set_title(f'GD Path: LR = {lr_custom}\\nStatus: {res[\"status\"]}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlim(-15, 15)\n",
    "        ax1.set_ylim(-5, min(100, max(f_hist) + 10))\n",
    "        \n",
    "        # Right: Convergence\n",
    "        ax2.semilogy(range(len(res['f'])), np.array(res['f']) + 1e-10, 'o-',\n",
    "                    color='purple', linewidth=2, markersize=4)\n",
    "        ax2.set_xlabel('Step', fontsize=11)\n",
    "        ax2.set_ylabel('f(x) [log scale]', fontsize=11)\n",
    "        ax2.set_title('Convergence Curve', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, which='both')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nLearning Rate: {lr_custom}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Status: {res['status']}\")\n",
    "        print(f\"Steps: {res['n_steps']}\")\n",
    "        print(f\"Final x: {res['x'][-1]:.6f}\")\n",
    "        print(f\"Final f(x): {res['f'][-1]:.6f}\")\n",
    "        print(f\"Converged: {'Yes' if res['converged'] else 'No'}\")\n",
    "        \n",
    "        # Provide feedback\n",
    "        if res['status'] == 'converged' and res['n_steps'] < 30:\n",
    "            print(\"\\n✓ Excellent! Fast and stable convergence.\")\n",
    "        elif res['status'] == 'converged':\n",
    "            print(\"\\n✓ Converged, but slowly. Try a larger LR.\")\n",
    "        elif res['status'] == 'oscillating':\n",
    "            print(\"\\n⚠ Oscillating! Try a smaller LR.\")\n",
    "        elif res['status'] == 'diverged':\n",
    "            print(\"\\n✗ Diverged! LR is too large.\")\n",
    "        else:\n",
    "            print(\"\\n⚠ Slow convergence. Try a larger LR.\")\n",
    "\n",
    "run_custom_button.on_click(on_run_custom)\n",
    "\n",
    "print(\"Interactive Learning Rate Exploration\")\n",
    "print(\"=\"*80)\n",
    "print(\"Experiment with different learning rates!\")\n",
    "print(\"\\nChallenges:\")\n",
    "print(\"  1. Find the LARGEST LR that still converges\")\n",
    "print(\"  2. Find an LR that converges in < 20 steps\")\n",
    "print(\"  3. Find an LR that causes divergence\\n\")\n",
    "\n",
    "display(VBox([\n",
    "    lr_custom_input,\n",
    "    run_custom_button,\n",
    "    output_custom\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Questions for Your Answer Sheet\n\n**Q11.** Based on your observations, describe the behavior for each learning rate category:\n- Too small (e.g., 0.001): What happens? Why is this wasteful?\n- Just right (e.g., 0.1): What makes this optimal?\n- Too large (e.g., 0.8): What problems occur? Why?\n- Way too large (e.g., 3.0): How quickly does it diverge? What does this tell you?\n\n**Q12.** How would you choose a learning rate for a new optimization problem?\n- What strategy would you use?\n- What signs would indicate your LR is too large? Too small?\n- Why is learning rate called a \"hyperparameter\"?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Learning Rate Trade-off:\n",
    "\n",
    "| Learning Rate | Speed | Stability | Outcome |\n",
    "|--------------|-------|-----------|----------|\n",
    "| Too small | Very slow | Very stable | Wastes computation |\n",
    "| Optimal | Fast | Stable | Best performance |\n",
    "| Too large | Fast (initially) | Unstable | Oscillation/divergence |\n",
    "\n",
    "### In Real Machine Learning:\n",
    "\n",
    "- Learning rate is the **most important hyperparameter**\n",
    "- Real models: Often start with LR ≈ 0.001-0.01\n",
    "- **Learning rate schedules**: Start large, decrease over time\n",
    "- **Adaptive methods**: Adam, RMSprop adjust LR automatically\n",
    "- **Grid search / tuning**: Try multiple LRs, pick best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q10, Q11, Q12** on your answer sheet\n",
    "2. **Return to the LMS** and continue to Module 4\n",
    "3. In Module 4, you'll see GD's limitations with local optima on the mountain landscape!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}