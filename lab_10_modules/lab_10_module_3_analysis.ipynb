{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 - Module 3: Analysis and Visualization\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "In this module, you'll visualize the patterns in your data to answer the core question:\n",
    "\n",
    "> **Does AI confidence match actual accuracy?**\n",
    "\n",
    "You'll create three visualizations:\n",
    "1. **Confusion Matrix Heatmap** - How often confidence matched performance\n",
    "2. **Error Rate Bar Chart** - Which prompt categories had the most errors\n",
    "3. **Overconfidence Examples** - Specific cases where AI was confident but wrong\n",
    "\n",
    "These visualizations will reveal patterns that might be invisible when looking at individual prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Drive Setup\n",
    "\n",
    "**IMPORTANT:** This module loads data from your Google Drive (saved in previous modules).\n",
    "\n",
    "Run the cell below to mount Google Drive. You may need to authorize access again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "LAB_DIR = '/content/drive/MyDrive/DATA1010/Lab10'\n",
    "print(\"Google Drive mounted successfully!\")\n",
    "print(f\"Lab directory: {LAB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Your Group Code\n",
    "\n",
    "Use the **same group code** from Modules 0, 1, and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_code = int(input(\"Enter your group code: \"))\n",
    "\n",
    "# Load all data files\n",
    "prompts_filename = f\"{LAB_DIR}/lab10_group_{group_code}_prompts.csv\"\n",
    "predictions_filename = f\"{LAB_DIR}/lab10_group_{group_code}_predictions.csv\"\n",
    "evaluations_filename = f\"{LAB_DIR}/lab10_group_{group_code}_evaluations.csv\"\n",
    "\n",
    "try:\n",
    "    prompts_df = pd.read_csv(prompts_filename)\n",
    "    predictions_df = pd.read_csv(predictions_filename)\n",
    "    evaluations_df = pd.read_csv(evaluations_filename)\n",
    "    \n",
    "    print(f\"âœ“ Loaded prompts from {prompts_filename}\")\n",
    "    print(f\"âœ“ Loaded predictions from {predictions_filename}\")\n",
    "    print(f\"âœ“ Loaded evaluations from {evaluations_filename}\")\n",
    "    print(f\"âœ“ Group Code: {group_code}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ ERROR: Could not find required files\")\n",
    "    print(f\"Missing: {e.filename}\")\n",
    "    print(\"\\nMake sure you:\")\n",
    "    print(\"1. Ran Module 0 (to generate prompts)\")\n",
    "    print(\"2. Ran Module 1 (to collect predictions)\")\n",
    "    print(\"3. Ran Module 2 (to evaluate responses)\")\n",
    "    print(\"4. Are using the same group code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge All Data\n",
    "\n",
    "Combine prompts, predictions, and evaluations into one complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "complete_df = prompts_df.merge(predictions_df, on=['prompt_id', 'category'])\n",
    "complete_df = complete_df.merge(evaluations_df, on=['prompt_id', 'category'])\n",
    "\n",
    "# Create simplified categories for visualization\n",
    "def simplify_confidence(conf):\n",
    "    if 'No caveats' in conf:\n",
    "        return 'Confident'\n",
    "    elif 'Refused' in conf:\n",
    "        return 'Refused'\n",
    "    else:\n",
    "        return 'Cautious'\n",
    "\n",
    "def simplify_accuracy(acc):\n",
    "    if 'Fully accurate' in acc or 'Mostly accurate' in acc:\n",
    "        return 'Accurate'\n",
    "    elif 'Refused' in acc:\n",
    "        return 'Refused'\n",
    "    else:\n",
    "        return 'Inaccurate'\n",
    "\n",
    "complete_df['confidence_simple'] = complete_df['ai_confidence'].apply(simplify_confidence)\n",
    "complete_df['accuracy_simple'] = complete_df['actual_accuracy'].apply(simplify_accuracy)\n",
    "\n",
    "# Save complete dataset\n",
    "complete_filename = f\"{LAB_DIR}/lab10_group_{group_code}_complete.csv\"\n",
    "complete_df.to_csv(complete_filename, index=False)\n",
    "\n",
    "print(f\"âœ“ Merged {len(complete_df)} prompts\")\n",
    "print(f\"âœ“ Saved complete dataset to {complete_filename}\")\n",
    "print(\"\\nFirst few rows of your complete dataset:\")\n",
    "display(complete_df[['prompt_id', 'category_display', 'confidence_simple', 'accuracy_simple']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Confusion Matrix Heatmap\n",
    "\n",
    "This heatmap shows how AI's **expressed confidence** compares to **actual performance**.\n",
    "\n",
    "**What to look for:**\n",
    "- **Diagonal cells** (Confident+Accurate, Cautious+Inaccurate): Confidence matched reality\n",
    "- **Off-diagonal cells** (Confident+Inaccurate): Overconfidence\n",
    "- **Cell counts**: How many prompts fall into each combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "ct = pd.crosstab(complete_df['confidence_simple'], \n",
    "                 complete_df['accuracy_simple'],\n",
    "                 margins=False)\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(ct, annot=True, fmt='d', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Count'}, \n",
    "            linewidths=2, linecolor='black',\n",
    "            ax=ax)\n",
    "\n",
    "plt.title('AI Confidence vs Actual Accuracy\\n(Confusion Matrix)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('AI Confidence Level', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Actual Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display overconfidence rate\n",
    "try:\n",
    "    confident_total = complete_df[complete_df['confidence_simple'] == 'Confident'].shape[0]\n",
    "    overconfident = complete_df[\n",
    "        (complete_df['confidence_simple'] == 'Confident') & \n",
    "        (complete_df['accuracy_simple'] == 'Inaccurate')\n",
    "    ].shape[0]\n",
    "    \n",
    "    if confident_total > 0:\n",
    "        overconfidence_rate = (overconfident / confident_total) * 100\n",
    "        print(\"=\"*60)\n",
    "        print(\"KEY METRIC: OVERCONFIDENCE RATE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Confident responses: {confident_total}\")\n",
    "        print(f\"Overconfident (Confident but Inaccurate): {overconfident}\")\n",
    "        print(f\"\\nðŸš¨ OVERCONFIDENCE RATE: {overconfidence_rate:.1f}%\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "        \n",
    "        if overconfidence_rate > 50:\n",
    "            print(\"âš ï¸ HIGH OVERCONFIDENCE: More than half of confident responses were inaccurate!\")\n",
    "        elif overconfidence_rate > 25:\n",
    "            print(\"â„¹ï¸ MODERATE OVERCONFIDENCE: About 1 in 4 confident responses were inaccurate.\")\n",
    "        elif overconfidence_rate > 0:\n",
    "            print(\"âœ“ LOW OVERCONFIDENCE: Most confident responses were accurate.\")\n",
    "        else:\n",
    "            print(\"âœ“ NO OVERCONFIDENCE: All confident responses were accurate!\")\n",
    "    else:\n",
    "        print(\"Note: AI expressed no confident responses in your data.\")\n",
    "except:\n",
    "    print(\"Could not calculate overconfidence rate (possibly no confident responses).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Error Rates by Category\n",
    "\n",
    "This bar chart shows which **prompt categories** had the highest error rates.\n",
    "\n",
    "**What to look for:**\n",
    "- **Tallest bars**: Categories where AI struggles most\n",
    "- **Shortest bars**: Categories where AI performs well\n",
    "- **Patterns**: Do citation requests have more errors than commonsense questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error rate by category\n",
    "error_rates = complete_df.groupby('category_display').apply(\n",
    "    lambda x: (x['accuracy_simple'] == 'Inaccurate').sum() / len(x) * 100\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "bars = ax.bar(range(len(error_rates)), error_rates.values, \n",
    "              color='coral', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, error_rates.values)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.0f}%',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_xticks(range(len(error_rates)))\n",
    "ax.set_xticklabels(error_rates.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Error Rate (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Prompt Category', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Error Rate by Prompt Category\\n(Sorted from Highest to Lowest)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 110)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR RATES BY CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "for cat, rate in error_rates.items():\n",
    "    print(f\"{cat:25s}: {rate:5.1f}%\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nHardest category: {error_rates.index[0]} ({error_rates.values[0]:.1f}%)\")\n",
    "print(f\"Easiest category: {error_rates.index[-1]} ({error_rates.values[-1]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Overconfidence Examples\n",
    "\n",
    "This table shows **specific prompts** where AI was confident but inaccurate.\n",
    "\n",
    "**What to look for:**\n",
    "- What kinds of prompts trigger overconfident failures?\n",
    "- What specific errors did the AI make?\n",
    "- Were the errors subtle or obvious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for overconfident cases\n",
    "overconfident_df = complete_df[\n",
    "    (complete_df['confidence_simple'] == 'Confident') & \n",
    "    (complete_df['accuracy_simple'] == 'Inaccurate')\n",
    "]\n",
    "\n",
    "if len(overconfident_df) > 0:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"OVERCONFIDENCE EXAMPLES: {len(overconfident_df)} cases found\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    # Display table\n",
    "    display_cols = ['prompt_id', 'category_display', 'prompt_text', \n",
    "                    'ai_confidence', 'actual_accuracy', 'error_types']\n",
    "    \n",
    "    display_df = overconfident_df[display_cols].copy()\n",
    "    display_df.columns = ['ID', 'Category', 'Prompt', 'AI Confidence', \n",
    "                           'Actual Result', 'Error Types']\n",
    "    \n",
    "    # Style the table\n",
    "    styled_html = display_df.to_html(index=False, escape=False, \n",
    "                                       classes='table table-striped table-bordered')\n",
    "    display(HTML(styled_html))\n",
    "    \n",
    "    # Show detailed error information\n",
    "    print(\"\\nDETAILED ERROR DESCRIPTIONS:\")\n",
    "    print(\"=\"*70)\n",
    "    for idx, row in overconfident_df.iterrows():\n",
    "        print(f\"\\nPrompt #{row['prompt_id']}: {row['category_display']}\")\n",
    "        print(f\"Prompt: {row['prompt_text']}\")\n",
    "        if pd.notna(row['error_details']) and row['error_details'].strip():\n",
    "            print(f\"Errors found: {row['error_details']}\")\n",
    "        else:\n",
    "            print(f\"Error types: {row['error_types']}\")\n",
    "        print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"NO OVERCONFIDENCE DETECTED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nYour AI model did not show any cases where it was:\")\n",
    "    print(\"- Confident (no caveats) AND\")\n",
    "    print(\"- Inaccurate (major errors or hallucinations)\")\n",
    "    print()\n",
    "    print(\"This could mean:\")\n",
    "    print(\"1. The AI was well-calibrated on your prompts\")\n",
    "    print(\"2. The AI expressed caution on difficult prompts\")\n",
    "    print(\"3. The AI was generally accurate\")\n",
    "    print()\n",
    "    print(\"Check the confusion matrix above for the full picture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Key metrics about your group's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive statistics\n",
    "total_prompts = len(complete_df)\n",
    "accurate_count = (complete_df['accuracy_simple'] == 'Accurate').sum()\n",
    "inaccurate_count = (complete_df['accuracy_simple'] == 'Inaccurate').sum()\n",
    "refused_count = (complete_df['accuracy_simple'] == 'Refused').sum()\n",
    "\n",
    "confident_count = (complete_df['confidence_simple'] == 'Confident').sum()\n",
    "cautious_count = (complete_df['confidence_simple'] == 'Cautious').sum()\n",
    "refused_conf_count = (complete_df['confidence_simple'] == 'Refused').sum()\n",
    "\n",
    "# Calibration metrics\n",
    "well_calibrated = complete_df[\n",
    "    ((complete_df['confidence_simple'] == 'Confident') & (complete_df['accuracy_simple'] == 'Accurate')) |\n",
    "    ((complete_df['confidence_simple'] == 'Cautious') & (complete_df['accuracy_simple'] == 'Inaccurate'))\n",
    "].shape[0]\n",
    "\n",
    "overconfident_count = complete_df[\n",
    "    (complete_df['confidence_simple'] == 'Confident') & \n",
    "    (complete_df['accuracy_simple'] == 'Inaccurate')\n",
    "].shape[0]\n",
    "\n",
    "underconfident_count = complete_df[\n",
    "    (complete_df['confidence_simple'] == 'Cautious') & \n",
    "    (complete_df['accuracy_simple'] == 'Accurate')\n",
    "].shape[0]\n",
    "\n",
    "# Display summary\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal prompts tested: {total_prompts}\")\n",
    "print()\n",
    "print(\"ACCURACY BREAKDOWN:\")\n",
    "print(f\"  Accurate responses:   {accurate_count:2d} ({accurate_count/total_prompts*100:5.1f}%)\")\n",
    "print(f\"  Inaccurate responses: {inaccurate_count:2d} ({inaccurate_count/total_prompts*100:5.1f}%)\")\n",
    "print(f\"  Refused responses:    {refused_count:2d} ({refused_count/total_prompts*100:5.1f}%)\")\n",
    "print()\n",
    "print(\"CONFIDENCE BREAKDOWN:\")\n",
    "print(f\"  Confident (no caveats): {confident_count:2d} ({confident_count/total_prompts*100:5.1f}%)\")\n",
    "print(f\"  Cautious (with caveats): {cautious_count:2d} ({cautious_count/total_prompts*100:5.1f}%)\")\n",
    "print(f\"  Refused:                {refused_conf_count:2d} ({refused_conf_count/total_prompts*100:5.1f}%)\")\n",
    "print()\n",
    "print(\"CALIBRATION ANALYSIS:\")\n",
    "print(f\"  Well-calibrated (confidence matched accuracy): {well_calibrated:2d} ({well_calibrated/total_prompts*100:5.1f}%)\")\n",
    "print(f\"  Overconfident (confident but wrong):          {overconfident_count:2d} ({overconfident_count/total_prompts*100:5.1f}%)\")\n",
    "print(f\"  Underconfident (cautious but right):          {underconfident_count:2d} ({underconfident_count/total_prompts*100:5.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "if overconfident_count > underconfident_count:\n",
    "    print(\"â†’ Your AI showed MORE overconfidence than underconfidence.\")\n",
    "    print(\"  This means it was more likely to sound sure when wrong than\")\n",
    "    print(\"  to sound uncertain when right.\")\n",
    "elif underconfident_count > overconfident_count:\n",
    "    print(\"â†’ Your AI showed MORE underconfidence than overconfidence.\")\n",
    "    print(\"  This means it was more likely to sound cautious when right than\")\n",
    "    print(\"  to sound sure when wrong.\")\n",
    "else:\n",
    "    print(\"â†’ Your AI showed EQUAL amounts of overconfidence and underconfidence.\")\n",
    "\n",
    "if well_calibrated / total_prompts > 0.75:\n",
    "    print(\"\\nâ†’ Overall, the AI was fairly WELL-CALIBRATED (>75% match).\")\n",
    "elif well_calibrated / total_prompts > 0.5:\n",
    "    print(\"\\nâ†’ The AI showed MODERATE calibration (50-75% match).\")\n",
    "else:\n",
    "    print(\"\\nâ†’ The AI showed POOR calibration (<50% match).\")\n",
    "    print(\"  Confidence did not reliably predict accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary for Cross-Group Comparison\n",
    "\n",
    "Save your key metrics for instructor-led cross-group discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dict\n",
    "summary = {\n",
    "    'group_code': group_code,\n",
    "    'total_prompts': total_prompts,\n",
    "    'accurate_count': accurate_count,\n",
    "    'inaccurate_count': inaccurate_count,\n",
    "    'confident_count': confident_count,\n",
    "    'overconfident_count': overconfident_count,\n",
    "    'underconfident_count': underconfident_count,\n",
    "    'overconfidence_rate': (overconfident_count / confident_count * 100) if confident_count > 0 else 0,\n",
    "    'calibration_rate': (well_calibrated / total_prompts * 100)\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "summary_filename = f\"{LAB_DIR}/lab10_group_{group_code}_summary.json\"\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Summary saved to {summary_filename}\")\n",
    "print(\"\\nShare these metrics with other groups for comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Module 3\n",
    "\n",
    "Answer these questions on your lab handout using the visualizations and statistics above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: Confusion Matrix Interpretation\n",
    "\n",
    "Looking at the confusion matrix heatmap, how often did the AI's self-assessment (expressed confidence) match its actual performance?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: Overconfidence vs Underconfidence\n",
    "\n",
    "Did the AI show **overconfidence** (confident tone but inaccurate) or **underconfidence** (cautious but accurate)? Which pattern was more common in your data?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: Category Analysis\n",
    "\n",
    "Which prompt category had the highest error rate in the bar chart? Why do you think this category is difficult for AI models?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16: Overconfidence Example Analysis\n",
    "\n",
    "Looking at the overconfidence examples table, pick your group's most \"overconfident\" prompt (confident but very wrong). What made the AI fail despite sounding sure of itself?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17: Uncertainty Correlation\n",
    "\n",
    "Did expressing uncertainty (caveats like \"I might be wrong\") correlate with lower accuracy? Use your data to support your answer.\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18: Cross-Group Comparison\n",
    "\n",
    "(Cross-group comparison) Compare your confusion matrix or overconfidence rate with another group's results. Did different groups find similar patterns of overconfidence?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q13-Q18** on your lab handout\n",
    "2. **Discuss findings** with other groups (if time permits)\n",
    "3. **Continue to Module 4** for synthesis and real-world implications\n",
    "4. **Use the same group code** in Module 4\n",
    "\n",
    "In Module 4, you'll connect these patterns to real-world AI failures and develop best practices for responsible AI use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}