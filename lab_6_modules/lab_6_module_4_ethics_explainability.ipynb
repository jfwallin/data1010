{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6, Module 4: Ethics & Explainability in Practice\n",
    "\n",
    "**Estimated time:** 10 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: Bringing It All Together**\n",
    "\n",
    "You've now explored saliency across three different modalities:\n",
    "- **Module 1:** Text (word masking)\n",
    "- **Module 2:** Images (gradient heatmaps)\n",
    "- **Module 3:** Tabular data (feature perturbation)\n",
    "\n",
    "In every case, saliency revealed **what drives model decisions**‚Äîsometimes confirming our intuitions, sometimes revealing problems.\n",
    "\n",
    "Now let's step back and reflect on the **bigger picture**: Why does explainability matter for responsible AI? When should we demand it? What are its limits?\n",
    "\n",
    "---\n",
    "\n",
    "# üìò **The Explainability Imperative**\n",
    "\n",
    "## **When Explainability Is Required**\n",
    "\n",
    "Not all AI applications need the same level of explainability. Consider these scenarios:\n",
    "\n",
    "### **High-Stakes Decisions (Explainability CRITICAL)**\n",
    "\n",
    "| Domain | Decision | Why Explainability Matters |\n",
    "|--------|----------|----------------------------|\n",
    "| **Healthcare** | Diagnosis, treatment recommendations | Doctors need to verify AI reasoning; patients have right to understand |\n",
    "| **Criminal Justice** | Sentencing, parole, risk assessment | Constitutional rights to due process; detect algorithmic bias |\n",
    "| **Finance** | Loan approvals, credit scoring | Legal requirements (Equal Credit Opportunity Act); consumer rights |\n",
    "| **Hiring** | Resume screening, candidate ranking | Anti-discrimination laws; fairness auditing |\n",
    "| **Education** | College admissions, scholarship awards | Equity concerns; detect bias against protected groups |\n",
    "\n",
    "### **Low-Stakes Decisions (Explainability Nice-to-Have)**\n",
    "\n",
    "| Domain | Decision | Why Less Critical |\n",
    "|--------|----------|--------------------|\n",
    "| **Entertainment** | Movie recommendations | Wrong recommendation = minor inconvenience |\n",
    "| **Shopping** | Product suggestions | Easy to ignore, low consequence |\n",
    "| **Content Ranking** | Social media feed order | User can scroll past, personalization expected |\n",
    "\n",
    "**Key principle:** The higher the stakes, the greater the need for explainability.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 19 (Ethics)**\n",
    "\n",
    "**Q19.** Think of a specific high-stakes AI application (medical, legal, financial, etc.). Why would explainability be critical in that context? What could go wrong without it?\n",
    "\n",
    "*Consider: Legal requirements, ethical obligations, potential for harm, need for trust.*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **The Explainability-Performance Tradeoff (Revisited)**\n",
    "\n",
    "Remember from Module 0 that there's often a tradeoff:\n",
    "\n",
    "```\n",
    "Simple models (linear, trees):\n",
    "  ‚úì High explainability\n",
    "  ‚úó Lower performance\n",
    "\n",
    "Complex models (deep learning):\n",
    "  ‚úì High performance\n",
    "  ‚úó Lower explainability\n",
    "```\n",
    "\n",
    "### **Real-World Case Study: Healthcare AI**\n",
    "\n",
    "**Scenario:** A hospital wants to predict patient readmission risk.\n",
    "\n",
    "**Option A: Logistic Regression**\n",
    "- **Performance:** 75% accuracy\n",
    "- **Explainability:** Perfect‚Äîdoctors can see exact feature weights\n",
    "- **Trust:** High‚Äîdoctors understand and accept the model\n",
    "\n",
    "**Option B: Deep Neural Network**\n",
    "- **Performance:** 85% accuracy\n",
    "- **Explainability:** Low‚Äîblack box\n",
    "- **Trust:** Low‚Äîdoctors suspicious of \"magic\" predictions\n",
    "\n",
    "**Option C: Neural Network + Saliency**\n",
    "- **Performance:** 85% accuracy\n",
    "- **Explainability:** Moderate‚Äîsaliency maps show important features\n",
    "- **Trust:** Medium‚Äîdoctors can verify AI focuses on relevant factors\n",
    "\n",
    "**Key question:** Is 10% extra accuracy worth reduced explainability?\n",
    "\n",
    "### **The Answer Depends On:**\n",
    "- **Stakes:** Life-or-death decisions may favor explainability\n",
    "- **Regulations:** Some domains legally require explainability\n",
    "- **Trust:** Will users trust and adopt the system?\n",
    "- **Auditability:** Can we detect when it fails?\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 20 (Tradeoffs)**\n",
    "\n",
    "**Q20.** In a medical diagnosis system, would you choose a 75% accurate explainable model or an 85% accurate black-box model? Explain your reasoning.\n",
    "\n",
    "*Think about: Patient safety, doctor trust, legal liability, ability to detect errors, right to explanation.*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **When Saliency Reveals Problems**\n",
    "\n",
    "Throughout this lab, you've seen examples where saliency exposes issues:\n",
    "\n",
    "### **1. Spurious Correlations (Module 2)**\n",
    "- **Problem:** Medical AI focuses on hospital logos, not anatomy\n",
    "- **Saliency reveals:** Heatmap highlights corner watermark\n",
    "- **Action:** Remove watermarks, retrain, add data augmentation\n",
    "\n",
    "### **2. Problematic Proxies (Module 3)**\n",
    "- **Problem:** Loan model uses zip code as proxy for race\n",
    "- **Saliency reveals:** Zip code is top feature\n",
    "- **Action:** Remove zip code, audit for other proxies, ensure fairness\n",
    "\n",
    "### **3. Negation Failures (Module 1)**\n",
    "- **Problem:** Sentiment model can't handle \"not good\"\n",
    "- **Saliency reveals:** \"good\" has positive saliency even when negated\n",
    "- **Action:** Use more sophisticated models (BERT, transformers) that understand context\n",
    "\n",
    "### **4. Background Focus (Module 2)**\n",
    "- **Problem:** Object classifier focuses on backgrounds (grass for dogs, snow for wolves)\n",
    "- **Saliency reveals:** Heatmap highlights wrong regions\n",
    "- **Action:** Collect diverse backgrounds, use data augmentation\n",
    "\n",
    "**Common theme:** Saliency is a **debugging and auditing tool** that reveals failure modes invisible from accuracy alone.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 21 (Application)**\n",
    "\n",
    "**Q21.** Imagine you're deploying a resume screening AI for hiring. How would you use saliency to audit for bias before deployment?\n",
    "\n",
    "*Think about: What features should the model focus on? What features would indicate bias? What would you check in the saliency maps?*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üåç **Real-World Explainability in Practice**\n",
    "\n",
    "Let's look at how major organizations approach explainability:\n",
    "\n",
    "### **Google: Model Cards**\n",
    "- Document model purpose, training data, performance, limitations\n",
    "- Include fairness metrics across demographics\n",
    "- Specify intended use cases and misuse risks\n",
    "\n",
    "### **Microsoft: Responsible AI Standard**\n",
    "- Requires impact assessments for high-risk AI\n",
    "- Mandates explainability for consequential decisions\n",
    "- Provides Fairlearn toolkit for bias detection\n",
    "\n",
    "### **Financial Services: FCRA & ECOA Compliance**\n",
    "- Must provide \"adverse action notices\" for loan denials\n",
    "- Explain which factors led to decision\n",
    "- Face legal liability for discriminatory algorithms\n",
    "\n",
    "### **European Union: GDPR Article 22**\n",
    "- Right to explanation for automated decisions\n",
    "- Right to human review\n",
    "- Restrictions on fully automated consequential decisions\n",
    "\n",
    "### **Healthcare: FDA AI/ML Guidance**\n",
    "- Requires explainability for medical device AI\n",
    "- Continuous monitoring and retraining protocols\n",
    "- Clinical validation and interpretability standards\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 22 (Regulation)**\n",
    "\n",
    "**Q22.** Why do you think regulations like GDPR require \"right to explanation\" for automated decisions? What problem is this trying to solve?\n",
    "\n",
    "*Think about: Power imbalances, accountability, discrimination, human dignity, trust in institutions.*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **Limitations of Saliency (Important!)**\n",
    "\n",
    "While powerful, saliency has important limitations:\n",
    "\n",
    "### **1. Saliency ‚â† Causality**\n",
    "- **What saliency shows:** Feature X affects prediction\n",
    "- **What it doesn't show:** Whether X *causes* outcome\n",
    "- **Example:** \"Umbrella usage\" is salient for \"rain prediction\"‚Äîbut umbrellas don't cause rain!\n",
    "\n",
    "### **2. Saliency Can Be Misleading**\n",
    "- **Adversarial examples:** Small, imperceptible changes can fool models\n",
    "- **Saliency can be manipulated:** Attackers can create inputs with misleading saliency\n",
    "- **Example:** Image looks like panda to humans, model sees gibbon‚Äîsaliency highlights \"gibbon features\" that don't exist\n",
    "\n",
    "### **3. Saliency Doesn't Explain Interactions**\n",
    "- **Shows:** Individual feature importance\n",
    "- **Doesn't show:** How features combine (\"age\" + \"income\" together)\n",
    "- **Example:** \"Hours studied\" alone is moderate, but \"hours studied\" + \"attendance\" together is strong‚Äîsaliency misses this\n",
    "\n",
    "### **4. Multiple Methods, Inconsistent Results**\n",
    "- **Gradients:** One saliency map\n",
    "- **Integrated Gradients:** Different map\n",
    "- **LIME:** Yet another map\n",
    "- **SHAP:** Still different\n",
    "- **Problem:** Which one is \"right\"? (They all answer slightly different questions)\n",
    "\n",
    "### **5. Explanations Can Create False Confidence**\n",
    "- Seeing a saliency map makes people trust the model more\n",
    "- Even if the saliency is misleading!\n",
    "- **Danger:** \"Explanation theater\" without real understanding\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 23 (Limitations)**\n",
    "\n",
    "**Q23.** Given the limitations of saliency, what else would you want to know about a model before deploying it in a high-stakes application?\n",
    "\n",
    "*Think about: Performance on different groups, failure modes, training data quality, human oversight, recourse mechanisms.*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Best Practices for Responsible AI**\n",
    "\n",
    "Based on everything you've learned, here are key principles:\n",
    "\n",
    "### **1. Explainability Is One Tool, Not The Solution**\n",
    "- Use saliency alongside fairness metrics, performance analysis, and human oversight\n",
    "- Don't rely on explanations alone\n",
    "\n",
    "### **2. Match Explainability to Stakes**\n",
    "- High-stakes ‚Üí demand high explainability (even at performance cost)\n",
    "- Low-stakes ‚Üí can prioritize performance\n",
    "\n",
    "### **3. Audit Early and Often**\n",
    "- Check saliency during development, not just deployment\n",
    "- Look for spurious correlations and problematic features\n",
    "- Test on diverse data\n",
    "\n",
    "### **4. Document Everything**\n",
    "- Training data sources and biases\n",
    "- Model architecture and hyperparameters\n",
    "- Performance across demographics\n",
    "- Known limitations and failure modes\n",
    "\n",
    "### **5. Enable Human Oversight**\n",
    "- Don't fully automate consequential decisions\n",
    "- Provide tools for humans to inspect and override\n",
    "- Create appeal and recourse mechanisms\n",
    "\n",
    "### **6. Monitor in Production**\n",
    "- Saliency at deployment time ‚â† saliency after data drift\n",
    "- Continuously monitor for shifts in feature importance\n",
    "- Retrain when needed\n",
    "\n",
    "### **7. Consider Not Using AI**\n",
    "- Sometimes the right answer is \"don't automate this\"\n",
    "- If you can't explain it, can't audit it, can't ensure fairness ‚Üí don't deploy it\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 24 (Synthesis)**\n",
    "\n",
    "**Q24.** Reflecting on the entire lab (Modules 0-4): What is the most important takeaway about explainability and responsible AI? Why does this matter for your future work with AI systems?\n",
    "\n",
    "*Think about: What surprised you? What changed your thinking? How will you approach AI differently now?*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Connection to the Lab Sequence**\n",
    "\n",
    "Let's see how Lab 6 completes your journey through the DATA 1010 labs:\n",
    "\n",
    "**Lab 1:** You learned what models are and how they make predictions\n",
    "\n",
    "**Lab 2:** You learned how models learn through gradient descent\n",
    "\n",
    "**Lab 3:** You learned how activation functions enable nonlinearity\n",
    "\n",
    "**Lab 4:** You learned how hidden layers create powerful representations\n",
    "\n",
    "**Lab 5:** You learned how embeddings encode meaning as geometry\n",
    "\n",
    "**Lab 6:** You learned how to understand **what models actually do** and **whether they should be trusted**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Where to Go Next**\n",
    "\n",
    "If you're interested in explainability and responsible AI, explore:\n",
    "\n",
    "### **Advanced Explainability Methods**\n",
    "- **SHAP (SHapley Additive exPlanations):** Game theory approach to feature importance\n",
    "- **LIME (Local Interpretable Model-Agnostic Explanations):** Local approximations\n",
    "- **Integrated Gradients:** More stable gradient method\n",
    "- **Attention Visualization:** For transformers and language models\n",
    "\n",
    "### **Fairness and Bias**\n",
    "- **Fairlearn (Microsoft):** Bias detection and mitigation toolkit\n",
    "- **AI Fairness 360 (IBM):** Comprehensive fairness library\n",
    "- **Aequitas:** Bias and fairness audit toolkit\n",
    "\n",
    "### **Responsible AI Frameworks**\n",
    "- **Google Model Cards:** Documentation standard\n",
    "- **Microsoft Responsible AI Standard:** Company-wide principles\n",
    "- **Partnership on AI:** Industry consortium\n",
    "\n",
    "### **Academic Resources**\n",
    "- **\"Interpretable Machine Learning\" by Christoph Molnar:** Free online book\n",
    "- **\"Fairness and Machine Learning\" by Barocas, Hardt, Narayanan:** Comprehensive textbook\n",
    "- **ACM FAccT Conference:** Top venue for fairness, accountability, transparency\n",
    "\n",
    "### **Regulatory Developments**\n",
    "- **EU AI Act:** Risk-based regulation framework\n",
    "- **NIST AI Risk Management Framework:** US government guidance\n",
    "- **Algorithmic Accountability Acts:** Proposed US legislation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Lab 6 Complete!\n",
    "\n",
    "Congratulations! You've completed Lab 6 and learned:\n",
    "\n",
    "### **Technical Skills:**\n",
    "- ‚úì How to compute saliency for text, images, and tabular data\n",
    "- ‚úì How to visualize feature importance\n",
    "- ‚úì How to interpret saliency maps\n",
    "\n",
    "### **Conceptual Understanding:**\n",
    "- ‚úì Why explainability matters for responsible AI\n",
    "- ‚úì When explainability is critical vs. nice-to-have\n",
    "- ‚úì The interpretability-performance tradeoff\n",
    "- ‚úì Limitations of saliency methods\n",
    "\n",
    "### **Ethical Reasoning:**\n",
    "- ‚úì How to detect spurious correlations\n",
    "- ‚úì How to identify problematic features\n",
    "- ‚úì How to audit models for bias\n",
    "- ‚úì When not to deploy AI systems\n",
    "\n",
    "**Most importantly:** You understand that building accurate models is not enough‚Äîwe must also ensure they're **fair, transparent, and accountable**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **Before You Submit**\n",
    "\n",
    "Make sure you have:\n",
    "\n",
    "- [ ] Completed Module 0 (What Is Saliency?)\n",
    "- [ ] Completed Module 1 (Text Saliency)\n",
    "- [ ] Completed Module 2 (Image Saliency)\n",
    "- [ ] Completed Module 3 (Tabular Saliency)\n",
    "- [ ] Completed Module 4 (Ethics & Explainability)\n",
    "- [ ] Answered all 24 questions (Q1-Q24)\n",
    "- [ ] Experimented with your own text, images, or data\n",
    "- [ ] Reflected on the ethical implications\n",
    "- [ ] Understood connections to Labs 1-5\n",
    "\n",
    "---\n",
    "\n",
    "## üéì **Final Reflection**\n",
    "\n",
    "Take a moment to appreciate what you've learned across all six labs:\n",
    "\n",
    "You started by manually adjusting weights to fit a line to data (Lab 1). Now you understand how to audit billion-parameter models for bias and fairness (Lab 6).\n",
    "\n",
    "You've gone from **building** AI to **understanding** AI to **responsibly deploying** AI.\n",
    "\n",
    "That's a remarkable journey.\n",
    "\n",
    "**Thank you for taking responsible AI seriously.** The field needs more people like you who ask not just \"can we build this?\" but \"should we build this?\" and \"how do we build this responsibly?\"\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Lab 6!**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
