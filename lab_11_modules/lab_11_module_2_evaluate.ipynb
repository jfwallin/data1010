{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 - Module 2: Evaluate (Stage 2)\n",
    "\n",
    "**Time:** ~20 minutes\n",
    "\n",
    "## Stage 2: Systematic Evaluation with Rubrics\n",
    "\n",
    "In Module 1, you gave a quick \"first impression\" rating. Now you'll evaluate more systematically using **explicit criteria**.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Apply explicit rubrics to evaluate content quality\n",
    "- Compare human evaluation vs. AI self-evaluation\n",
    "- Discover where AI's self-assessment is accurate or inaccurate\n",
    "- Understand that \"good\" requires clear, measurable standards\n",
    "\n",
    "### What You'll Do\n",
    "\n",
    "1. Select a rubric that fits your content type\n",
    "2. Score the AI's output (human judgment) using the rubric\n",
    "3. Ask the AI to score its OWN work using the same rubric\n",
    "4. Compare: Where do you agree? Where do you disagree?\n",
    "5. Identify the weakest areas for revision in Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML, Markdown\n\nprint(\"‚úì Libraries loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Your AI's Output\n",
    "\n",
    "Here's what the AI generated in Module 1. You'll be evaluating this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt generation system (copied from Module 0)\ndef generate_group_scenario(group_code):\n    \"\"\"\n    Generates a deterministic creative prompt based on group code.\n    8 prompt families √ó 5-6 variants each = unique scenarios for each group.\n    \"\"\"\n    np.random.seed(group_code)\n    \n    # 8 Prompt Families\n    families = [\n        'Science Explainer',\n        'Public Service Announcement',\n        'Museum Exhibit Panel',\n        'Product Pitch/Campaign',\n        'Infographic Structure',\n        'Short Narrative',\n        'Educational Analogy',\n        'Debate Position Statement'\n    ]\n    \n    # Select family (deterministic based on group code)\n    family_idx = group_code % 8\n    family = families[family_idx]\n    \n    # Rubric mapping\n    rubric_map = {\n        'Science Explainer': 'General Communication',\n        'Public Service Announcement': 'Persuasion/Campaign',\n        'Museum Exhibit Panel': 'General Communication',\n        'Product Pitch/Campaign': 'Persuasion/Campaign',\n        'Infographic Structure': 'General Communication',\n        'Short Narrative': 'Creative/Narrative',\n        'Educational Analogy': 'General Communication',\n        'Debate Position Statement': 'Persuasion/Campaign'\n    }\n    \n    # Family-specific prompt generation\n    if family == 'Science Explainer':\n        topics = ['quantum entanglement', 'CRISPR gene editing', 'dark matter', \n                  'neural plasticity', 'photosynthesis']\n        audiences = ['middle school students', 'first-year college students', 'general public']\n        tones = ['serious and formal', 'playful and engaging', 'inspiring and aspirational']\n        \n        topic = np.random.choice(topics)\n        audience = np.random.choice(audiences)\n        tone = np.random.choice(tones)\n        \n        prompt = f\"\"\"Write a 300-word explanation of {topic} for {audience}.\n\nRequirements:\n- Use a {tone} tone\n- Avoid unnecessary jargon\n- Include at least one concrete example or analogy\n- Make it engaging and accessible\"\"\"\n        \n        variants = {'topic': topic, 'audience': audience, 'tone': tone}\n    \n    elif family == 'Public Service Announcement':\n        issues = ['water conservation', 'mental health awareness', 'digital privacy', \n                  'food waste reduction', 'voter registration']\n        audiences = ['young adults (18-25)', 'seniors (65+)', 'parents of young children', 'general public']\n        constraints = ['include a specific statistic', 'include a clear call to action', \n                       'address a common misconception']\n        urgencies = ['moderate', 'high']\n        \n        issue = np.random.choice(issues)\n        audience = np.random.choice(audiences)\n        constraint = np.random.choice(constraints)\n        urgency = np.random.choice(urgencies)\n        \n        prompt = f\"\"\"Create a Public Service Announcement about {issue} targeting {audience}.\n\nRequirements:\n- Urgency level: {urgency}\n- Length: 250-350 words\n- Must {constraint}\n- Be persuasive but not preachy\"\"\"\n        \n        variants = {'issue': issue, 'audience': audience, 'urgency': urgency, 'constraint': constraint}\n    \n    elif family == 'Museum Exhibit Panel':\n        topics = ['Apollo 11 moon landing', 'invention of the printing press', \n                  'discovery of DNA structure', 'fall of the Berlin Wall', \n                  'development of the internet']\n        audience_levels = ['general public', 'high school students', 'history enthusiasts']\n        tones = ['formal and academic', 'accessible and engaging', 'narrative storytelling']\n        \n        topic = np.random.choice(topics)\n        audience = np.random.choice(audience_levels)\n        tone = np.random.choice(tones)\n        \n        prompt = f\"\"\"Write a museum exhibit panel about the {topic} for {audience}.\n\nRequirements:\n- Length: 300-400 words\n- Tone: {tone}\n- Include historical context and significance\n- Make it informative yet engaging\"\"\"\n        \n        variants = {'topic': topic, 'audience': audience, 'tone': tone}\n    \n    elif family == 'Product Pitch/Campaign':\n        products = ['reusable water bottle with built-in filter', 'productivity app for students',\n                    'sustainable fashion clothing line', 'meal-prep kit service', \n                    'noise-canceling headphones for remote work']\n        markets = ['college students', 'busy professionals', 'environmentally conscious consumers', \n                   'health-focused individuals']\n        benefits = ['environmental impact', 'cost savings', 'convenience', 'health benefits', 'quality/durability']\n        tones = ['aspirational and lifestyle-focused', 'practical and problem-solving', \n                 'humorous and memorable']\n        \n        product = np.random.choice(products)\n        market = np.random.choice(markets)\n        benefit = np.random.choice(benefits)\n        tone = np.random.choice(tones)\n        \n        prompt = f\"\"\"Create a campaign outline for a {product} targeting {market}.\n\nRequirements:\n- Length: 300-400 words\n- Primary benefit to emphasize: {benefit}\n- Tone: {tone}\n- Include key messaging and target audience insights\"\"\"\n        \n        variants = {'product': product, 'market': market, 'benefit': benefit, 'tone': tone}\n    \n    elif family == 'Infographic Structure':\n        topics = ['renewable energy sources comparison', 'stages of sleep and their functions',\n                  'water cycle explained', 'history of social media platforms', \n                  'nutrition basics: macro vs micronutrients']\n        formats = ['timeline', 'comparison chart', 'process flow', 'hierarchical breakdown']\n        audiences = ['students', 'policymakers', 'general public', 'professionals in the field']\n        messages = ['emphasize cost-effectiveness', 'highlight environmental impact', \n                    'focus on practical applications', 'stress health benefits']\n        \n        topic = np.random.choice(topics)\n        format_type = np.random.choice(formats)\n        audience = np.random.choice(audiences)\n        message = np.random.choice(messages)\n        \n        prompt = f\"\"\"Outline an infographic about {topic} using a {format_type} format for {audience}.\n\nRequirements:\n- Describe the visual structure and organization\n- Key message to {message}\n- Length: 250-350 words\n- Focus on clear, scannable information hierarchy\"\"\"\n        \n        variants = {'topic': topic, 'format': format_type, 'audience': audience, 'message': message}\n    \n    elif family == 'Short Narrative':\n        settings = ['space station orbiting Mars', 'lighthouse during a storm', \n                    'abandoned subway tunnel', 'research lab in Antarctica', \n                    'small caf√© in a foreign city']\n        tones = ['hopeful and uplifting', 'tense and suspenseful', 'mysterious and contemplative', \n                 'bittersweet and nostalgic']\n        constraints = ['include a metaphor about light or darkness', \n                       'avoid clich√©s about fate or destiny',\n                       'include dialogue that reveals character',\n                       'use sensory details (sound, smell, texture)']\n        \n        setting = np.random.choice(settings)\n        tone = np.random.choice(tones)\n        constraint = np.random.choice(constraints)\n        \n        prompt = f\"\"\"Write a 400-500 word short story set in a {setting}.\n\nRequirements:\n- Tone: {tone}\n- Must {constraint}\n- Create a complete narrative arc (beginning, middle, end)\n- Show, don't just tell\"\"\"\n        \n        variants = {'setting': setting, 'tone': tone, 'constraint': constraint}\n    \n    elif family == 'Educational Analogy':\n        concepts = ['machine learning', 'blockchain technology', 'quantum computing', \n                    'climate feedback loops', 'compound interest']\n        analogy_domains = ['cooking', 'sports', 'gardening', 'everyday household objects', \n                           'transportation systems']\n        audiences = ['non-technical managers', 'high school students', 'curious beginners', \n                     'professionals from another field']\n        \n        concept = np.random.choice(concepts)\n        domain = np.random.choice(analogy_domains)\n        audience = np.random.choice(audiences)\n        \n        prompt = f\"\"\"Explain {concept} using an analogy from {domain} for {audience}.\n\nRequirements:\n- Length: 300-400 words\n- Make the analogy clear and accurate\n- Explain how the analogy maps to the concept\n- Avoid oversimplification that loses key insights\"\"\"\n        \n        variants = {'concept': concept, 'domain': domain, 'audience': audience}\n    \n    else:  # Debate Position Statement\n        topics = ['universal basic income', 'social media age verification requirements',\n                  'mandatory voting', 'AI regulation in creative industries', \n                  'genetic modification of food crops']\n        positions = ['FOR (supporting)', 'AGAINST (opposing)']\n        audiences = ['academic/scholarly', 'general public', 'policymakers', 'industry professionals']\n        \n        topic = np.random.choice(topics)\n        position = np.random.choice(positions)\n        audience = np.random.choice(audiences)\n        \n        prompt = f\"\"\"Write a position statement {position} {topic} for an {audience} audience.\n\nRequirements:\n- Length: 350-450 words\n- Present clear arguments with supporting evidence\n- Acknowledge and address counterarguments\n- Maintain logical structure and persuasive tone\"\"\"\n        \n        variants = {'topic': topic, 'position': position, 'audience': audience}\n    \n    default_rubric = rubric_map[family]\n    \n    return {\n        'group_code': group_code,\n        'family': family,\n        'prompt': prompt,\n        'default_rubric': default_rubric,\n        'variants': variants\n    }\n\n# Enter group code and regenerate scenario\ngroup_code = int(input(\"Enter your group code: \"))\nscenario = generate_group_scenario(group_code)\n\n# Create a mock module1_data structure for the AI output from Module 1\n# Students will need to paste their AI output again in the review cell\nmodule1_data = {\n    'group_code': group_code,\n    'model_used': 'To be entered below',\n    'ai_output': 'Paste AI output in the review cell below',\n    'first_impression_rating': 'Recorded on answer sheet'\n}\n\nprint(f\"‚úì Regenerated scenario for Group {group_code}\")\nprint(f\"\\nPrompt Family: {scenario['family']}\")\nprint(f\"\\nNote: You'll paste your AI's output from Module 1 in the next cell for evaluation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Review: Your AI's Output\n\nPaste your AI's output from Module 1 below for evaluation:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paste your AI output from Module 1\nai_output_widget = widgets.Textarea(\n    value='',\n    placeholder='Paste the AI output from Module 1 here...',\n    description='AI Output:',\n    layout=widgets.Layout(width='100%', height='300px'),\n    style={'description_width': '120px'}\n)\n\ndisplay(ai_output_widget)\n\n# Button to confirm and display\nconfirm_button = widgets.Button(\n    description='‚úì Confirm Output',\n    button_style='info',\n    layout=widgets.Layout(width='200px')\n)\n\noutput_display = widgets.Output()\n\ndef on_confirm_clicked(b):\n    with output_display:\n        clear_output()\n        module1_data['ai_output'] = ai_output_widget.value\n        print(\"=\"*70)\n        print(\"AI OUTPUT TO EVALUATE\")\n        print(\"=\"*70)\n        print()\n        print(module1_data['ai_output'])\n        print()\n        print(\"=\"*70)\n\nconfirm_button.on_click(on_confirm_clicked)\n\ndisplay(confirm_button)\ndisplay(output_display)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubric selection widget\n",
    "rubric_selector = widgets.Dropdown(\n",
    "    options=list(RUBRICS.keys()),\n",
    "    value=scenario['default_rubric'],\n",
    "    description='Select Rubric:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "rubric_justification = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Why did you choose this rubric? How does it fit your content?',\n",
    "    description='Justification:',\n",
    "    layout=widgets.Layout(width='100%', height='80px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"SELECT YOUR RUBRIC\")\n",
    "print(\"=\"*70)\n",
    "display(rubric_selector)\n",
    "display(rubric_justification)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Define the three rubrics\nRUBRICS = {\n    'General Communication': {\n        'description': 'Best for: Science Explainers, Museum Panels, Educational Analogies, Infographic Structures',\n        'criteria': [\n            {\n                'name': 'Clarity',\n                'levels': {\n                    1: 'Confusing, jargon-heavy, hard to follow',\n                    3: 'Mostly clear with some unclear parts',\n                    5: 'Crystal clear, no confusion, well-explained'\n                }\n            },\n            {\n                'name': 'Audience Fit',\n                'levels': {\n                    1: 'Wrong level/tone for target audience',\n                    3: 'Generally appropriate for audience',\n                    5: 'Perfectly tailored to audience needs'\n                }\n            },\n            {\n                'name': 'Structure',\n                'levels': {\n                    1: 'Disorganized, hard to follow flow',\n                    3: 'Logical but could be tighter',\n                    5: 'Exceptionally well-organized, smooth flow'\n                }\n            },\n            {\n                'name': 'Specificity',\n                'levels': {\n                    1: 'Vague, generic statements',\n                    3: 'Some specifics mixed with generic content',\n                    5: 'Rich in concrete details and examples'\n                }\n            },\n            {\n                'name': 'Accuracy',\n                'levels': {\n                    1: 'Factual errors present',\n                    3: 'Mostly accurate with minor issues',\n                    5: 'Fully accurate and defensible'\n                }\n            }\n        ]\n    },\n    'Persuasion/Campaign': {\n        'description': 'Best for: PSAs, Product Pitches, Debate Positions',\n        'criteria': [\n            {\n                'name': 'Message Focus',\n                'levels': {\n                    1: 'Unclear or multiple conflicting messages',\n                    3: 'Clear message but could be sharper',\n                    5: 'Single, powerful, memorable message'\n                }\n            },\n            {\n                'name': 'Evidence/Support',\n                'levels': {\n                    1: 'No evidence or weak anecdotes only',\n                    3: 'Some credible support provided',\n                    5: 'Strong, credible, well-integrated evidence'\n                }\n            },\n            {\n                'name': 'Call to Action',\n                'levels': {\n                    1: 'Missing or extremely vague',\n                    3: 'Present but not compelling',\n                    5: 'Clear, specific, highly motivating'\n                }\n            },\n            {\n                'name': 'Tone Fit',\n                'levels': {\n                    1: 'Mismatched tone for purpose/audience',\n                    3: 'Appropriate but not distinctive',\n                    5: 'Perfectly calibrated, engaging tone'\n                }\n            },\n            {\n                'name': 'Ethics',\n                'levels': {\n                    1: 'Misleading or manipulative',\n                    3: 'Honest with some weak points',\n                    5: 'Transparent, balanced, trustworthy'\n                }\n            }\n        ]\n    },\n    'Creative/Narrative': {\n        'description': 'Best for: Short Narratives, Creative Content',\n        'criteria': [\n            {\n                'name': 'Engagement',\n                'levels': {\n                    1: 'Boring, loses attention quickly',\n                    3: 'Intermittently interesting',\n                    5: 'Captivating throughout'\n                }\n            },\n            {\n                'name': 'Coherence',\n                'levels': {\n                    1: 'Confusing, disjointed narrative',\n                    3: 'Logical but rough transitions',\n                    5: 'Seamlessly flows, well-integrated'\n                }\n            },\n            {\n                'name': 'Creativity',\n                'levels': {\n                    1: 'Clich√©d, completely predictable',\n                    3: 'Some original elements',\n                    5: 'Fresh, unexpected, memorable'\n                }\n            },\n            {\n                'name': 'Constraint Adherence',\n                'levels': {\n                    1: 'Ignores key requirements',\n                    3: 'Meets most constraints',\n                    5: 'Perfectly fulfills all constraints'\n                }\n            }\n        ]\n    }\n}\n\n# Display rubric information\nprint(f\"\\nRECOMMENDED RUBRIC FOR YOUR SCENARIO:\")\nprint(f\"  {scenario['default_rubric']}\")\nprint(f\"\\nALL AVAILABLE RUBRICS:\")\nprint()\nfor rubric_name, rubric_info in RUBRICS.items():\n    print(f\"‚Ä¢ {rubric_name}\")\n    print(f\"  {rubric_info['description']}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 1: Select a Rubric\n\nChoose the rubric that best fits your content type. The **recommended rubric** for your scenario is shown, but you can choose a different one if it fits better.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Display Your Selected Rubric\n",
    "\n",
    "Run this cell to see the full rubric with all criteria and scoring levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rubric(rubric_name):\n",
    "    rubric = RUBRICS[rubric_name]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"{rubric_name.upper()} RUBRIC\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{rubric['description']}\")\n",
    "    print()\n",
    "    \n",
    "    for i, criterion in enumerate(rubric['criteria'], 1):\n",
    "        print(f\"{i}. {criterion['name']}\")\n",
    "        print(f\"   1 (Weak):      {criterion['levels'][1]}\")\n",
    "        print(f\"   3 (Good):      {criterion['levels'][3]}\")\n",
    "        print(f\"   5 (Excellent): {criterion['levels'][5]}\")\n",
    "        print()\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Button to display rubric\n",
    "display_button = widgets.Button(\n",
    "    description='üìã Display Rubric',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "rubric_output = widgets.Output()\n",
    "\n",
    "def on_display_clicked(b):\n",
    "    with rubric_output:\n",
    "        clear_output()\n",
    "        display_rubric(rubric_selector.value)\n",
    "\n",
    "display_button.on_click(on_display_clicked)\n",
    "\n",
    "display(display_button)\n",
    "display(rubric_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Human Evaluation (Your Scores)\n",
    "\n",
    "Now score the AI's output using the rubric. Use your best judgment.\n",
    "\n",
    "**Scoring Scale:** 1, 2, 3, 4, or 5 for each criterion\n",
    "- **1-2:** Closer to \"Weak\" description\n",
    "- **3:** Matches \"Good\" description\n",
    "- **4-5:** Closer to \"Excellent\" description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scoring widgets dynamically based on selected rubric\n",
    "def create_human_scoring_widgets():\n",
    "    selected_rubric = RUBRICS[rubric_selector.value]\n",
    "    human_scores = {}\n",
    "    human_justifications = {}\n",
    "    \n",
    "    print(\"HUMAN EVALUATION (Your Scores)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Score each criterion from 1-5 based on the rubric.\")\n",
    "    print()\n",
    "    \n",
    "    for criterion in selected_rubric['criteria']:\n",
    "        name = criterion['name']\n",
    "        \n",
    "        score_widget = widgets.Dropdown(\n",
    "            options=[1, 2, 3, 4, 5],\n",
    "            description=f\"{name}:\",\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        )\n",
    "        \n",
    "        justification_widget = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder=f'Why this score for {name}? Be specific.',\n",
    "            layout=widgets.Layout(width='100%', height='60px')\n",
    "        )\n",
    "        \n",
    "        human_scores[name] = score_widget\n",
    "        human_justifications[name] = justification_widget\n",
    "        \n",
    "        display(score_widget)\n",
    "        display(justification_widget)\n",
    "        print()\n",
    "    \n",
    "    return human_scores, human_justifications\n",
    "\n",
    "# Button to create scoring interface\n",
    "create_scoring_button = widgets.Button(\n",
    "    description='‚úèÔ∏è Start Scoring',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "scoring_output = widgets.Output()\n",
    "human_scores_dict = {}\n",
    "human_justifications_dict = {}\n",
    "\n",
    "def on_create_scoring_clicked(b):\n",
    "    global human_scores_dict, human_justifications_dict\n",
    "    with scoring_output:\n",
    "        clear_output()\n",
    "        human_scores_dict, human_justifications_dict = create_human_scoring_widgets()\n",
    "\n",
    "create_scoring_button.on_click(on_create_scoring_clicked)\n",
    "\n",
    "print(\"Click 'Start Scoring' after you've displayed and reviewed the rubric:\")\n",
    "display(create_scoring_button)\n",
    "display(scoring_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: AI Self-Evaluation\n",
    "\n",
    "Now go back to your AI system and ask it to evaluate its OWN work using the same rubric.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Go back to the AI conversation from Module 1 (or start a new one and paste the AI's output)\n",
    "\n",
    "2. Copy the **prompt below** (it includes the rubric and asks AI to self-evaluate)\n",
    "\n",
    "3. Paste it into the AI and get its response\n",
    "\n",
    "4. Record the AI's self-scores and reasoning below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate self-evaluation prompt\n",
    "def generate_self_eval_prompt():\n",
    "    selected_rubric = RUBRICS[rubric_selector.value]\n",
    "    \n",
    "    prompt = f\"\"\"I need you to evaluate the quality of your previous response using this rubric:\n",
    "\n",
    "**{rubric_selector.value} Rubric:**\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, criterion in enumerate(selected_rubric['criteria'], 1):\n",
    "        prompt += f\"{i}. **{criterion['name']}**\\n\"\n",
    "        prompt += f\"   - Score 1 (Weak): {criterion['levels'][1]}\\n\"\n",
    "        prompt += f\"   - Score 3 (Good): {criterion['levels'][3]}\\n\"\n",
    "        prompt += f\"   - Score 5 (Excellent): {criterion['levels'][5]}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    \n",
    "    prompt += \"\"\"Please:\n",
    "1. Score your response on EACH criterion (1-5 scale)\n",
    "2. Briefly explain your reasoning for each score (1-2 sentences)\n",
    "3. Identify the TWO WEAKEST areas (lowest scores)\n",
    "4. For each weak area, suggest a specific improvement\n",
    "\n",
    "Format your response clearly with the criterion name, score, and reasoning.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Button to generate and display prompt\n",
    "generate_prompt_button = widgets.Button(\n",
    "    description='üìù Generate Prompt',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "prompt_output = widgets.Output()\n",
    "\n",
    "def on_generate_prompt_clicked(b):\n",
    "    with prompt_output:\n",
    "        clear_output()\n",
    "        prompt = generate_self_eval_prompt()\n",
    "        print(\"=\"*70)\n",
    "        print(\"COPY THIS PROMPT TO YOUR AI\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        print(prompt)\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "\n",
    "generate_prompt_button.on_click(on_generate_prompt_clicked)\n",
    "\n",
    "print(\"\\nGenerate the self-evaluation prompt to give to your AI:\")\n",
    "display(generate_prompt_button)\n",
    "display(prompt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Record AI's Self-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget to record AI's complete self-evaluation\n",
    "ai_self_eval_full = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder=\"Paste the AI's COMPLETE self-evaluation response here...\",\n",
    "    description='AI Self-Eval:',\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "ai_weaknesses_identified = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder=\"What 2 weaknesses did the AI identify? What improvements did it suggest?\",\n",
    "    description='AI Weaknesses:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"RECORD AI'S SELF-EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "display(ai_self_eval_full)\n",
    "display(ai_weaknesses_identified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract AI's Scores\n",
    "\n",
    "Manually enter the scores the AI gave itself for each criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI score entry widgets\n",
    "def create_ai_score_widgets():\n",
    "    selected_rubric = RUBRICS[rubric_selector.value]\n",
    "    ai_scores = {}\n",
    "    \n",
    "    print(\"AI'S SELF-SCORES (Enter what AI scored itself)\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    for criterion in selected_rubric['criteria']:\n",
    "        name = criterion['name']\n",
    "        \n",
    "        score_widget = widgets.Dropdown(\n",
    "            options=[1, 2, 3, 4, 5],\n",
    "            description=f\"{name}:\",\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        )\n",
    "        \n",
    "        ai_scores[name] = score_widget\n",
    "        display(score_widget)\n",
    "    \n",
    "    return ai_scores\n",
    "\n",
    "# Button to create AI score entry\n",
    "create_ai_scores_button = widgets.Button(\n",
    "    description='ü§ñ Enter AI Scores',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "ai_scores_output = widgets.Output()\n",
    "ai_scores_dict = {}\n",
    "\n",
    "def on_create_ai_scores_clicked(b):\n",
    "    global ai_scores_dict\n",
    "    with ai_scores_output:\n",
    "        clear_output()\n",
    "        ai_scores_dict = create_ai_score_widgets()\n",
    "\n",
    "create_ai_scores_button.on_click(on_create_ai_scores_clicked)\n",
    "\n",
    "display(create_ai_scores_button)\n",
    "display(ai_scores_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Evaluations\n",
    "\n",
    "Now let's see where you and the AI agree and disagree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "compare_button = widgets.Button(\n",
    "    description='üìä Compare Scores',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "compare_output = widgets.Output()\n",
    "\n",
    "def on_compare_clicked(b):\n",
    "    with compare_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if not human_scores_dict or not ai_scores_dict:\n",
    "            print(\"‚ùå Please complete both human scoring and AI score entry first.\")\n",
    "            return\n",
    "        \n",
    "        # Create comparison table\n",
    "        selected_rubric = RUBRICS[rubric_selector.value]\n",
    "        \n",
    "        comparison_data = []\n",
    "        total_human = 0\n",
    "        total_ai = 0\n",
    "        disagreements = []\n",
    "        \n",
    "        for criterion in selected_rubric['criteria']:\n",
    "            name = criterion['name']\n",
    "            human_score = human_scores_dict[name].value\n",
    "            ai_score = ai_scores_dict[name].value\n",
    "            diff = human_score - ai_score\n",
    "            \n",
    "            total_human += human_score\n",
    "            total_ai += ai_score\n",
    "            \n",
    "            agreement = \"‚úì Match\" if diff == 0 else (\"‚ö†Ô∏è Human higher\" if diff > 0 else \"‚ö†Ô∏è AI higher\")\n",
    "            \n",
    "            if abs(diff) >= 2:\n",
    "                disagreements.append((name, human_score, ai_score, diff))\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Criterion': name,\n",
    "                'Human Score': human_score,\n",
    "                'AI Score': ai_score,\n",
    "                'Difference': diff,\n",
    "                'Agreement': agreement\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"HUMAN VS. AI EVALUATION COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        print(df.to_string(index=False))\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total Human Score: {total_human}/{len(selected_rubric['criteria']) * 5}\")\n",
    "        print(f\"Total AI Score:    {total_ai}/{len(selected_rubric['criteria']) * 5}\")\n",
    "        print(f\"Overall Difference: {total_human - total_ai}\")\n",
    "        print()\n",
    "        \n",
    "        if disagreements:\n",
    "            print(\"SIGNIFICANT DISAGREEMENTS (¬±2 or more):\")\n",
    "            for name, h_score, a_score, diff in disagreements:\n",
    "                if diff > 0:\n",
    "                    print(f\"  ‚Ä¢ {name}: You scored {h_score}, AI scored {a_score} (AI underrated itself by {diff})\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ {name}: You scored {h_score}, AI scored {a_score} (AI overrated itself by {abs(diff)})\")\n",
    "        else:\n",
    "            print(\"No major disagreements (all within ¬±1 point)\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "\n",
    "compare_button.on_click(on_compare_clicked)\n",
    "\n",
    "display(compare_button)\n",
    "display(compare_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Reflection on Disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_analysis = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Where did you most disagree with AI? Why do you think this disagreement occurred?',\n",
    "    description='Analysis:',\n",
    "    layout=widgets.Layout(width='100%', height='120px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"\\nANALYZE DISAGREEMENTS\")\n",
    "print(\"=\"*70)\n",
    "display(disagreement_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üìù Recording Your Evaluation\n\n**Record ALL of the following on your Lab 11 Answer Sheet:**\n\n1. **Rubric Selection**: Which rubric you chose and why\n2. **Your Scores**: Your score (1-5) for each criterion with brief justification\n3. **AI Self-Scores**: The AI's score for each criterion\n4. **Comparison**: Note where you and the AI disagreed significantly (¬±2 points)\n5. **Weakest Areas**: The 2 lowest-scoring criteria (for use in Module 3)\n\n**The interactive widgets above are for exploration only - record everything on paper.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Q8: Human Evaluation\n\nRecord your scores for each criterion. Include brief justification for each score.\n\n*(Answer on your answer sheet - use the comparison table)*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Q7: Rubric Selection\n\nWhich rubric did you choose? Why is it the best fit for your content type?\n\n*(Answer on your answer sheet)*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Module 2 Questions\n\nAnswer these on your Lab 11 Answer Sheet.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: AI Self-Scores\n",
    "\n",
    "What scores did the AI give itself? Record the AI's reasoning for each score.\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: AI-Identified Weaknesses\n",
    "\n",
    "Which 2 weaknesses did the AI identify in its own work? What improvements did it suggest?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Biggest Disagreement\n",
    "\n",
    "Where did you MOST disagree with the AI's self-evaluation? Why do you think this disagreement occurred? Was the AI overconfident or underconfident?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Accomplished\n",
    "\n",
    "‚úì Selected a rubric that fits your content type\n",
    "\n",
    "‚úì Systematically scored the AI's output (human judgment)\n",
    "\n",
    "‚úì Asked the AI to self-evaluate using the same rubric\n",
    "\n",
    "‚úì Compared human vs. AI scores\n",
    "\n",
    "‚úì Identified areas of agreement and disagreement\n",
    "\n",
    "‚úì Found the 2 weakest criteria for revision\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "You've likely discovered that **AI self-evaluation is unreliable**. AI may:\n",
    "- Be **overconfident** (score itself higher than it deserves)\n",
    "- Be **underconfident** (score itself lower than deserved)\n",
    "- Miss obvious flaws that humans catch\n",
    "- Focus on the wrong aspects of quality\n",
    "\n",
    "This is why **human judgment remains essential** in evaluation!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 3 (Revise & Compare)**, you will:\n",
    "- Ask the AI to revise its work, targeting the 2 weakest criteria\n",
    "- Score the revised version\n",
    "- Compare: Did it improve? Did new problems appear?\n",
    "- Test whether AI is better at revision than self-evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}