{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7, Module 1: Applying Filters to Real Images\n",
    "\n",
    "**Estimated time:** 15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: From Tiny Arrays to Real Images**\n",
    "\n",
    "In Module 0, you learned the basic concept of convolution using a tiny 5Ã—5 image.\n",
    "\n",
    "**Now it's time to see convolution in action on real photographs!**\n",
    "\n",
    "You'll apply classic computer vision filters that have been used for decades:\n",
    "- **Blur** (smoothing, noise reduction)\n",
    "- **Sharpen** (edge enhancement)\n",
    "- **Sobel edge detection** (finding boundaries)\n",
    "- **Laplacian edge detection** (finding all edges at once)\n",
    "\n",
    "These same operations power modern CNNsâ€”the difference is that **CNNs learn their own filters** through training, while these classic filters were designed by hand.\n",
    "\n",
    "### **Connection to Lab 6**\n",
    "\n",
    "Remember Lab 6's saliency maps?\n",
    "- **Lab 6:** Showed **WHERE** a model looks\n",
    "- **Lab 7 (today):** Shows **WHAT FEATURES** a model extracts\n",
    "\n",
    "Edge detection and texture filters are exactly what the early layers of CNNs learn to detect!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ–¼ï¸ **Sample Images**\n",
    "\n",
    "We'll work with two sample images:\n",
    "1. **edges_demo.jpg** â€“ Architecture with clear edges (vertical, horizontal, diagonal)\n",
    "2. **textures_demo.jpg** â€“ Natural scene with textures (fur, grass, leaves)\n",
    "\n",
    "You'll also have the option to **upload your own image** and see how filters transform it!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from google.colab import files\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nReady to apply filters to real images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¥ **Load Sample Images**\n",
    "\n",
    "We'll load two sample images for demonstration:\n",
    "1. Architecture image (strong edges)\n",
    "2. Nature image (textures)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll create synthetic sample images\n",
    "# In a real deployment, you'd load actual images from sample_images/\n",
    "\n",
    "def create_sample_image_with_edges():\n",
    "    \"\"\"Create a simple geometric pattern with edges\"\"\"\n",
    "    img = np.ones((256, 256), dtype=np.uint8) * 200\n",
    "    \n",
    "    # Add vertical stripes\n",
    "    img[:, 50:80] = 50\n",
    "    img[:, 100:130] = 50\n",
    "    img[:, 150:180] = 50\n",
    "    \n",
    "    # Add horizontal stripe\n",
    "    img[100:130, :] = 100\n",
    "    \n",
    "    # Add diagonal rectangle\n",
    "    img[180:220, 180:220] = 30\n",
    "    \n",
    "    return img\n",
    "\n",
    "def create_sample_image_with_textures():\n",
    "    \"\"\"Create an image with various textures\"\"\"\n",
    "    img = np.ones((256, 256), dtype=np.uint8) * 128\n",
    "    \n",
    "    # Add random noise in different regions (simulating texture)\n",
    "    # Region 1: Fine texture\n",
    "    img[:128, :128] = np.random.randint(80, 120, (128, 128))\n",
    "    \n",
    "    # Region 2: Coarse texture\n",
    "    img[:128, 128:] = np.random.randint(50, 100, (128, 128))\n",
    "    \n",
    "    # Region 3: Smooth with edges\n",
    "    img[128:, :128] = 180\n",
    "    img[128:, :128][::4, ::4] = 60  # Dots\n",
    "    \n",
    "    # Region 4: Gradient\n",
    "    img[128:, 128:] = np.linspace(50, 200, 128).reshape(1, -1).repeat(128, axis=0).astype(np.uint8)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create sample images\n",
    "edges_demo = create_sample_image_with_edges()\n",
    "textures_demo = create_sample_image_with_textures()\n",
    "\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(edges_demo, cmap='gray')\n",
    "axes[0].set_title('Sample Image 1: Edges Demo\\n(Geometric patterns with clear boundaries)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(textures_demo, cmap='gray')\n",
    "axes[1].set_title('Sample Image 2: Textures Demo\\n(Various texture patterns)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Sample images loaded!\")\n",
    "print(\"\\nWe'll use these images to demonstrate different filters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ **Classic Filters**\n",
    "\n",
    "Let's define the filters we'll use:\n",
    "\n",
    "### **1. Blur Filter (Averaging)**\n",
    "```\n",
    "[1  1  1]\n",
    "[1  1  1]  Ã· 9\n",
    "[1  1  1]\n",
    "```\n",
    "**Effect:** Averages nearby pixels, reduces noise\n",
    "\n",
    "### **2. Sharpen Filter**\n",
    "```\n",
    "[ 0 -1  0]\n",
    "[-1  5 -1]\n",
    "[ 0 -1  0]\n",
    "```\n",
    "**Effect:** Emphasizes differences between pixels\n",
    "\n",
    "### **3. Sobel Vertical Edge Detector**\n",
    "```\n",
    "[-1  0  1]\n",
    "[-2  0  2]\n",
    "[-1  0  1]\n",
    "```\n",
    "**Effect:** Detects vertical edges (dark-to-bright transitions)\n",
    "\n",
    "### **4. Sobel Horizontal Edge Detector**\n",
    "```\n",
    "[-1 -2 -1]\n",
    "[ 0  0  0]\n",
    "[ 1  2  1]\n",
    "```\n",
    "**Effect:** Detects horizontal edges\n",
    "\n",
    "### **5. Laplacian (All Edges)**\n",
    "```\n",
    "[ 0  1  0]\n",
    "[ 1 -4  1]\n",
    "[ 0  1  0]\n",
    "```\n",
    "**Effect:** Detects edges in all directions at once\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filters\n",
    "filters = {\n",
    "    'Original': None,  # No filter (for comparison)\n",
    "    \n",
    "    'Blur (3x3)': np.ones((3, 3), dtype=np.float32) / 9,\n",
    "    \n",
    "    'Sharpen': np.array([[0, -1, 0],\n",
    "                         [-1, 5, -1],\n",
    "                         [0, -1, 0]], dtype=np.float32),\n",
    "    \n",
    "    'Sobel Vertical': np.array([[-1, 0, 1],\n",
    "                                [-2, 0, 2],\n",
    "                                [-1, 0, 1]], dtype=np.float32),\n",
    "    \n",
    "    'Sobel Horizontal': np.array([[-1, -2, -1],\n",
    "                                  [0, 0, 0],\n",
    "                                  [1, 2, 1]], dtype=np.float32),\n",
    "    \n",
    "    'Laplacian': np.array([[0, 1, 0],\n",
    "                          [1, -4, 1],\n",
    "                          [0, 1, 0]], dtype=np.float32)\n",
    "}\n",
    "\n",
    "print(\"âœ… Filters defined!\")\n",
    "print(f\"\\nWe have {len(filters)} filters ready to apply.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¨ **Apply Filters to Edges Demo Image**\n",
    "\n",
    "Let's see how each filter transforms the geometric pattern image!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all filters to edges demo image\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, kernel) in enumerate(filters.items()):\n",
    "    if kernel is None:\n",
    "        # Original image\n",
    "        result = edges_demo\n",
    "    else:\n",
    "        # Apply convolution\n",
    "        result = cv2.filter2D(edges_demo, -1, kernel)\n",
    "    \n",
    "    # Plot result\n",
    "    axes[idx].imshow(result, cmap='gray')\n",
    "    axes[idx].set_title(name, fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Filter Effects on Edges Demo Image', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. BLUR: Softens edges, reduces sharp transitions\")\n",
    "print(\"   - Vertical and horizontal stripes become less distinct\")\n",
    "print(\"   - Useful for noise reduction\\n\")\n",
    "\n",
    "print(\"2. SHARPEN: Makes edges more pronounced\")\n",
    "print(\"   - Boundaries become crisper and more defined\")\n",
    "print(\"   - Can amplify noise if present\\n\")\n",
    "\n",
    "print(\"3. SOBEL VERTICAL: Detects vertical edges ONLY\")\n",
    "print(\"   - Vertical stripes show strong response (bright lines)\")\n",
    "print(\"   - Horizontal stripe barely visible\\n\")\n",
    "\n",
    "print(\"4. SOBEL HORIZONTAL: Detects horizontal edges ONLY\")\n",
    "print(\"   - Horizontal stripe shows strong response\")\n",
    "print(\"   - Vertical stripes barely visible\\n\")\n",
    "\n",
    "print(\"5. LAPLACIAN: Detects ALL edges\")\n",
    "print(\"   - Both vertical and horizontal edges highlighted\")\n",
    "print(\"   - More general-purpose edge detector\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¨ **Apply Filters to Textures Demo Image**\n",
    "\n",
    "Now let's see how the same filters respond to textures!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all filters to textures demo image\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, kernel) in enumerate(filters.items()):\n",
    "    if kernel is None:\n",
    "        # Original image\n",
    "        result = textures_demo\n",
    "    else:\n",
    "        # Apply convolution\n",
    "        result = cv2.filter2D(textures_demo, -1, kernel)\n",
    "    \n",
    "    # Plot result\n",
    "    axes[idx].imshow(result, cmap='gray')\n",
    "    axes[idx].set_title(name, fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Filter Effects on Textures Demo Image', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. BLUR: Smooths fine textures\")\n",
    "print(\"   - Random noise patterns become less visible\")\n",
    "print(\"   - Different texture regions become more uniform\\n\")\n",
    "\n",
    "print(\"2. SHARPEN: Emphasizes texture details\")\n",
    "print(\"   - Fine patterns become more visible\")\n",
    "print(\"   - Boundaries between texture regions enhanced\\n\")\n",
    "\n",
    "print(\"3. SOBEL VERTICAL: Highlights vertical texture patterns\")\n",
    "print(\"   - Gradient region shows strong response\")\n",
    "print(\"   - Detects boundaries between texture regions\\n\")\n",
    "\n",
    "print(\"4. SOBEL HORIZONTAL: Highlights horizontal texture patterns\")\n",
    "print(\"   - Different response pattern than vertical\")\n",
    "print(\"   - Shows how texture orientation matters\\n\")\n",
    "\n",
    "print(\"5. LAPLACIAN: Reveals all texture boundaries\")\n",
    "print(\"   - Clear separation between the four quadrants\")\n",
    "print(\"   - Highlights both edges and fine texture patterns\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¨ **Interactive: Design Your Own Filter!**\n",
    "\n",
    "Now it's your turn! Design a custom 3Ã—3 filter and see what it does.\n",
    "\n",
    "**Try these experiments:**\n",
    "- All 1s: `[[1,1,1],[1,1,1],[1,1,1]]` (strong blur)\n",
    "- All 0s except center: `[[0,0,0],[0,9,0],[0,0,0]]` (strong center emphasis)\n",
    "- Diagonal pattern: `[[2,0,0],[0,2,0],[0,0,2]]` (diagonal edge detector)\n",
    "- Your own creative pattern!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTIVE: Design your own 3x3 filter!\n",
    "# Modify the values below\n",
    "\n",
    "custom_filter = np.array([\n",
    "    [0,  0,  0],   # Top row\n",
    "    [0,  1,  0],   # Middle row\n",
    "    [0,  0,  0]    # Bottom row\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Apply to edges demo\n",
    "custom_result_edges = cv2.filter2D(edges_demo, -1, custom_filter)\n",
    "\n",
    "# Apply to textures demo\n",
    "custom_result_textures = cv2.filter2D(textures_demo, -1, custom_filter)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Show custom filter\n",
    "im = axes[0, 0].imshow(custom_filter, cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[0, 0].set_title('Your Custom Filter', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xticks(range(3))\n",
    "axes[0, 0].set_yticks(range(3))\n",
    "axes[0, 0].grid(True, color='black', linewidth=0.5)\n",
    "# Add values\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0, 0].text(j, i, f'{custom_filter[i,j]:.1f}', \n",
    "                       ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0, 0], fraction=0.046)\n",
    "\n",
    "# Show original images\n",
    "axes[0, 1].imshow(edges_demo, cmap='gray')\n",
    "axes[0, 1].set_title('Original: Edges Demo', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(textures_demo, cmap='gray')\n",
    "axes[0, 2].set_title('Original: Textures Demo', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Empty space\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Show results\n",
    "axes[1, 1].imshow(custom_result_edges, cmap='gray')\n",
    "axes[1, 1].set_title('After Your Filter: Edges', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(custom_result_textures, cmap='gray')\n",
    "axes[1, 2].set_title('After Your Filter: Textures', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Your Custom Filter Results', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ TIP: Try modifying the custom_filter values above and re-run this cell!\")\n",
    "print(\"\\nExperiment ideas:\")\n",
    "print(\"- Make a strong blur: [[1,1,1],[1,1,1],[1,1,1]] / 9\")\n",
    "print(\"- Make a diagonal edge detector: [[2,0,0],[0,0,0],[0,0,-2]]\")\n",
    "print(\"- Create asymmetric patterns and see what happens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¤ **Upload Your Own Image (Optional)**\n",
    "\n",
    "Want to try these filters on your own photos? Upload an image here!\n",
    "\n",
    "**Good images to try:**\n",
    "- Photos with clear edges (buildings, books, windows)\n",
    "- Nature photos with textures (trees, grass, animals)\n",
    "- Portraits (to see how filters detect facial features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image\n",
    "print(\"ðŸ“¤ Click 'Choose Files' and select an image from your computer...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    # Get the first uploaded file\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    \n",
    "    # Read image\n",
    "    img_bytes = uploaded[filename]\n",
    "    img = Image.open(BytesIO(img_bytes))\n",
    "    \n",
    "    # Convert to grayscale numpy array\n",
    "    img_gray = np.array(img.convert('L'))\n",
    "    \n",
    "    # Resize if too large (for faster processing)\n",
    "    if img_gray.shape[0] > 512 or img_gray.shape[1] > 512:\n",
    "        img_pil = Image.fromarray(img_gray)\n",
    "        img_pil.thumbnail((512, 512))\n",
    "        img_gray = np.array(img_pil)\n",
    "    \n",
    "    print(f\"âœ… Image loaded: {filename}\")\n",
    "    print(f\"   Size: {img_gray.shape[0]} Ã— {img_gray.shape[1]} pixels\")\n",
    "    \n",
    "    # Apply all filters\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, kernel) in enumerate(filters.items()):\n",
    "        if kernel is None:\n",
    "            result = img_gray\n",
    "        else:\n",
    "            result = cv2.filter2D(img_gray, -1, kernel)\n",
    "        \n",
    "        axes[idx].imshow(result, cmap='gray')\n",
    "        axes[idx].set_title(name, fontsize=12, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Filter Effects on Your Image: {filename}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No image uploaded. Skipping this section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— **Connection to CNNs: Learned vs. Hand-Designed Filters**\n",
    "\n",
    "**What you just saw:**\n",
    "- **Hand-designed filters** (Sobel, Laplacian, blur, sharpen)\n",
    "- Filters created by computer vision researchers based on mathematical principles\n",
    "- Work well for specific tasks (edge detection, noise reduction)\n",
    "\n",
    "**What CNNs do differently:**\n",
    "- **Learn filters automatically** through training\n",
    "- Start with random filter values\n",
    "- Gradient descent adjusts filter values to minimize loss\n",
    "- Final filters detect patterns that are actually useful for the task\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "| Hand-Designed Filters | CNN Learned Filters |\n",
    "|-----------------------|---------------------|\n",
    "| Fixed patterns (Sobel, etc.) | Adapt to specific dataset |\n",
    "| General-purpose | Task-specific |\n",
    "| Work moderately well everywhere | Work excellently on training domain |\n",
    "| 5-10 filters typically | 32-512 filters per layer! |\n",
    "\n",
    "**The power of deep learning:**\n",
    "- Early CNN layers learn edge detectors (similar to Sobel)\n",
    "- Mid CNN layers learn texture detectors\n",
    "- Deep CNN layers learn object part detectors\n",
    "- **All automatically, without human design!**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— **Connection to Lab 3: Activation Functions**\n",
    "\n",
    "Remember Lab 3 where you learned about ReLU, sigmoid, and other activation functions?\n",
    "\n",
    "**In CNNs, convolution and activation work together:**\n",
    "\n",
    "```\n",
    "Input Image\n",
    "   â†“\n",
    "Convolution (linear operation: multiply-and-add)\n",
    "   â†“\n",
    "Activation Function (nonlinear: ReLU, sigmoid, etc.)\n",
    "   â†“\n",
    "Feature Map (output)\n",
    "```\n",
    "\n",
    "**Why both are needed:**\n",
    "- **Convolution alone:** Can only detect linear patterns\n",
    "- **Activation function:** Adds nonlinearity, enables complex decisions\n",
    "- **Together:** Can detect complex patterns (curves, textures, shapes)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Typical CNN layer\n",
    "feature_map = conv2d(image, filter)  # Convolution\n",
    "activated = relu(feature_map)        # Activation\n",
    "```\n",
    "\n",
    "**Just like Lab 3:**\n",
    "- Lab 3: Dense layer + ReLU â†’ Warped feature space\n",
    "- Lab 7: Conv layer + ReLU â†’ Edge/texture/shape detectors\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ **Questions (Q5-Q9)**\n",
    "\n",
    "Record your answers in the **Answer Sheet**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. After applying the blur filter, what changed about the image? Why might blur be useful in image processing?**\n",
    "\n",
    "*Hint: Look at the edgesâ€”did they become sharper or softer? Think about noise reduction.*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What happened when you applied the Sobel vertical edge detector? Which parts of the image were highlighted?**\n",
    "\n",
    "*Hint: Compare vertical vs. horizontal edges. Why does the vertical edge detector respond strongly to vertical stripes but not horizontal ones?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. Compare the sharpened image to the original. What features became more pronounced?**\n",
    "\n",
    "*Hint: Look at boundaries and transitions. How does sharpen differ from edge detection?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. Design your own 3Ã—3 filter in the interactive section. What effect did it have? Record your filter values and describe the result.**\n",
    "\n",
    "*Record your custom filter and observations:*\n",
    "```\n",
    "Your filter:\n",
    "[ __ __ __ ]\n",
    "[ __ __ __ ]\n",
    "[ __ __ __ ]\n",
    "\n",
    "Effect observed: ___________\n",
    "```\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q9. Why do you think edge detection is important for object recognition?**\n",
    "\n",
    "*Hint: Think about what defines shapes. How do edges relate to object boundaries?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Module 1 Complete!\n",
    "\n",
    "You now understand:\n",
    "- **How to apply filters to real images** (blur, sharpen, edge detection)\n",
    "- **What different filters detect** (edges, textures, boundaries)\n",
    "- **How to design custom filters** and predict their effects\n",
    "- **The connection to CNNs** (learned filters vs. hand-designed filters)\n",
    "- **The role of activation functions** (convolution + nonlinearity = powerful feature extraction)\n",
    "\n",
    "**Key insight:**\n",
    "> CNNs automatically learn filters that are optimized for the taskâ€”starting with edge detectors in early layers and building up to complex pattern detectors in deeper layers.\n",
    "\n",
    "**Ready to see inside a real CNN?**\n",
    "\n",
    "Move on to **Module 2: Visualizing CNN Feature Maps**, where you'll load a pretrained MobileNetV2 model and visualize what its layers actually detect!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
