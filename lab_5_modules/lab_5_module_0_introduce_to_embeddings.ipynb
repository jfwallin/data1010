{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b39edd",
   "metadata": {},
   "source": "# Lab 5, Module 0: Introduction to Embeddings\n\n**Estimated time:** 20 minutes\n\n---\n\n## **Opening: How Does Your Brain Know Meaning?**\n\nThink about this for a moment: How does your brain know that \"cat\" and \"dog\" are similar, but \"cat\" and \"galaxy\" are completely different? You've never explicitly been taught a rule that says \"pets go together\" and \"space objects go separately.\" Yet somehow, through experience, your brain has learned these relationships.\n\n**Modern AI systems like ChatGPT work the same way.** They don't store word definitions or rules. Instead, they represent words and sentences as **vectors**‚Äîlists of numbers‚Äîwhere similar meanings point in similar directions in a high-dimensional space.\n\nThis module gives you an intuitive, \"behind the curtain\" understanding of how that works.\n\n---\n\n# üìò **How Do Embeddings Work?**  \n### *A simple, concrete explanation*\n\nModern AI systems (like ChatGPT and other foundation models) represent **words** and **sentences** as **vectors**‚Äîlists of numbers. These vectors encode meaning based on how language is used across millions of sentences.\n\n---\n\n## **1. Words become vectors using \"distributed meaning\"**\n\nEvery word (e.g., **\"galaxy\"**) becomes a long list of numbers:\n\n$[0.12, -0.88, 0.43, ... , 0.04]$\n\nEach number captures a tiny statistical association learned from language use.\n\nYou can think of each dimension as asking a vague, fuzzy question like:\n\n- **\"How strongly does this idea tend to appear in scientific or technical contexts?\"**\n- **\"How much does this concept appear in discussions about animals?\"**\n- **\"How much does this idea align with everyday activities or objects?\"**\n\nNo single dimension has a clean human meaning.  \nBut **together**, they form a rich representation of the word's usage patterns.\n\nWords used in similar ways end up with **similar vectors**.\n\n---\n\n## **2. The model learns meaning by predicting missing words**\n\nEmbedding models are typically trained with a simple game:\n\n> **Look at a sentence with a missing word and guess what goes there.**\n\nExample:\n\n*\"The _____ orbits the Sun every year.\"*\n\nThe model is rewarded for predicting words like:\n- *Earth*\n- *planet*\n- *object*\n\nand penalized for predicting:\n- *banana*\n- *giraffe*\n\nAfter doing this **millions of times**, the model learns patterns such as:\n\n- Which words appear in similar contexts  \n- Which words are interchangeable in certain situations  \n- How tone, topic, and structure influence meaning  \n\nThis process pulls related words together in vector space.\n\n---\n\n## **3. Sentences become vectors too**\n\nEmbedding models for sentences (like the one you'll use in Module 2) work by:\n\n1. Converting each word into a vector  \n2. Processing the whole sentence through a small transformer  \n3. Producing one final vector that represents the meaning of the sentence  \n\nTwo sentences with the *same meaning* end up very close:\n\n- \"The Earth orbits the Sun.\"  \n- \"The Sun is orbited by the Earth.\"\n\nEven though the wording is different.\n\n---\n\n## **4. Measuring similarity: cosine similarity**\n\nTo compare meanings, we use **cosine similarity**, which measures the angle between two vectors:\n\n- **1.0** ‚Üí almost identical meaning  \n- **0.8** ‚Üí very similar  \n- **0.4** ‚Üí loosely related  \n- **0.0** ‚Üí unrelated  \n\nThis is the core idea behind semantic search:  \n> Find the corpus sentences whose vectors are closest to the query vector.\n\n---\n\n## **5. The big picture: why this matters**\n\n- Embeddings are **numerical fingerprints of meaning**.  \n- Similar meanings ‚Üí similar vectors.  \n- The model learns this automatically through massive exposure to language.  \n- Geometry (distances and directions) encodes semantic relationships.  \n- This is the same idea as **hidden representations** from Lab 4‚Äî  \n  just scaled up to hundreds of millions of parameters.\n\n**Connection to Lab 4:** Remember how hidden layers in neural networks created new representations that made problems solvable? Embeddings do the same thing, but for language. They transform words into a space where \"meaning\" becomes measurable geometry."
  },
  {
   "cell_type": "markdown",
   "id": "6f87e4b8",
   "metadata": {},
   "source": "# Building Our Own Tiny Embedding System\n\n## Why Build from Scratch?\n\nYou might wonder: \"If professional embedding models exist, why build our own?\"\n\n**Answer:** Building the simplest possible version helps you understand the core idea. Once you see how a tiny 27-sentence embedding system works, you'll understand what GloVe, BERT, and GPT are doing‚Äîjust at a much larger scale.\n\nThink of this as building a bicycle before learning to fly a plane. Same basic principles, different scale.\n\n---\n\n## üß± How We Create an Embedding Matrix (Simple Example)\n\nTo understand how word embeddings work, we start by building a **co-occurrence matrix**.  \nThis matrix captures *how often words appear near each other* in our tiny corpus.\n\n### 1. Build an empty matrix\nWe create a matrix with one row and one column for every word in the vocabulary.\n\nIf we have 92 words, this becomes a **92 √ó 92** table.\n\n- **Rows** represent the \"target\" word  \n- **Columns** represent the \"context\" word  \n- Entries store **how many times** they appear together\n\nAt the start, everything is zero.\n\n---\n\n### 2. Fill the matrix by counting co-occurrences\nFor each sentence:\n\n1. Split it into words  \n2. For every pair of words in that sentence  \n3. Add **+1** to the cell for (word‚ÇÅ, word‚ÇÇ)\n\nExample sentence: *\"cats chase mice\"*\n\nWe add +1 to: (cats, chase), (cats, mice), (chase, cats), (chase, mice), (mice, cats), (mice, chase)\n\n---\n\n### 3. The matrix becomes a simple embedding\nAfter processing all sentences:\n\n- Row **i** contains the \"context fingerprint\" of word *i*  \n- Words that appear in **similar contexts** have **similar rows**  \n- These rows *are* our first version of embeddings\n\nFor example:\n\n- \"cats\" and \"dogs\" may have similar rows because they appear next to \"pets,\" \"chase,\" etc.  \n- \"stars\" and \"galaxies\" may have similar rows because both appear near science words  \n- \"neural\" and \"networks\" will strongly co-occur\n\nThis creates meaningful structure without any neural networks.\n\n---\n\n## ‚≠ê 4. Why real embeddings reduce dimension (PCA, SVD, neural models)\n\nOur tiny embedding matrix is **vocab_size √ó vocab_size**.  \nWith a vocabulary of 50,000 words, that becomes a **50,000 √ó 50,000** matrix ‚Äî far too big and noisy.\n\nReal embedding systems (word2vec, GloVe, MiniLM, BERT, GPT) therefore:\n\n### ‚úî Learn a **much smaller number of dimensions**  \nusually **50‚Äì1000**, instead of tens of thousands.\n\n### ‚úî Use mathematical tools to compress the co-occurrence structure:\n- **SVD (Singular Value Decomposition)** in GloVe  \n- **PCA-like dimensionality reduction**  \n- **Neural networks (skip-gram, transformers)** that learn compact vectors directly  \n\n### ‚úî The goal is to keep the **important patterns**  \nand discard the noise.\n\nYou can think of this like:\n\n> \"Boiling down all the ways a word is used into a small, dense fingerprint of meaning.\"\n\nSo instead of each word having a 50,000-dimensional sparse vector,  \na model might learn a **384-dimensional** vector that captures the same semantic relationships.\n\nThis is why real embeddings:\n- are smaller  \n- generalize better  \n- work fast  \n- encode meaning in a compact geometric space  \n\nNow let's build our tiny embedding system and see this in action!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a789a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9liqvfxfx",
   "source": "---\n\n## üìù Question 1 (Prediction)\n\nBefore running the code below, make a prediction:\n\n**Q1.** Will \"cats\" and \"dogs\" have similar embedding vectors? Why or why not?\n\n*Think about: Do these words appear in similar sentences in the corpus? What contexts do they share?*\n\n**Write your prediction in the answer sheet, then run the code to see if you were correct!**\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 27 sentences\n",
      "\n",
      "Vocabulary size: 92\n",
      "Words: ['algorithms', 'analyze', 'aquariums', 'are', 'astronauts', 'astronomers', 'astronomy', 'at', 'balls', 'be', 'billions', 'birds', 'bring', 'can', 'cats', 'causes', 'chase', 'computers', 'concerts', 'contain', 'cooking', 'data', 'dogs', 'eat', 'exercise', 'fish', 'friends', 'galaxies', 'games', 'good', 'gravity', 'great', 'hamsters', 'help', 'home', 'images', 'in', 'information', 'instructions', 'is', 'learn', 'light', 'loyal', 'machines', 'mice', 'mimic', 'moon', 'move', 'music', 'networks', 'neural', 'objects', 'observe', 'ocean', 'of', 'on', 'orbit', 'patterns', 'people', 'pets', 'planets', 'played', 'problems', 'produce', 'pulls', 'rabbits', 'recognize', 'relaxing', 'robots', 'run', 'running', 'scientists', 'slowly', 'solve', 'space', 'speech', 'stars', 'studies', 'sun', 'swim', 'telescopes', 'the', 'tides', 'to', 'together', 'travel', 'turtles', 'using', 'vegetables', 'video', 'wheels', 'with'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title ### üß± Build Your Own Tiny Word Embedding Model (Expanded Corpus + Clear Plot)\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Expanded Corpus (~3√ó larger)\n",
    "# -------------------------------------\n",
    "corpus = [\n",
    "    # Pets / animals\n",
    "    \"cats are great pets\",\n",
    "    \"dogs are loyal pets\",\n",
    "    \"cats chase mice\",\n",
    "    \"dogs chase balls\",\n",
    "    \"hamsters run on wheels\",\n",
    "    \"fish swim in aquariums\",\n",
    "    \"birds can mimic speech\",\n",
    "    \"rabbits eat vegetables\",\n",
    "    \"turtles move slowly\",\n",
    "\n",
    "    # Astronomy / science\n",
    "    \"astronomy studies stars\",\n",
    "    \"stars produce light\",\n",
    "    \"galaxies contain billions of stars\",\n",
    "    \"planets orbit the sun\",\n",
    "    \"the moon causes ocean tides\",\n",
    "    \"telescopes help astronomers observe galaxies\",\n",
    "    \"gravity pulls objects together\",\n",
    "    \"astronauts travel to space\",\n",
    "\n",
    "    # Technology / AI\n",
    "    \"computers run neural networks\",\n",
    "    \"neural networks learn patterns\",\n",
    "    \"machines can recognize images\",\n",
    "    \"algorithms solve problems\",\n",
    "    \"data scientists analyze information\",\n",
    "    \"robots move using instructions\",\n",
    "\n",
    "    # Daily life / misc\n",
    "    \"music concerts bring people together\",\n",
    "    \"cooking at home is relaxing\",\n",
    "    \"running is good exercise\",\n",
    "    \"video games can be played with friends\"\n",
    "]\n",
    "\n",
    "print(\"Corpus size:\", len(corpus), \"sentences\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. Build vocabulary - make a list of words, an index, and note the vocabulary size\n",
    "# -------------------------------------\n",
    "words = sorted({w for s in corpus for w in s.split()})\n",
    "word_to_idx = {w: i for i, w in enumerate(words)}\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Words:\", words, \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Build co-occurrence matrix\n",
    "# -------------------------------------\n",
    "cooc = np.zeros((vocab_size, vocab_size), dtype=float)\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = sentence.split()\n",
    "    for i, w1 in enumerate(tokens):\n",
    "        for j, w2 in enumerate(tokens):\n",
    "            if i != j:\n",
    "                cooc[word_to_idx[w1], word_to_idx[w2]] += 1\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Normalize rows ‚Üí simple embeddings\n",
    "# -------------------------------------\n",
    "embeddings = cooc / (cooc.sum(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 5. Measure the similarity between sample word pairs\n",
    "# -------------------------------------\n",
    "def similarity(w1, w2):\n",
    "    v1 = embeddings[word_to_idx[w1]].reshape(1, -1)\n",
    "    v2 = embeddings[word_to_idx[w2]].reshape(1, -1)\n",
    "    return cosine_similarity(v1, v2)[0, 0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad9fee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities:\n",
      "\n",
      "cats       ~ dogs       ‚Üí 0.600\n",
      "stars      ~ galaxies   ‚Üí 0.375\n",
      "neural     ~ networks   ‚Üí 0.500\n",
      "pets       ~ stars      ‚Üí 0.000\n",
      "astronomy  ~ galaxies   ‚Üí 0.250\n",
      "games      ~ music      ‚Üí 0.000\n",
      "cats       ~ galaxies   ‚Üí 0.000\n",
      "dogs       ~ networks   ‚Üí 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title ### üß± Experiment 1: look at how closely linked pairs of words are based on their embeddings\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (\"cats\", \"dogs\"),\n",
    "    (\"stars\", \"galaxies\"),\n",
    "    (\"neural\", \"networks\"),\n",
    "    (\"pets\", \"stars\"),\n",
    "    (\"astronomy\", \"galaxies\"),\n",
    "    (\"games\", \"music\"),\n",
    "    (\"cats\",\"galaxies\"),\n",
    "    (\"dogs\",\"networks\"),\n",
    "    \n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\\n\")\n",
    "for a, b in pairs:\n",
    "    print(f\"{a:10s} ~ {b:10s} ‚Üí {similarity(a,b):.3f}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcefef",
   "metadata": {},
   "source": "## Understanding the Results\n\nJust from the sentence context, the very simple embedding system has generated a set of vectors that show how words are related to each other. We can use cosine similarity to see that this system works in this simple context. \n\n**Notice the patterns:**\n- \"cats\" and \"dogs\" have similarity of **0.600** ‚Äî fairly similar!\n- \"stars\" and \"galaxies\" have similarity of **0.375** ‚Äî related\n- \"neural\" and \"networks\" have similarity of **0.500** ‚Äî closely linked\n- But \"pets\" and \"stars\" have similarity of **0.000** ‚Äî completely unrelated\n- \"cats\" and \"galaxies\" also have **0.000** ‚Äî no overlap in usage\n\nDogs are not related to networks, but cats are related to dogs. The embedding system learned this purely from which words appear together in sentences.\n\n---\n\n## üìù Questions 2-3 (Observation)\n\n**Q2.** Looking at the cosine similarities in the output above, which word pair is most similar? Does this match your intuition?\n\n**Q3.** Why do \"cats\" and \"galaxies\" have a similarity of 0.000? What does this tell you about their co-occurrence in the corpus?\n\n*Record your answers in the answer sheet.*\n\n---\n\nNow let's visualize this embedding space to see the clusters more clearly!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f3287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 92)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61262b7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ee5797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "tech"
          ],
          [
           "tech"
          ],
          [
           "tech"
          ],
          [
           "tech"
          ],
          [
           "tech"
          ],
          [
           "tech"
          ],
          [
           "tech"
          ],
          [
           "tech"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>topic=%{customdata[0]}<extra></extra>",
         "hovertext": [
          "algorithms",
          "computers",
          "data",
          "machines",
          "networks",
          "neural",
          "robots",
          "scientists"
         ],
         "legendgroup": "tech",
         "marker": {
          "color": "green",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "tech",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -0.025435162732948675,
          -0.05158195094798076,
          -0.0254351792506168,
          -0.1078763574120277,
          -0.03808734755584729,
          -0.03808734755584731,
          -0.04733473329397233,
          -0.025435179250616783
         ],
         "xaxis": "x",
         "y": [
          -0.030358093819313346,
          -0.09149214257846969,
          -0.030358123587985283,
          0.3342811205513644,
          -0.06512234711454201,
          -0.06512234711454201,
          -0.10183461338397137,
          -0.030358123587985293
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ],
          [
           "other"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>topic=%{customdata[0]}<extra></extra>",
         "hovertext": [
          "analyze",
          "aquariums",
          "are",
          "astronomers",
          "at",
          "be",
          "billions",
          "bring",
          "can",
          "causes",
          "chase",
          "concerts",
          "contain",
          "cooking",
          "eat",
          "exercise",
          "friends",
          "games",
          "good",
          "great",
          "help",
          "home",
          "images",
          "in",
          "information",
          "instructions",
          "is",
          "learn",
          "light",
          "loyal",
          "mimic",
          "move",
          "music",
          "objects",
          "observe",
          "ocean",
          "of",
          "on",
          "orbit",
          "patterns",
          "people",
          "pets",
          "played",
          "problems",
          "produce",
          "pulls",
          "recognize",
          "relaxing",
          "run",
          "running",
          "slowly",
          "solve",
          "space",
          "speech",
          "studies",
          "swim",
          "the",
          "tides",
          "to",
          "together",
          "travel",
          "using",
          "vegetables",
          "video",
          "wheels",
          "with"
         ],
         "legendgroup": "other",
         "marker": {
          "color": "gray",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "other",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -0.025435179250616828,
          -0.02543517925061635,
          -0.04323915215621425,
          0.09603553590482374,
          -0.03664660422044094,
          -0.08132791355042301,
          0.3165654291069698,
          -0.03664660422044103,
          -0.057974489759987695,
          -0.03664660422044104,
          -0.037074082798719536,
          -0.03664660422044105,
          0.3165654291069699,
          -0.03664660422044095,
          -0.025435162732948904,
          -0.03810473378792804,
          -0.08132791355042303,
          -0.08132791355042303,
          -0.03810473378792804,
          -0.05278444239419979,
          0.09603553590482379,
          -0.03664660422044097,
          -0.10787635741202774,
          -0.025435179250616346,
          -0.025435179250616786,
          -0.047334733293972336,
          -0.028931281889874757,
          -0.04289259582108934,
          0.4790651810659079,
          -0.05278444239419978,
          -0.10787635741202772,
          -0.03233018520493716,
          -0.03664660422044101,
          -0.03810473378792808,
          0.09603553590482382,
          -0.03664660422044102,
          0.3165654291069699,
          -0.036552136617827115,
          -0.038104733787928134,
          -0.04289259582108932,
          -0.03664660422044101,
          -0.04323915215621405,
          -0.08132791355042306,
          -0.025435162732948657,
          0.47906518106590795,
          -0.03810473378792808,
          -0.1078763574120277,
          -0.03664660422044096,
          -0.034791145283920986,
          -0.03810473378792806,
          -0.05234700315865081,
          -0.025435162732948657,
          -0.02543517925061694,
          -0.10787635741202777,
          0.47906518106590795,
          -0.02543517925061631,
          -0.028931281889874823,
          -0.03664660422044104,
          -0.025435179250616995,
          -0.02893128188987474,
          -0.025435179250616977,
          -0.047334733293972316,
          -0.025435162732948935,
          -0.08132791355042301,
          -0.03655213661782712,
          -0.08132791355042301
         ],
         "xaxis": "x",
         "y": [
          -0.03035812358798532,
          -0.030358123587984436,
          -0.09218466018749177,
          -0.0017788634648223769,
          -0.05619322914935223,
          0.2340852011265352,
          0.0395802632566838,
          -0.056193229149351605,
          0.15807966935298015,
          -0.05619322914935151,
          -0.0770654519965208,
          -0.05619322914935159,
          0.03958026325668381,
          -0.05619322914935216,
          -0.030358093819312267,
          -0.05892920538920227,
          0.23408520112653528,
          0.23408520112653525,
          -0.0589292053892023,
          -0.11564685958285272,
          -0.0017788634648223413,
          -0.056193229149352146,
          0.33428112055136444,
          -0.03035812358798467,
          -0.030358123587985262,
          -0.10183461338397132,
          -0.04280385935960035,
          -0.07499540621569008,
          0.0722097357117724,
          -0.11564685958285274,
          0.33428112055136444,
          -0.06492968457104185,
          -0.05619322914935163,
          -0.0589292053892018,
          -0.0017788634648223053,
          -0.05619322914935152,
          0.03958026325668377,
          -0.060814659539623346,
          -0.058929205389201766,
          -0.07499540621569008,
          -0.056193229149351646,
          -0.09218466018749204,
          0.2340852011265353,
          -0.030358093819312707,
          0.07220973571177242,
          -0.05892920538920181,
          0.3342811205513645,
          -0.05619322914935217,
          -0.0581934841315901,
          -0.05892920538920228,
          -0.11480956865966398,
          -0.030358093819312724,
          -0.030358123587985154,
          0.33428112055136444,
          0.07220973571177271,
          -0.030358123587984644,
          -0.04280385935959993,
          -0.056193229149351556,
          -0.03035812358798516,
          -0.04280385935960003,
          -0.030358123587985165,
          -0.10183461338397136,
          -0.03035809381931228,
          0.2340852011265352,
          -0.06081465953962339,
          0.2340852011265352
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ],
          [
           "space"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>topic=%{customdata[0]}<extra></extra>",
         "hovertext": [
          "astronauts",
          "astronomy",
          "galaxies",
          "gravity",
          "moon",
          "planets",
          "stars",
          "sun",
          "telescopes"
         ],
         "legendgroup": "space",
         "marker": {
          "color": "red",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "space",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -0.025435179250616828,
          0.47906518106590784,
          0.17218650516666809,
          -0.038104733787928086,
          -0.03664660422044102,
          -0.038104733787928134,
          0.15545751427225596,
          -0.03810473378792815,
          0.09603553590482387
         ],
         "xaxis": "x",
         "y": [
          -0.030358123587985272,
          0.07220973571177258,
          0.01559348941233758,
          -0.0589292053892018,
          -0.0561932291493515,
          -0.058929205389201766,
          0.015945932854774303,
          -0.058929205389201766,
          -0.0017788634648223172
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ],
          [
           "pets"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>topic=%{customdata[0]}<extra></extra>",
         "hovertext": [
          "balls",
          "birds",
          "cats",
          "dogs",
          "fish",
          "hamsters",
          "mice",
          "rabbits",
          "turtles"
         ],
         "legendgroup": "pets",
         "marker": {
          "color": "blue",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "pets",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          -0.05481346858654396,
          -0.10787635741202778,
          -0.04072411899032894,
          -0.04072411899032896,
          -0.025435179250616308,
          -0.03655213661782713,
          -0.05481346858654392,
          -0.025435162732948935,
          -0.052347003158650776
         ],
         "xaxis": "x",
         "y": [
          -0.12172048624745223,
          0.3342811205513644,
          -0.08607742001800767,
          -0.08607742001800775,
          -0.03035812358798463,
          -0.06081465953962337,
          -0.12172048624745213,
          -0.03035809381931224,
          -0.11480956865966399
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 800,
        "legend": {
         "title": {
          "text": "topic"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Tiny Word Embedding Space (Co-occurrence + PCA)"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "PCA Component 1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "PCA Component 2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------\n",
    "# 6. PCA for visualization (2D)\n",
    "# -------------------------------------\n",
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(embeddings)\n",
    "\n",
    "# Optional: Light topic-based coloring (simple heuristic)\n",
    "def guess_topic(w):\n",
    "    if w in {\"cat\",\"cats\",\"dogs\",\"dog\",\"hamsters\",\"fish\",\"birds\",\"rabbits\",\"turtles\",\"mice\",\"balls\"}:\n",
    "        return \"pets\"\n",
    "    if w in {\"stars\",\"astronomy\",\"galaxies\",\"planets\",\"sun\",\"moon\",\"gravity\",\"astronauts\",\"telescopes\"}:\n",
    "        return \"space\"\n",
    "    if w in {\"computers\",\"neural\",\"networks\",\"machines\",\"algorithms\",\"data\",\"scientists\",\"robots\"}:\n",
    "        return \"tech\"\n",
    "    return \"other\"\n",
    "\n",
    "colors = {\n",
    "    \"pets\": \"blue\",\n",
    "    \"space\": \"red\",\n",
    "    \"tech\": \"green\",\n",
    "    \"other\": \"gray\"\n",
    "}\n",
    "\n",
    "topic_colors = [colors[guess_topic(w)] for w in words]\n",
    "\n",
    "# -------------------------------------\n",
    "# 7. Interactive PCA Visualization with Hover Labels (Plotly)\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Build a DataFrame for Plotly\n",
    "df = pd.DataFrame({\n",
    "    \"word\": words,\n",
    "    \"pc1\": points[:, 0],\n",
    "    \"pc2\": points[:, 1],\n",
    "    \"topic\": [guess_topic(w) for w in words]\n",
    "})\n",
    "\n",
    "# Color mapping consistent with your previous colors\n",
    "color_map = {\n",
    "    \"pets\": \"blue\",\n",
    "    \"space\": \"red\",\n",
    "    \"tech\": \"green\",\n",
    "    \"other\": \"gray\"\n",
    "}\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    color=\"topic\",\n",
    "    text=None,\n",
    "    hover_name=\"word\",\n",
    "    hover_data={\"topic\": True, \"pc1\": False, \"pc2\": False},\n",
    "    color_discrete_map=color_map,\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Tiny Word Embedding Space (Co-occurrence + PCA)\",\n",
    "    xaxis_title=\"PCA Component 1\",\n",
    "    yaxis_title=\"PCA Component 2\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9aee7a",
   "metadata": {},
   "source": "## üîç Understanding the PCA Plot: What You're Seeing\n\nThe interactive plot shows a **2-dimensional picture** of word embeddings that actually live in a **much higher-dimensional space** (92 dimensions in our case, one per vocabulary word). Because we can't easily visualize high-dimensional geometry, we use a tool called **Principal Component Analysis**, or **PCA**, to create a simplified view.\n\n### **What PCA Does (Intuition Only)**\nPCA looks at all of the high-dimensional word vectors and asks:\n\n> *\"If I had to draw these points on a flat piece of paper, what are the two directions that preserve the most structure?\"*\n\nIt then finds the two directions along which the words vary the most in meaning. These become the **x-axis** and **y-axis** of the plot.\n\nThink of PCA as:\n- flattening a crumpled map onto a table,  \n- while trying to keep neighborhoods and directions as faithful as possible.\n\n### **Why This Helps**\nEven though words actually live in a 92-dimensional space, PCA lets us see:\n\n- **clusters** of related words  \n  (e.g., pets cluster together: cats, dogs, hamsters)\n- **separation** between different topics  \n  (e.g., \"cats/dogs\" far from \"stars/galaxies\")\n- **relative similarity**  \n  (close points = similar contexts; far points = different meanings)\n\n### **What the Plot Represents**\n- Each **dot** is a word.  \n- Dots placed close together tend to appear in **similar contexts** in the corpus.  \n- Dots far apart rarely appear in similar contexts.  \n- Colors help highlight rough topic categories (pets=blue, space=red, tech=green, other=gray).\n\n**Notice how:**\n- Pet words (blue) cluster in one region\n- Space/astronomy words (red) cluster in another region\n- Technology words (green) form their own group\n- The clusters are separated from each other!\n\n### **Important Note**\nPCA doesn't capture *all* the meaning relationships‚Äîonly the most visible ones.  \nWords might be similar in dimensions we can't visualize, but PCA gives us a useful snapshot that reveals the **general structure** of the embedding space.\n\nIn short:\n\n> **PCA gives us a 2D window into a high-dimensional world of meaning.**  \n> It's not perfect, but it's extremely helpful for seeing patterns and relationships at a glance.\n\n---\n\n## üìù Questions 4-5 (Analysis)\n\n**Q4.** In the PCA visualization above, which words cluster together? Why do you think they form these groups?\n\n*Hint: Look at the colored regions. What do words in the same cluster have in common?*\n\n**Q5.** The original embedding space has 92 dimensions (one per word). PCA reduces this to 2D. What information might be lost in this reduction?\n\n*Think about: Can all word relationships be perfectly represented in just 2 dimensions?*\n\n*Record your answers in the answer sheet.*\n\n---"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "b0ecc99b",
   "metadata": {},
   "outputs": [],
   "source": "## üìù Question 6 (Synthesis - Connection to Lab 4)\n\n**Q6.** How is this co-occurrence approach similar to what you learned about hidden layers in Lab 4? \n\n*Hint: Both create new representations. In Lab 4, hidden layers transformed input features into a new space. What does the co-occurrence matrix transform words into?*\n\n*Record your answer in the answer sheet.*\n\n---\n\n## ‚úÖ Module 0 Complete!\n\nYou've just built a tiny embedding system from scratch! Here's what you learned:\n\n- **Words become vectors** based on which other words they appear with\n- **Similar contexts ‚Üí similar vectors** (cats and dogs both appear with \"pets\")\n- **Cosine similarity** measures how close meanings are\n- **PCA** lets us visualize high-dimensional spaces in 2D\n- **This is the same core idea** that powers modern AI systems‚Äîjust scaled up\n\n**In Module 1**, you'll explore pre-trained embeddings trained on BILLIONS of words from Wikipedia. You'll see how these professional embeddings capture fascinating relationships like analogies (king - man + woman = queen).\n\n**Ready?** Move on to **Module 1: Word Embeddings & Vector Arithmetic**!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}