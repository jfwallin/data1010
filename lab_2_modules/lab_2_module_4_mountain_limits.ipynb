{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Module 4: Mountain Landscape - Gradient Descent Limitations\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how GD gets trapped at local optima\n",
    "- See the importance of starting position\n",
    "- Connect to real neural network training challenges\n",
    "- Recognize when GD fails despite perfect implementation\n",
    "\n",
    "**Time:** ~15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**IMPORTANT:** Enter the same group code from Lab 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Lab 1\n",
    "\n",
    "In **Lab 1 Module 5**, you:\n",
    "- Manually explored a mountain landscape with multiple peaks\n",
    "- Chose (x, y) locations to sample\n",
    "- Tried to find the global maximum\n",
    "- Discovered that finding all peaks was hard!\n",
    "\n",
    "**Today:** Gradient descent will face the same challenge!\n",
    "\n",
    "### The Problem:\n",
    "- GD only sees **local slope** (gradient)\n",
    "- GD follows the steepest uphill direction\n",
    "- GD gets **stuck** at the first peak it reaches\n",
    "- GD cannot \"see\" that a higher peak exists elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Generate Same Mountain Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatText, Button, Checkbox, Output, VBox, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "group_code = int(input(\"Enter your group code: \"))\n",
    "np.random.seed(group_code)\n",
    "\n",
    "# Skip line and parabola parameters (we only need mountain)\n",
    "_ = np.random.uniform(-3, 3)  # true_m\n",
    "_ = np.random.uniform(-5, 5)  # true_b\n",
    "_ = np.random.uniform(0.5, 2.0)  # hidden_a\n",
    "_ = np.random.uniform(-4, 4)  # hidden_b\n",
    "_ = np.random.uniform(-10, 10)  # hidden_c\n",
    "\n",
    "# Mountain landscape parameters (same as Lab 1)\n",
    "num_peaks = np.random.randint(3, 6)\n",
    "peak_centers = []\n",
    "peak_heights = []\n",
    "peak_widths = []\n",
    "\n",
    "for _ in range(num_peaks):\n",
    "    cx = np.random.uniform(-3.0, 3.0)\n",
    "    cy = np.random.uniform(-3.0, 3.0)\n",
    "    height = np.random.uniform(1.0, 5.0)\n",
    "    width = np.random.uniform(0.6, 1.5)\n",
    "    peak_centers.append((cx, cy))\n",
    "    peak_heights.append(height)\n",
    "    peak_widths.append(width)\n",
    "\n",
    "def mountain_height(pos):\n",
    "    \"\"\"Mountain landscape with multiple Gaussian peaks.\n",
    "    \n",
    "    Args:\n",
    "        pos: [x, y] array or scalars\n",
    "    \n",
    "    Returns:\n",
    "        Altitude (scalar or array)\n",
    "    \"\"\"\n",
    "    if isinstance(pos, (list, tuple)):\n",
    "        pos = np.array(pos)\n",
    "    \n",
    "    x = pos[..., 0] if pos.ndim > 1 else pos[0]\n",
    "    y = pos[..., 1] if pos.ndim > 1 else pos[1]\n",
    "    \n",
    "    z = np.zeros_like(x, dtype=float)\n",
    "    for (cx, cy), h, w in zip(peak_centers, peak_heights, peak_widths):\n",
    "        z += h * np.exp(-(((x - cx)**2 + (y - cy)**2) / (2 * w**2)))\n",
    "    return z\n",
    "\n",
    "# Find global maximum\n",
    "grid_size = 100\n",
    "x_vals = np.linspace(-5, 5, grid_size)\n",
    "y_vals = np.linspace(-5, 5, grid_size)\n",
    "Xg, Yg = np.meshgrid(x_vals, y_vals)\n",
    "Zg = np.zeros_like(Xg)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        Zg[i, j] = mountain_height([Xg[i, j], Yg[i, j]])\n",
    "\n",
    "flat_idx = np.argmax(Zg)\n",
    "i_max, j_max = np.unravel_index(flat_idx, Zg.shape)\n",
    "x_global = Xg[i_max, j_max]\n",
    "y_global = Yg[i_max, j_max]\n",
    "h_global = Zg[i_max, j_max]\n",
    "\n",
    "print(f\"âœ“ Mountain landscape loaded (same as Lab 1 Module 5)\")\n",
    "print(f\"Number of peaks: {num_peaks}\")\n",
    "print(f\"Global maximum at: ({x_global:.2f}, {y_global:.2f}) with height {h_global:.2f}\")\n",
    "print(\"(Revealed for learning purposes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Ascent (Uphill Climbing)\n",
    "\n",
    "To find peaks (maxima), we'll use **gradient ascent** instead of descent:\n",
    "\n",
    "```\n",
    "new = old + learning_rate Ã— gradient  (note: + instead of -)\n",
    "```\n",
    "\n",
    "This moves **uphill** toward local peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mountain(pos, h=1e-4):\n",
    "    \"\"\"Compute numerical gradient of mountain height.\n",
    "    \n",
    "    Args:\n",
    "        pos: [x, y] position\n",
    "    \n",
    "    Returns:\n",
    "        [grad_x, grad_y]\n",
    "    \"\"\"\n",
    "    pos = np.array(pos, dtype=float)\n",
    "    grad = np.zeros(2)\n",
    "    \n",
    "    for i in range(2):\n",
    "        pos_forward = pos.copy()\n",
    "        pos_backward = pos.copy()\n",
    "        pos_forward[i] += h\n",
    "        pos_backward[i] -= h\n",
    "        grad[i] = (mountain_height(pos_forward) - mountain_height(pos_backward)) / (2 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def gradient_ascent_step(pos, learning_rate):\n",
    "    \"\"\"One step of gradient ascent (climbing uphill).\"\"\"\n",
    "    grad = compute_gradient_mountain(pos)\n",
    "    return pos + learning_rate * grad  # + for ascent (uphill)\n",
    "\n",
    "def run_gradient_ascent(start_pos, learning_rate, max_steps=50, tol=1e-4):\n",
    "    \"\"\"Run gradient ascent from starting position.\"\"\"\n",
    "    history = {\n",
    "        'pos': [np.array(start_pos, dtype=float)],\n",
    "        'height': [mountain_height(start_pos)],\n",
    "        'grad': [],\n",
    "        'converged': False,\n",
    "        'n_steps': 0\n",
    "    }\n",
    "    \n",
    "    pos_current = np.array(start_pos, dtype=float)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        grad = compute_gradient_mountain(pos_current)\n",
    "        history['grad'].append(grad)\n",
    "        \n",
    "        # Ascent step\n",
    "        pos_new = gradient_ascent_step(pos_current, learning_rate)\n",
    "        h_new = mountain_height(pos_new)\n",
    "        \n",
    "        history['pos'].append(pos_new.copy())\n",
    "        history['height'].append(h_new)\n",
    "        history['n_steps'] = step + 1\n",
    "        \n",
    "        # Check convergence (gradient near zero)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            history['converged'] = True\n",
    "            break\n",
    "        \n",
    "        pos_current = pos_new\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"âœ“ Gradient ascent functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction Questions (Answer BEFORE running)\n",
    "\n",
    "**Q13 (PREDICTION):** \n",
    "\n",
    "Think about starting positions:\n",
    "- Starting at (1, 1): Which peak will gradient ascent reach?\n",
    "- Will it find the global maximum?\n",
    "- What if you start at (-2, 3)? Same peak or different?\n",
    "- Why does starting position matter so much?\n",
    "\n",
    "Write your predictions on the answer sheet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive: Run GD from Different Starting Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State for multiple GD runs\n",
    "gd_runs = []\n",
    "colors_runs = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "show_landscape = False\n",
    "\n",
    "# Widgets\n",
    "x_start_input = FloatText(description=\"Start x:\", value=0.0, step=0.5)\n",
    "y_start_input = FloatText(description=\"Start y:\", value=0.0, step=0.5)\n",
    "lr_mountain_input = FloatText(description=\"Learning rate:\", value=0.5, step=0.1)\n",
    "run_ga_button = Button(description=\"Run Gradient Ascent\", button_style='success')\n",
    "reveal_landscape_button = Button(description=\"Reveal Landscape\", button_style='primary')\n",
    "reset_mountain_button = Button(description=\"Reset All\", button_style='warning')\n",
    "output_mountain = Output()\n",
    "\n",
    "def plot_mountain_results():\n",
    "    \"\"\"Plot all GD runs on mountain landscape.\"\"\"\n",
    "    with output_mountain:\n",
    "        output_mountain.clear_output(wait=True)\n",
    "        \n",
    "        if not gd_runs:\n",
    "            print(\"No gradient ascent runs yet. Enter starting position and click 'Run Gradient Ascent'.\")\n",
    "            return\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 8), dpi=100)\n",
    "        \n",
    "        # Show landscape if revealed\n",
    "        if show_landscape:\n",
    "            contour = ax.contourf(Xg, Yg, Zg, levels=30, cmap='plasma', alpha=0.6)\n",
    "            plt.colorbar(contour, ax=ax, label='Altitude')\n",
    "            \n",
    "            # Mark global maximum\n",
    "            ax.scatter([x_global], [y_global], c='yellow', marker='*', \n",
    "                      s=500, edgecolors='black', linewidths=3, zorder=20, label='Global maximum')\n",
    "        \n",
    "        # Plot each GD run\n",
    "        for i, run in enumerate(gd_runs):\n",
    "            pos_hist = np.array(run['history']['pos'])\n",
    "            h_hist = run['history']['height']\n",
    "            color = colors_runs[i % len(colors_runs)]\n",
    "            \n",
    "            # Path\n",
    "            ax.plot(pos_hist[:, 0], pos_hist[:, 1], 'o-', \n",
    "                   color=color, linewidth=2, markersize=5, alpha=0.8, \n",
    "                   label=f\"Run {i+1}: start ({run['start'][0]:.1f}, {run['start'][1]:.1f})\")\n",
    "            \n",
    "            # Mark start\n",
    "            ax.scatter([pos_hist[0, 0]], [pos_hist[0, 1]], \n",
    "                      c='white', marker='o', s=150, edgecolors=color, linewidths=3, zorder=10)\n",
    "            \n",
    "            # Mark end (peak reached)\n",
    "            ax.scatter([pos_hist[-1, 0]], [pos_hist[-1, 1]], \n",
    "                      c=color, marker='*', s=300, edgecolors='black', linewidths=2, zorder=11)\n",
    "        \n",
    "        ax.set_xlabel('x', fontsize=12)\n",
    "        ax.set_ylabel('y', fontsize=12)\n",
    "        ax.set_title('Gradient Ascent on Mountain Landscape', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_ylim(-5, 5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=9, loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary table\n",
    "        print(\"\\nSummary of Gradient Ascent Runs:\")\n",
    "        print(\"=\"*80)\n",
    "        summary_data = []\n",
    "        for i, run in enumerate(gd_runs):\n",
    "            final_pos = run['history']['pos'][-1]\n",
    "            final_h = run['history']['height'][-1]\n",
    "            summary_data.append({\n",
    "                'Run': i + 1,\n",
    "                'Start (x, y)': f\"({run['start'][0]:.2f}, {run['start'][1]:.2f})\",\n",
    "                'Final (x, y)': f\"({final_pos[0]:.2f}, {final_pos[1]:.2f})\",\n",
    "                'Final height': f\"{final_h:.3f}\",\n",
    "                'Steps': run['history']['n_steps'],\n",
    "                'Found global?': 'Yes' if abs(final_h - h_global) < 0.5 else 'No'\n",
    "            })\n",
    "        \n",
    "        display(pd.DataFrame(summary_data))\n",
    "        print(f\"\\nGlobal maximum: ({x_global:.2f}, {y_global:.2f}), height = {h_global:.2f}\")\n",
    "        print(f\"\\nNotice: Different starting points lead to different local peaks!\")\n",
    "\n",
    "def on_run_ga_click(b):\n",
    "    x_start = x_start_input.value\n",
    "    y_start = y_start_input.value\n",
    "    lr = lr_mountain_input.value\n",
    "    \n",
    "    # Run gradient ascent\n",
    "    history = run_gradient_ascent([x_start, y_start], lr, max_steps=50)\n",
    "    \n",
    "    gd_runs.append({\n",
    "        'start': [x_start, y_start],\n",
    "        'lr': lr,\n",
    "        'history': history\n",
    "    })\n",
    "    \n",
    "    plot_mountain_results()\n",
    "\n",
    "def on_reveal_click(b):\n",
    "    global show_landscape\n",
    "    show_landscape = True\n",
    "    plot_mountain_results()\n",
    "\n",
    "def on_reset_click(b):\n",
    "    global gd_runs, show_landscape\n",
    "    gd_runs = []\n",
    "    show_landscape = False\n",
    "    with output_mountain:\n",
    "        output_mountain.clear_output()\n",
    "        print(\"Reset! Try new starting positions.\")\n",
    "\n",
    "run_ga_button.on_click(on_run_ga_click)\n",
    "reveal_landscape_button.on_click(on_reveal_click)\n",
    "reset_mountain_button.on_click(on_reset_click)\n",
    "\n",
    "print(\"Interactive Gradient Ascent on Mountain Landscape\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Enter starting (x, y) position (range: -5 to 5)\")\n",
    "print(\"2. Set learning rate (try 0.5)\")\n",
    "print(\"3. Click 'Run Gradient Ascent' to see GD climb to nearest peak\")\n",
    "print(\"4. Try MULTIPLE starting points to see different outcomes\")\n",
    "print(\"5. Click 'Reveal Landscape' to see all peaks\\n\")\n",
    "print(\"Suggested starting points to try:\")\n",
    "print(\"  - (0, 0)\")\n",
    "print(\"  - (2, 2)\")\n",
    "print(\"  - (-3, 1)\")\n",
    "print(\"  - (1, -2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "display(VBox([\n",
    "    HBox([x_start_input, y_start_input, lr_mountain_input]),\n",
    "    HBox([run_ga_button, reveal_landscape_button, reset_mountain_button]),\n",
    "    output_mountain\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Your Answer Sheet\n",
    "\n",
    "**Q14.** Based on your experiments:\n",
    "- Did gradient ascent find the global maximum?\n",
    "- Why or why not?\n",
    "- How did the starting position affect which peak was reached?\n",
    "- Can GD \"see\" distant peaks? Why not?\n",
    "\n",
    "**Q15.** Connection to neural network training:\n",
    "- Neural networks have loss landscapes with millions of local minima\n",
    "- How is this mountain problem similar to training a neural network?\n",
    "- What strategies might help overcome getting stuck at local optima?\n",
    "  (Hint: random restarts, momentum, adaptive learning rates, etc.)\n",
    "- Why is neural network optimization still successful despite local minima?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: Limitations of Gradient Descent\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Local Optima Problem:**\n",
    "   - GD only sees **local slope**, not the full landscape\n",
    "   - Gets trapped at the first peak/valley it reaches\n",
    "   - Cannot escape local optima on its own\n",
    "\n",
    "2. **Starting Point Matters:**\n",
    "   - Different starting points â†’ different local optima\n",
    "   - No way to know if you found global optimum\n",
    "   - In ML: Weight initialization is crucial!\n",
    "\n",
    "3. **Gradient Descent is \"Greedy\":**\n",
    "   - Always follows steepest descent/ascent\n",
    "   - Never \"backtracks\" or explores\n",
    "   - Deterministic path from starting point\n",
    "\n",
    "### Real Machine Learning Solutions:\n",
    "\n",
    "1. **Random Restarts:** Try multiple starting points\n",
    "2. **Momentum:** Add \"inertia\" to push through small barriers\n",
    "3. **Stochastic GD:** Add noise to escape local minima\n",
    "4. **Adaptive Methods:** Adjust learning rate dynamically (Adam, RMSprop)\n",
    "5. **Good Initialization:** Smart starting points (Xavier, He initialization)\n",
    "6. **Acceptance of Local Minima:** In practice, \"good enough\" local minima work!\n",
    "\n",
    "### Why Neural Networks Still Work:\n",
    "\n",
    "- High-dimensional spaces have many \"good\" local minima\n",
    "- Most local minima are close in performance to global minimum\n",
    "- Overparameterization helps: many paths to good solutions\n",
    "- Modern architectures and techniques reduce the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 Complete!\n",
    "\n",
    "Congratulations! You've completed Lab 2 and learned:\n",
    "\n",
    "âœ“ The universal update rule: `new = old - learning_rate Ã— gradient`\n",
    "\n",
    "âœ“ How gradient descent automates the manual search from Lab 1\n",
    "\n",
    "âœ“ The critical importance of learning rate (Goldilocks problem)\n",
    "\n",
    "âœ“ How GD navigates parameter space systematically\n",
    "\n",
    "âœ“ The fundamental limitation: getting stuck at local optima\n",
    "\n",
    "âœ“ Why starting position matters enormously\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Answer Q13, Q14, Q15** on your answer sheet\n",
    "2. **Review your predictions** - How accurate were they?\n",
    "3. **Return to the LMS** to submit your work\n",
    "\n",
    "Great work! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
