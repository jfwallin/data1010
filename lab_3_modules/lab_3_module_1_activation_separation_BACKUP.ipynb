{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Module 1: How Activation Functions Separate Data\n",
    "\n",
    "**Learning Objectives:**\n",
    "- See how activation functions transform mixed data into separated classes\n",
    "- Understand that activation functions \"squash\" or \"compress\" values into specific ranges\n",
    "- Build intuition for why activation functions help with classification\n",
    "\n",
    "**Time:** ~15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**From Module 0:** You discovered that some patterns cannot be separated by straight lines.\n",
    "\n",
    "**Today's Big Idea:** Activation functions can **transform** numbers in a way that makes mixed-up data become clearly separated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: A Simple Example\n",
    "\n",
    "Let's start with a simple scenario:\n",
    "- You have a bunch of numbers (they could be distances, scores, measurements, etc.)\n",
    "- Some numbers should be classified as **Class 0** (blue)\n",
    "- Other numbers should be classified as **Class 1** (red)\n",
    "- But the values overlap - they're mixed together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FloatSlider, interact, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create some mixed data\n",
    "# Class 0: centered around -2\n",
    "# Class 1: centered around +2\n",
    "# But they overlap!\n",
    "n_per_class = 40\n",
    "\n",
    "values_class0 = np.random.randn(n_per_class) * 1.2 - 2.0  # Mean -2, some spread\n",
    "values_class1 = np.random.randn(n_per_class) * 1.2 + 2.0  # Mean +2, some spread\n",
    "\n",
    "# Combine\n",
    "values = np.concatenate([values_class0, values_class1])\n",
    "labels = np.concatenate([np.zeros(n_per_class), np.ones(n_per_class)])\n",
    "\n",
    "print(\"‚úì Data created!\")\n",
    "print(f\"\\n{n_per_class} values for Class 0 (blue) - centered around -2\")\n",
    "print(f\"{n_per_class} values for Class 1 (red) - centered around +2\")\n",
    "print(\"\\nBut they OVERLAP in the middle - some blue and red values are mixed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem: Overlapping Values\n",
    "\n",
    "Let's visualize these values on a number line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the values on a number line\n",
    "fig, ax = plt.subplots(figsize=(14, 4), dpi=100)\n",
    "\n",
    "# Plot Class 0 (blue)\n",
    "ax.scatter(values_class0, np.zeros(n_per_class), c='blue', s=100, alpha=0.6, \n",
    "          label='Class 0 (Blue)', edgecolors='k', linewidths=1.5)\n",
    "\n",
    "# Plot Class 1 (red)\n",
    "ax.scatter(values_class1, np.zeros(n_per_class), c='red', s=100, alpha=0.6, \n",
    "          label='Class 1 (Red)', edgecolors='k', linewidths=1.5)\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "ax.axvline(0, color='green', linewidth=2, linestyle='--', alpha=0.5, label='Threshold at 0')\n",
    "\n",
    "ax.set_xlabel('Value', fontsize=13)\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('Original Values: Classes Overlap!', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks([])\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.set_xlim(-6, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many are on the \"wrong\" side of zero\n",
    "wrong_class0 = np.sum(values_class0 > 0)  # Blue points that are positive\n",
    "wrong_class1 = np.sum(values_class1 < 0)  # Red points that are negative\n",
    "total_wrong = wrong_class0 + wrong_class1\n",
    "accuracy = (len(values) - total_wrong) / len(values) * 100\n",
    "\n",
    "print(f\"\\nIf we use a simple threshold at 0:\")\n",
    "print(f\"  ‚Ä¢ {wrong_class0} blue points are on the wrong side (positive)\")\n",
    "print(f\"  ‚Ä¢ {wrong_class1} red points are on the wrong side (negative)\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {accuracy:.1f}%\")\n",
    "print(f\"\\n‚ö†Ô∏è The classes overlap - we can't perfectly separate them with just a threshold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions: The Transformation Tools\n",
    "\n",
    "Now let's see what happens when we apply **activation functions** to these values.\n",
    "\n",
    "We'll use a **sigmoid function** - one of the most common activation functions:\n",
    "- It takes any number (negative, positive, huge, tiny)\n",
    "- It **squashes** it into the range 0 to 1\n",
    "- Negative numbers ‚Üí close to 0\n",
    "- Positive numbers ‚Üí close to 1\n",
    "- Zero ‚Üí exactly 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function: squashes values to range (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def step(x):\n",
    "    \"\"\"Step function: hard threshold at 0\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh_activation(x):\n",
    "    \"\"\"Tanh activation: squashes to range (-1, 1)\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: keeps positive, zeros negative\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Show what sigmoid looks like\n",
    "x_plot = np.linspace(-6, 6, 200)\n",
    "y_sigmoid = sigmoid(x_plot)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "ax.plot(x_plot, y_sigmoid, 'purple', linewidth=3, label='Sigmoid(x)')\n",
    "ax.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "ax.axhline(1, color='black', linewidth=1, alpha=0.3, linestyle='--')\n",
    "ax.axhline(0.5, color='green', linewidth=2, linestyle='--', alpha=0.5, label='Middle (0.5)')\n",
    "ax.axvline(0, color='black', linewidth=1, alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Input Value (x)', fontsize=13)\n",
    "ax.set_ylabel('Sigmoid Output', fontsize=13)\n",
    "ax.set_title('Sigmoid Function: Squashes Everything to 0-1 Range', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSigmoid Key Properties:\")\n",
    "print(\"  ‚Ä¢ Large negative inputs ‚Üí Output ‚âà 0\")\n",
    "print(\"  ‚Ä¢ Large positive inputs ‚Üí Output ‚âà 1\")\n",
    "print(\"  ‚Ä¢ Input = 0 ‚Üí Output = 0.5 (middle)\")\n",
    "print(\"  ‚Ä¢ Smooth S-curve shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Sigmoid: Watch the Separation Happen!\n",
    "\n",
    "Now let's apply sigmoid to our mixed-up values and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid to all values\n",
    "values_transformed = sigmoid(values)\n",
    "values_class0_transformed = sigmoid(values_class0)\n",
    "values_class1_transformed = sigmoid(values_class1)\n",
    "\n",
    "# Create side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\n",
    "\n",
    "# LEFT: Before (original values)\n",
    "ax1.scatter(values_class0, np.zeros(n_per_class), c='blue', s=100, alpha=0.6, \n",
    "           label='Class 0 (Blue)', edgecolors='k', linewidths=1.5)\n",
    "ax1.scatter(values_class1, np.zeros(n_per_class), c='red', s=100, alpha=0.6, \n",
    "           label='Class 1 (Red)', edgecolors='k', linewidths=1.5)\n",
    "ax1.axvline(0, color='green', linewidth=2, linestyle='--', alpha=0.5, label='Threshold')\n",
    "ax1.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "\n",
    "ax1.set_xlabel('Original Value', fontsize=13)\n",
    "ax1.set_title('BEFORE: Original Values (Overlapping)', fontsize=13, fontweight='bold')\n",
    "ax1.set_yticks([])\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "ax1.set_xlim(-6, 6)\n",
    "\n",
    "# RIGHT: After sigmoid\n",
    "ax2.scatter(values_class0_transformed, np.zeros(n_per_class), c='blue', s=100, alpha=0.6, \n",
    "           label='Class 0 (Blue)', edgecolors='k', linewidths=1.5)\n",
    "ax2.scatter(values_class1_transformed, np.zeros(n_per_class), c='red', s=100, alpha=0.6, \n",
    "           label='Class 1 (Red)', edgecolors='k', linewidths=1.5)\n",
    "ax2.axvline(0.5, color='green', linewidth=2, linestyle='--', alpha=0.5, label='Threshold at 0.5')\n",
    "ax2.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('After Sigmoid', fontsize=13)\n",
    "ax2.set_title('AFTER: Sigmoid Applied (Better Separated!)', fontsize=13, fontweight='bold')\n",
    "ax2.set_yticks([])\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "ax2.set_xlim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate new accuracy\n",
    "predicted = (values_transformed > 0.5).astype(int)\n",
    "accuracy_after = np.mean(predicted == labels) * 100\n",
    "\n",
    "print(f\"\\nSeparation Results:\")\n",
    "print(f\"  BEFORE sigmoid: {accuracy:.1f}% accuracy\")\n",
    "print(f\"  AFTER sigmoid:  {accuracy_after:.1f}% accuracy\")\n",
    "print(f\"\\n‚úì The sigmoid function compressed the values into two clusters!\")\n",
    "print(f\"  ‚Ä¢ Blue points ‚Üí mostly below 0.5\")\n",
    "print(f\"  ‚Ä¢ Red points ‚Üí mostly above 0.5\")\n",
    "print(f\"  ‚Ä¢ Classes are now MORE separated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive: Try Different Activation Functions\n",
    "\n",
    "Let's compare how different activation functions separate the same data.\n",
    "\n",
    "**Try these:**\n",
    "- **Sigmoid**: Smooth squashing to 0-1\n",
    "- **Step**: Hard cut at 0 (binary 0 or 1)\n",
    "- **Tanh**: Smooth squashing to -1 to +1\n",
    "- **ReLU**: Keeps positive values, zeros out negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_activations(activation_name):\n",
    "    \"\"\"\n",
    "    Show before/after for different activation functions.\n",
    "    \"\"\"\n",
    "    # Select activation function\n",
    "    if activation_name == 'Sigmoid':\n",
    "        activation = sigmoid\n",
    "        threshold = 0.5\n",
    "        y_range = (-0.1, 1.1)\n",
    "    elif activation_name == 'Step':\n",
    "        activation = step\n",
    "        threshold = 0.5\n",
    "        y_range = (-0.1, 1.1)\n",
    "    elif activation_name == 'Tanh':\n",
    "        activation = tanh_activation\n",
    "        threshold = 0.0\n",
    "        y_range = (-1.1, 1.1)\n",
    "    else:  # ReLU\n",
    "        activation = relu\n",
    "        threshold = 1.0\n",
    "        y_range = (-0.5, 6)\n",
    "    \n",
    "    # Apply activation\n",
    "    values_transformed = activation(values)\n",
    "    values_class0_t = activation(values_class0)\n",
    "    values_class1_t = activation(values_class1)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5), dpi=100)\n",
    "    \n",
    "    # PLOT 1: Activation function curve\n",
    "    x_plot = np.linspace(-6, 6, 200)\n",
    "    y_plot = activation(x_plot)\n",
    "    ax1.plot(x_plot, y_plot, 'purple', linewidth=3, label=f'{activation_name}(x)')\n",
    "    ax1.axhline(threshold, color='green', linewidth=2, linestyle='--', \n",
    "               alpha=0.5, label=f'Threshold = {threshold}')\n",
    "    ax1.axvline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax1.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax1.set_xlabel('Input', fontsize=12)\n",
    "    ax1.set_ylabel('Output', fontsize=12)\n",
    "    ax1.set_title(f'{activation_name} Function', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-6, 6)\n",
    "    \n",
    "    # PLOT 2: Before\n",
    "    ax2.scatter(values_class0, np.zeros(n_per_class), c='blue', s=80, alpha=0.6, \n",
    "               label='Class 0', edgecolors='k', linewidths=1.5)\n",
    "    ax2.scatter(values_class1, np.zeros(n_per_class), c='red', s=80, alpha=0.6, \n",
    "               label='Class 1', edgecolors='k', linewidths=1.5)\n",
    "    ax2.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax2.set_xlabel('Original Value', fontsize=12)\n",
    "    ax2.set_title('BEFORE: Original Data', fontsize=13, fontweight='bold')\n",
    "    ax2.set_yticks([])\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    ax2.set_xlim(-6, 6)\n",
    "    \n",
    "    # PLOT 3: After\n",
    "    ax3.scatter(values_class0_t, np.zeros(n_per_class), c='blue', s=80, alpha=0.6, \n",
    "               label='Class 0', edgecolors='k', linewidths=1.5)\n",
    "    ax3.scatter(values_class1_t, np.zeros(n_per_class), c='red', s=80, alpha=0.6, \n",
    "               label='Class 1', edgecolors='k', linewidths=1.5)\n",
    "    ax3.axvline(threshold, color='green', linewidth=2, linestyle='--', \n",
    "               alpha=0.5, label=f'Threshold = {threshold}')\n",
    "    ax3.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax3.set_xlabel(f'After {activation_name}', fontsize=12)\n",
    "    ax3.set_title(f'AFTER: {activation_name} Applied', fontsize=13, fontweight='bold')\n",
    "    ax3.set_yticks([])\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3, axis='x')\n",
    "    ax3.set_xlim(y_range[0], y_range[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predicted = (values_transformed > threshold).astype(int)\n",
    "    accuracy_after = np.mean(predicted == labels) * 100\n",
    "    \n",
    "    print(f\"\\n{activation_name} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_after:.1f}%\")\n",
    "    \n",
    "    if accuracy_after > 90:\n",
    "        print(f\"  ‚úì Excellent separation!\")\n",
    "    elif accuracy_after > 75:\n",
    "        print(f\"  üëç Good separation\")\n",
    "    else:\n",
    "        print(f\"  üòê Some overlap remains\")\n",
    "\n",
    "# Interactive widget\n",
    "print(\"Compare Different Activation Functions\")\n",
    "print(\"=\"*70)\n",
    "print(\"Select an activation function to see how it transforms the data:\\n\")\n",
    "\n",
    "interact(\n",
    "    compare_activations,\n",
    "    activation_name=Dropdown(\n",
    "        options=['Sigmoid', 'Step', 'Tanh', 'ReLU'],\n",
    "        value='Sigmoid',\n",
    "        description='Activation:'\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Observations\n",
    "\n",
    "After trying different activation functions, you should notice:\n",
    "\n",
    "### What All Activation Functions Do:\n",
    "- Transform/compress input values\n",
    "- Push values into specific output ranges\n",
    "- Help separate overlapping classes\n",
    "\n",
    "### Differences Between Them:\n",
    "- **Sigmoid**: Smooth compression to 0-1, good for probabilities\n",
    "- **Step**: Hard binary cutoff (0 or 1), no middle ground\n",
    "- **Tanh**: Smooth compression to -1 to +1, centered at zero\n",
    "- **ReLU**: Keeps positive values, zeros negatives, no upper limit\n",
    "\n",
    "---\n",
    "\n",
    "**The Big Takeaway:**\n",
    "Activation functions take messy, overlapping data and **compress/transform** it into ranges where classes become more separated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Connection to Module 0\n",
    "\n",
    "Remember the circular pattern from Module 0 that couldn't be separated by a line?\n",
    "\n",
    "Here's what activation functions can do:\n",
    "1. Take the distance from the center for each point: `r = ‚àö(x‚ÇÅ¬≤ + x‚ÇÇ¬≤)`\n",
    "2. Apply an activation function to that distance\n",
    "3. Now inner points (small r) and outer points (large r) transform differently\n",
    "4. The transformation can help separate them!\n",
    "\n",
    "**You'll see this in action in Module 3 when we build perceptrons!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Your Answer Sheet\n",
    "\n",
    "**Q4.** How did the sigmoid function change the overlapping values? Describe what you observed.\n",
    "\n",
    "**Q5.** Which activation function created the clearest separation between classes? Why do you think that is?\n",
    "\n",
    "**Q6.** In your own words, what do activation functions do to data? Why might this help with classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q4, Q5, Q6** on your answer sheet\n",
    "2. **Return to the LMS** and continue to Module 2\n",
    "3. In Module 2, you'll learn more details about the four activation functions and their properties!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
