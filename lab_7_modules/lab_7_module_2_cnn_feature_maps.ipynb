{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7, Module 2: Visualizing CNN Feature Maps\n",
    "\n",
    "**Estimated time:** 20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: From Hand-Designed Filters to Learned Filters**\n",
    "\n",
    "In **Module 1**, you applied hand-designed filters (Sobel, blur, sharpen) to images.\n",
    "\n",
    "**Now the big question:** What filters does a real CNN actually learn?\n",
    "\n",
    "In this module, you'll:\n",
    "1. Load **MobileNetV2** (a production-quality image classifier)\n",
    "2. Extract its convolutional layers at different depths\n",
    "3. Visualize **feature maps** (what each layer detects)\n",
    "4. See how layers build from simple ‚Üí complex patterns\n",
    "\n",
    "### **Connection to Lab 6**\n",
    "\n",
    "- **Lab 6 (Saliency):** Showed **WHERE** the model looks (which pixels matter)\n",
    "- **Lab 7 Module 2 (Feature Maps):** Shows **WHAT** the model extracts (which patterns are detected)\n",
    "\n",
    "**Together:** You'll understand both **what features are extracted** and **which extracted features drive predictions**!\n",
    "\n",
    "---\n",
    "\n",
    "## üß± **What Is a Feature Map?**\n",
    "\n",
    "**Feature map** = The output of a convolutional layer when applied to an image.\n",
    "\n",
    "**Intuition:**\n",
    "- Each convolutional layer has multiple filters (32, 64, 128, etc.)\n",
    "- Each filter looks for a specific pattern\n",
    "- When a filter finds its pattern, the feature map \"lights up\" at that location\n",
    "\n",
    "**Example:**\n",
    "- **Filter 1:** Vertical edge detector ‚Üí Feature map shows where vertical edges are\n",
    "- **Filter 2:** Horizontal edge detector ‚Üí Feature map shows where horizontal edges are\n",
    "- **Filter 3:** Diagonal texture ‚Üí Feature map shows where that texture appears\n",
    "\n",
    "**Key insight:** Each layer produces **many feature maps**, one for each filter!\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è **About MobileNetV2**\n",
    "\n",
    "We'll use **MobileNetV2**, a lightweight CNN:\n",
    "- Pre-trained on **ImageNet** (1.4 million images, 1000 classes)\n",
    "- Only **14 MB** (runs fast on CPU!)\n",
    "- Used in production on phones, edge devices, embedded systems\n",
    "- Recognizes: animals, vehicles, objects, food, etc.\n",
    "\n",
    "**Architecture overview:**\n",
    "- 53 layers total\n",
    "- Multiple convolutional blocks at different depths\n",
    "- Early layers: Simple patterns (edges)\n",
    "- Mid layers: Textures and shapes\n",
    "- Deep layers: Object parts and complex patterns\n",
    "\n",
    "**We'll visualize 3 layers:**\n",
    "1. `block_1_conv1` (early layer ~2-3)\n",
    "2. `block_3_conv1` (middle layer ~10-12)\n",
    "3. `block_6_conv1` (deeper layer ~20-22)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import Model\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• **Load Pre-Trained MobileNetV2**\n",
    "\n",
    "This will download the model (14 MB) the first time you run it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full MobileNetV2 model\n",
    "print(\"Loading MobileNetV2 (this may take ~30 seconds the first time)...\\n\")\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=True)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  - Total layers: {len(base_model.layers)}\")\n",
    "print(f\"  - Output classes: {base_model.output_shape[1]} (ImageNet categories)\")\n",
    "print(f\"  - Input shape: {base_model.input_shape}\")\n",
    "\n",
    "# List some layer names for reference\n",
    "print(f\"\\nSample convolutional layer names:\")\n",
    "conv_layers = [layer.name for layer in base_model.layers if 'conv' in layer.name]\n",
    "for name in conv_layers[:10]:\n",
    "    print(f\"  - {name}\")\n",
    "print(f\"  ... and {len(conv_layers)-10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç **Create Feature Extraction Model**\n",
    "\n",
    "We'll create a new model that outputs **intermediate layer activations** (feature maps) instead of just the final prediction.\n",
    "\n",
    "**Layers we'll examine:**\n",
    "1. **`block_1_conv1`** (early) ‚Äì Simple patterns like edges\n",
    "2. **`block_3_conv1`** (middle) ‚Äì Textures and small shapes\n",
    "3. **`block_6_conv1`** (deep) ‚Äì Complex patterns and object parts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select layers to visualize\n",
    "layer_names = ['block_1_conv1', 'block_3_conv1', 'block_6_conv1']\n",
    "\n",
    "# Get layer outputs\n",
    "layer_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create feature extraction model\n",
    "feature_model = Model(inputs=base_model.input, outputs=layer_outputs)\n",
    "\n",
    "print(\"‚úÖ Feature extraction model created!\\n\")\n",
    "print(\"This model will output activations from 3 layers:\")\n",
    "for i, name in enumerate(layer_names, 1):\n",
    "    layer = base_model.get_layer(name)\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Shape: {layer.output_shape}\")\n",
    "    print(f\"   # Filters: {layer.output_shape[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üñºÔ∏è **Load and Classify a Sample Image**\n",
    "\n",
    "Let's start with a sample image to see what the model detects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test image (geometric pattern)\n",
    "# In practice, you'd load an actual photo\n",
    "\n",
    "def create_test_image():\n",
    "    \"\"\"Create a simple test image with various patterns\"\"\"\n",
    "    img = np.ones((224, 224, 3), dtype=np.uint8) * 200\n",
    "    \n",
    "    # Add vertical stripes (left side)\n",
    "    img[:, 20:40, :] = [50, 50, 50]\n",
    "    img[:, 50:70, :] = [50, 50, 50]\n",
    "    \n",
    "    # Add horizontal stripes (top)\n",
    "    img[20:40, :, :] = [100, 100, 100]\n",
    "    \n",
    "    # Add diagonal rectangle (bottom right)\n",
    "    img[140:180, 140:180, :] = [30, 30, 30]\n",
    "    \n",
    "    # Add circle (center-ish)\n",
    "    y, x = np.ogrid[:224, :224]\n",
    "    mask = (x - 112)**2 + (y - 112)**2 <= 30**2\n",
    "    img[mask] = [150, 150, 150]\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create test image\n",
    "test_img = create_test_image()\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_img)\n",
    "plt.title('Test Image: Geometric Patterns', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Preprocess for MobileNetV2\n",
    "img_array = test_img.astype('float32')\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# Classify\n",
    "predictions = base_model.predict(img_array, verbose=0)\n",
    "decoded = decode_predictions(predictions, top=3)[0]\n",
    "\n",
    "print(\"\\nTop 3 predictions:\")\n",
    "for i, (imagenet_id, label, score) in enumerate(decoded, 1):\n",
    "    print(f\"{i}. {label}: {score*100:.2f}%\")\n",
    "\n",
    "print(\"\\n(Note: Predictions may be unexpected for this synthetic image!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî• **Extract Feature Maps**\n",
    "\n",
    "Now let's pass the image through our feature extraction model and get the activations from each layer!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature maps from all 3 layers\n",
    "print(\"Extracting feature maps from 3 layers...\\n\")\n",
    "activations = feature_model.predict(img_array, verbose=0)\n",
    "\n",
    "print(\"‚úÖ Feature maps extracted!\\n\")\n",
    "for i, (name, activation) in enumerate(zip(layer_names, activations), 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Shape: {activation.shape}\")\n",
    "    print(f\"   (batch, height, width, filters) = {activation.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® **Visualize Layer 1 Feature Maps (Early Layer)**\n",
    "\n",
    "**Expected:** Simple patterns like edges, gradients, and color transitions.\n",
    "\n",
    "We'll visualize the first 16 filters (out of 32 total).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 16 feature maps from Layer 1\n",
    "layer1_activation = activations[0][0]  # Remove batch dimension\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    feature_map = layer1_activation[:, :, i]\n",
    "    axes[i].imshow(feature_map, cmap='viridis')\n",
    "    axes[i].set_title(f'Filter {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Layer 1 ({layer_names[0]}): Early Feature Maps\\n(First 16 of 32 filters)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT TO OBSERVE IN LAYER 1:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n- Each square shows one filter's response\")\n",
    "print(\"- Bright/yellow regions = filter detected its pattern\")\n",
    "print(\"- Dark/purple regions = filter didn't detect pattern\\n\")\n",
    "print(\"Expected patterns at this early layer:\")\n",
    "print(\"  ‚úì Vertical edges\")\n",
    "print(\"  ‚úì Horizontal edges\")\n",
    "print(\"  ‚úì Diagonal gradients\")\n",
    "print(\"  ‚úì Color transitions\")\n",
    "print(\"  ‚úì Simple contrasts\\n\")\n",
    "print(\"Notice: Similar to the Sobel filters from Module 1!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q10 (Prediction)**\n",
    "\n",
    "### **Q10. Before viewing the feature maps for Layers 2 and 3, predict: What will Layer 3 (middle layer) detect? What about Layer 6 (deep layer)?**\n",
    "\n",
    "*Hint: If Layer 1 detects edges, what might Layer 3 build from those edges? What might Layer 6 build from Layer 3's patterns?*\n",
    "\n",
    "**Record your prediction in the Answer Sheet BEFORE continuing!**\n",
    "\n",
    "---\n",
    "\n",
    "## üé® **Visualize Layer 3 Feature Maps (Middle Layer)**\n",
    "\n",
    "**Expected:** More complex patterns like corners, textures, and small shapes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 16 feature maps from Layer 3\n",
    "layer3_activation = activations[1][0]  # Remove batch dimension\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    feature_map = layer3_activation[:, :, i]\n",
    "    axes[i].imshow(feature_map, cmap='viridis')\n",
    "    axes[i].set_title(f'Filter {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Layer 3 ({layer_names[1]}): Middle Feature Maps\\n(First 16 filters)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT TO OBSERVE IN LAYER 3:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExpected patterns at this middle layer:\")\n",
    "print(\"  ‚úì Corners and junctions\")\n",
    "print(\"  ‚úì Texture patterns (stripes, dots)\")\n",
    "print(\"  ‚úì Curves and arcs\")\n",
    "print(\"  ‚úì Simple shapes (circles, rectangles)\")\n",
    "print(\"  ‚úì Combinations of edges\\n\")\n",
    "print(\"Notice: More abstract than Layer 1!\")\n",
    "print(\"Feature maps are smaller (image downsampled via pooling)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® **Visualize Layer 6 Feature Maps (Deep Layer)**\n",
    "\n",
    "**Expected:** Complex patterns like object parts, contextual features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 16 feature maps from Layer 6\n",
    "layer6_activation = activations[2][0]  # Remove batch dimension\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    feature_map = layer6_activation[:, :, i]\n",
    "    axes[i].imshow(feature_map, cmap='viridis')\n",
    "    axes[i].set_title(f'Filter {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Layer 6 ({layer_names[2]}): Deep Feature Maps\\n(First 16 filters)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT TO OBSERVE IN LAYER 6:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExpected patterns at this deep layer:\")\n",
    "print(\"  ‚úì Object parts (if real photo: wheels, windows, ears, etc.)\")\n",
    "print(\"  ‚úì Complex shapes and structures\")\n",
    "print(\"  ‚úì Contextual/semantic features\")\n",
    "print(\"  ‚úì Combinations of mid-level patterns\\n\")\n",
    "print(\"Notice: Much more abstract than Layers 1 and 3!\")\n",
    "print(\"Feature maps are even smaller (more pooling)\")\n",
    "print(\"Harder to interpret visually (very high-level representations)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Questions (Q11-Q15)**\n",
    "\n",
    "Record your answers in the **Answer Sheet**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q11. Looking at Layer 1 feature maps, which filters activated strongly? What patterns did they detect?**\n",
    "\n",
    "*Hint: Look for bright (yellow/white) regions. Which filters show strong activation? Do they correspond to edges, corners, or other patterns?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q12. Compare Layer 1 and Layer 6 feature maps. How are they different? What does this tell you about hierarchical learning?**\n",
    "\n",
    "*Hint: Compare the patterns you see. Are Layer 1 patterns simple or complex? What about Layer 6? How does this support the idea of building from simple to complex features?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q13. Find a feature map in Layer 3 that activated strongly for one part of the image. What pattern was it detecting?**\n",
    "\n",
    "*Hint: Look for filters that \"light up\" on specific features (circle, rectangle, stripes). What visual pattern caused that activation?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q14. Why do deeper layers show more abstract/complex patterns than early layers?**\n",
    "\n",
    "*Hint: Each layer builds on the previous one. If Layer 1 detects edges, Layer 3 can combine edges into shapes. What can Layer 6 do with Layer 3's shapes?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q15. How does this connect to the saliency maps from Lab 6?**\n",
    "\n",
    "*Hint: Saliency shows importance; feature maps show what's extracted. How do they work together to explain CNN predictions?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ **Upload Your Own Image (Optional)**\n",
    "\n",
    "Try feature map visualization with your own photo!\n",
    "\n",
    "**Best results with:**\n",
    "- Photos of animals (clear features like faces, fur)\n",
    "- Buildings/architecture (lots of edges and shapes)\n",
    "- Vehicles (distinctive parts like wheels, windows)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image\n",
    "print(\"üì§ Click 'Choose Files' and select an image...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    # Load image\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    img = Image.open(io.BytesIO(uploaded[filename]))\n",
    "    img = img.convert('RGB').resize((224, 224))\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Your Image: {filename}', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Preprocess\n",
    "    img_array = np.array(img).astype('float32')\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # Classify\n",
    "    predictions = base_model.predict(img_array, verbose=0)\n",
    "    decoded = decode_predictions(predictions, top=3)[0]\n",
    "    print(\"\\nTop 3 predictions:\")\n",
    "    for i, (imagenet_id, label, score) in enumerate(decoded, 1):\n",
    "        print(f\"{i}. {label}: {score*100:.2f}%\")\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"\\nExtracting feature maps...\\n\")\n",
    "    activations_user = feature_model.predict(img_array, verbose=0)\n",
    "    \n",
    "    # Visualize all 3 layers side by side\n",
    "    fig, axes = plt.subplots(3, 6, figsize=(18, 10))\n",
    "    \n",
    "    for layer_idx in range(3):\n",
    "        activation = activations_user[layer_idx][0]\n",
    "        for i in range(6):\n",
    "            axes[layer_idx, i].imshow(activation[:, :, i], cmap='viridis')\n",
    "            axes[layer_idx, i].set_title(f'Filter {i+1}', fontsize=9)\n",
    "            axes[layer_idx, i].axis('off')\n",
    "        \n",
    "        # Add layer label\n",
    "        axes[layer_idx, 0].text(-0.3, 0.5, f'Layer {layer_idx+1}\\n{layer_names[layer_idx]}', \n",
    "                               transform=axes[layer_idx, 0].transAxes,\n",
    "                               fontsize=11, fontweight='bold',\n",
    "                               rotation=90, va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Feature Maps for Your Image: {filename}', \n",
    "                 fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature maps generated for your image!\")\n",
    "    print(\"\\nCompare the 3 layers:\")\n",
    "    print(\"  Layer 1 (top row): Simple edge patterns\")\n",
    "    print(\"  Layer 3 (middle row): Textures and shapes\")\n",
    "    print(\"  Layer 6 (bottom row): Complex, abstract patterns\")\n",
    "else:\n",
    "    print(\"No image uploaded. Skipping this section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó **Connection to Lab 6: Saliency vs. Feature Maps**\n",
    "\n",
    "Let's connect what you learned in Lab 6 to what you're seeing today:\n",
    "\n",
    "| Concept | Lab 6 (Saliency) | Lab 7 Module 2 (Feature Maps) |\n",
    "|---------|------------------|-------------------------------|\n",
    "| **Question** | Which pixels matter? | What patterns are detected? |\n",
    "| **Method** | Gradients w.r.t. input | Forward pass through layers |\n",
    "| **Visualization** | Heatmap overlay | Grid of activation maps |\n",
    "| **Interpretation** | Importance of input regions | Patterns found by each filter |\n",
    "| **Reveals** | WHERE the model looks | WHAT the model extracts |\n",
    "\n",
    "### **Together, They Explain CNNs:**\n",
    "\n",
    "1. **Feature Maps (Lab 7):** Convolution layers extract hierarchical features\n",
    "   - Layer 1: Edges\n",
    "   - Layer 3: Textures and shapes\n",
    "   - Layer 6: Object parts\n",
    "\n",
    "2. **Saliency (Lab 6):** Shows which extracted features matter for the final decision\n",
    "   - \"The dog's ears and snout are important\"\n",
    "   - \"The background grass is not important\"\n",
    "\n",
    "**Full picture:**\n",
    "```\n",
    "Input Image\n",
    "    ‚Üì\n",
    "Layer 1 extracts edges ‚Üí Feature maps show edge locations ‚Üí Some edges more important than others\n",
    "    ‚Üì\n",
    "Layer 3 combines edges into shapes ‚Üí Feature maps show shapes ‚Üí Some shapes more important\n",
    "    ‚Üì\n",
    "Layer 6 combines shapes into parts ‚Üí Feature maps show parts ‚Üí Some parts more important\n",
    "    ‚Üì\n",
    "Final prediction: \"Dog\"\n",
    "    ‚Üë\n",
    "Saliency map reveals: Dog's face drove this decision\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Module 2 Complete!\n",
    "\n",
    "You now understand:\n",
    "- **What feature maps are** (outputs of convolutional layers)\n",
    "- **How to visualize them** (using intermediate layer outputs)\n",
    "- **What different layers detect** (edges ‚Üí textures ‚Üí shapes ‚Üí parts)\n",
    "- **Hierarchical learning** (simple patterns ‚Üí complex patterns)\n",
    "- **Connection to saliency** (feature maps show WHAT, saliency shows WHERE)\n",
    "\n",
    "**Key insight:**\n",
    "> CNNs automatically learn hierarchical feature detectors‚Äîstarting with simple edges and building up to complex object parts‚Äîall through gradient descent!\n",
    "\n",
    "**Ready to understand WHY this hierarchy works?**\n",
    "\n",
    "Move on to **Module 3: Hierarchical Feature Extraction**, where you'll learn the principles behind this powerful architecture!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
