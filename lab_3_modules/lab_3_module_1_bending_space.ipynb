{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Module 1: Activation Functions – Bending Space\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Describe what activation functions do to numbers (\"squash\", \"clip\", \"zero out\")\n",
    "- See how applying activation functions to 2D coordinates warps/bends space\n",
    "- Understand that activation functions are the \"nonlinear ingredient\" for flexible models\n",
    "- Recognize that straight rules after warping can look curved in original space\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**From Module 0:** You saw that some patterns (XOR, circles) can't be separated by straight lines.\n",
    "\n",
    "**Today's Big Idea:** Activation functions **warp or reshape space** itself. This warping is the key ingredient that will later let us use simple straight-line rules to solve complex problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Module 0\n",
    "\n",
    "In **Module 0**, you discovered:\n",
    "- Straight lines work great for some datasets (like two separated clouds)\n",
    "- But straight lines **completely fail** for XOR and circular patterns\n",
    "\n",
    "**Today:** We'll see how activation functions bend space so that simple rules become much more powerful.\n",
    "\n",
    "**Important:** Activation functions alone don't \"solve\" these problems. But they provide the **nonlinear transformation** that makes solutions possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: The Four Activation Functions\n",
    "\n",
    "Before we see how activation functions bend 2D space, let's recall what they do to single numbers.\n",
    "\n",
    "### The Four Functions:\n",
    "\n",
    "1. **Step**: Jumps at 0 → outputs 0 or 1\n",
    "2. **Sigmoid**: Smooth S-curve → outputs between 0 and 1\n",
    "3. **Tanh**: Smooth S-curve → outputs between -1 and 1\n",
    "4. **ReLU** (Rectified Linear Unit): Outputs 0 for negative inputs, keeps positive inputs as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Dropdown, interact, FloatSlider\n",
    "from IPython.display import display\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to avoid overflow\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def step(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "print(\"✓ Activation functions loaded!\")\n",
    "print(\"\\nKey behaviors:\")\n",
    "print(\"  • Sigmoid/Tanh: Compress large values into narrow range\")\n",
    "print(\"  • ReLU: Zeros out negatives, keeps positives\")\n",
    "print(\"  • Step: Hard jump at zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Individual Activation Functions\n",
    "\n",
    "Select an activation function to see:\n",
    "- Its graph\n",
    "- Example input → output pairs\n",
    "- How it behaves at large positive/negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_1d_activation(activation_name):\n",
    "    \"\"\"\n",
    "    Display 1D activation function with example values.\n",
    "    \"\"\"\n",
    "    # Select function\n",
    "    funcs = {\n",
    "        'Sigmoid': sigmoid,\n",
    "        'Tanh': tanh,\n",
    "        'ReLU': relu,\n",
    "        'Step': step\n",
    "    }\n",
    "    func = funcs[activation_name]\n",
    "    \n",
    "    # Create plot\n",
    "    x = np.linspace(-5, 5, 200)\n",
    "    y = func(x)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "    ax.plot(x, y, 'purple', linewidth=3, label=f'{activation_name}(x)')\n",
    "    ax.axhline(0, color='black', linewidth=0.8, alpha=0.3)\n",
    "    ax.axvline(0, color='black', linewidth=0.8, alpha=0.3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('Input (x)', fontsize=13)\n",
    "    ax.set_ylabel('Output', fontsize=13)\n",
    "    ax.set_title(f'{activation_name} Activation Function', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Example values\n",
    "    test_inputs = [-3, -1, 0, 1, 3]\n",
    "    print(f\"\\nExample Input → Output for {activation_name}:\")\n",
    "    print(\"=\"*50)\n",
    "    for inp in test_inputs:\n",
    "        out = func(np.array([inp]))[0]\n",
    "        print(f\"  {inp:4.1f} → {out:6.3f}\")\n",
    "    \n",
    "    # Behavior description\n",
    "    print(f\"\\nBehavior:\")\n",
    "    if activation_name == 'Sigmoid':\n",
    "        print(\"  • Large negative → close to 0\")\n",
    "        print(\"  • Large positive → close to 1\")\n",
    "        print(\"  • Smooth S-curve\")\n",
    "    elif activation_name == 'Tanh':\n",
    "        print(\"  • Large negative → close to -1\")\n",
    "        print(\"  • Large positive → close to +1\")\n",
    "        print(\"  • Smooth S-curve, centered at 0\")\n",
    "    elif activation_name == 'ReLU':\n",
    "        print(\"  • Negative → 0\")\n",
    "        print(\"  • Positive → keeps value\")\n",
    "        print(\"  • Sharp corner at 0\")\n",
    "    else:  # Step\n",
    "        print(\"  • Negative → 0\")\n",
    "        print(\"  • Positive → 1\")\n",
    "        print(\"  • Hard jump at 0\")\n",
    "\n",
    "# Interactive widget\n",
    "interact(\n",
    "    show_1d_activation,\n",
    "    activation_name=Dropdown(\n",
    "        options=['Sigmoid', 'Tanh', 'ReLU', 'Step'],\n",
    "        value='Sigmoid',\n",
    "        description='Activation:'\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Warping a 2D Grid – The Rubber Sheet\n",
    "\n",
    "Now here's where it gets interesting!\n",
    "\n",
    "### The Concept:\n",
    "\n",
    "Imagine drawing a square grid on a **rubber sheet** (the x₁-x₂ plane).\n",
    "\n",
    "When we apply an activation function to **both coordinates** (x₁ and x₂), it's like **stretching or compressing that rubber sheet** in different ways.\n",
    "\n",
    "**What happens:**\n",
    "- **Sigmoid**: Pulls the entire grid into [0,1] × [0,1] with curved edges\n",
    "- **Tanh**: Pulls into [-1,1] × [-1,1] with curved edges\n",
    "- **ReLU**: Negative half-planes collapse onto axes\n",
    "- **Step**: Almost everything collapses to four corner points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(n_lines=15, grid_range=3):\n",
    "    \"\"\"\n",
    "    Create a regular 2D grid of lines.\n",
    "    \"\"\"\n",
    "    lines_h = []  # Horizontal lines\n",
    "    lines_v = []  # Vertical lines\n",
    "    \n",
    "    grid_points = np.linspace(-grid_range, grid_range, n_lines)\n",
    "    n_points_per_line = 100\n",
    "    \n",
    "    for pos in grid_points:\n",
    "        # Horizontal line at x2 = pos\n",
    "        x1_h = np.linspace(-grid_range, grid_range, n_points_per_line)\n",
    "        x2_h = np.full(n_points_per_line, pos)\n",
    "        lines_h.append(np.column_stack([x1_h, x2_h]))\n",
    "        \n",
    "        # Vertical line at x1 = pos\n",
    "        x1_v = np.full(n_points_per_line, pos)\n",
    "        x2_v = np.linspace(-grid_range, grid_range, n_points_per_line)\n",
    "        lines_v.append(np.column_stack([x1_v, x2_v]))\n",
    "    \n",
    "    return lines_h, lines_v\n",
    "\n",
    "def apply_activation_2d(points, activation_func):\n",
    "    \"\"\"\n",
    "    Apply activation function to both coordinates.\n",
    "    \"\"\"\n",
    "    return np.column_stack([\n",
    "        activation_func(points[:, 0]),\n",
    "        activation_func(points[:, 1])\n",
    "    ])\n",
    "\n",
    "print(\"✓ Grid functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive: Watch the Grid Warp\n",
    "\n",
    "**Instructions:**\n",
    "1. Select an activation function from the dropdown\n",
    "2. Compare the LEFT plot (original) to the RIGHT plot (warped)\n",
    "3. Notice what happens to:\n",
    "   - Straight lines (do they stay straight?)\n",
    "   - The corners of the grid\n",
    "   - Points that were far from the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid_warping(activation_name):\n",
    "    \"\"\"\n",
    "    Show original grid and warped grid side-by-side.\n",
    "    \"\"\"\n",
    "    # Select function\n",
    "    funcs = {\n",
    "        'Sigmoid': sigmoid,\n",
    "        'Tanh': tanh,\n",
    "        'ReLU': relu,\n",
    "        'Step': step\n",
    "    }\n",
    "    func = funcs[activation_name]\n",
    "    \n",
    "    # Create grid\n",
    "    lines_h, lines_v = create_grid(n_lines=15, grid_range=3)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), dpi=100)\n",
    "    \n",
    "    # LEFT: Original grid\n",
    "    for line in lines_h:\n",
    "        ax1.plot(line[:, 0], line[:, 1], 'blue', alpha=0.5, linewidth=1)\n",
    "    for line in lines_v:\n",
    "        ax1.plot(line[:, 0], line[:, 1], 'blue', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    ax1.set_xlabel('x₁', fontsize=13)\n",
    "    ax1.set_ylabel('x₂', fontsize=13)\n",
    "    ax1.set_title('ORIGINAL Space (Before Activation)', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_xlim(-3.5, 3.5)\n",
    "    ax1.set_ylim(-3.5, 3.5)\n",
    "    ax1.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax1.axvline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    # RIGHT: Warped grid\n",
    "    for line in lines_h:\n",
    "        line_warped = apply_activation_2d(line, func)\n",
    "        ax2.plot(line_warped[:, 0], line_warped[:, 1], 'red', alpha=0.6, linewidth=1.5)\n",
    "    for line in lines_v:\n",
    "        line_warped = apply_activation_2d(line, func)\n",
    "        ax2.plot(line_warped[:, 0], line_warped[:, 1], 'red', alpha=0.6, linewidth=1.5)\n",
    "    \n",
    "    ax2.set_xlabel(f'{activation_name}(x₁)', fontsize=13)\n",
    "    ax2.set_ylabel(f'{activation_name}(x₂)', fontsize=13)\n",
    "    ax2.set_title(f'WARPED Space (After {activation_name})', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # Set appropriate limits based on activation\n",
    "    if activation_name == 'Sigmoid':\n",
    "        ax2.set_xlim(-0.1, 1.1)\n",
    "        ax2.set_ylim(-0.1, 1.1)\n",
    "    elif activation_name == 'Tanh':\n",
    "        ax2.set_xlim(-1.2, 1.2)\n",
    "        ax2.set_ylim(-1.2, 1.2)\n",
    "    elif activation_name == 'Step':\n",
    "        ax2.set_xlim(-0.2, 1.2)\n",
    "        ax2.set_ylim(-0.2, 1.2)\n",
    "    else:  # ReLU\n",
    "        ax2.set_xlim(-0.5, 3.5)\n",
    "        ax2.set_ylim(-0.5, 3.5)\n",
    "    \n",
    "    ax2.axhline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax2.axvline(0, color='black', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Observations\n",
    "    print(f\"\\nKey Observations for {activation_name}:\")\n",
    "    print(\"=\"*70)\n",
    "    if activation_name == 'Sigmoid':\n",
    "        print(\"  • Entire grid compressed into [0,1] × [0,1]\")\n",
    "        print(\"  • Straight lines become curves\")\n",
    "        print(\"  • Far-away points all squeezed near edges (0 or 1)\")\n",
    "    elif activation_name == 'Tanh':\n",
    "        print(\"  • Grid compressed into [-1,1] × [-1,1]\")\n",
    "        print(\"  • Similar to sigmoid but centered at zero\")\n",
    "        print(\"  • Straight lines become curved\")\n",
    "    elif activation_name == 'ReLU':\n",
    "        print(\"  • Three quadrants collapse onto the axes\")\n",
    "        print(\"  • Only positive quadrant (x₁>0, x₂>0) stays spread out\")\n",
    "        print(\"  • Sharp corner at origin\")\n",
    "    else:  # Step\n",
    "        print(\"  • Almost everything collapses to 4 points: (0,0), (0,1), (1,0), (1,1)\")\n",
    "        print(\"  • Extreme compression!\")\n",
    "\n",
    "# Interactive widget\n",
    "interact(\n",
    "    visualize_grid_warping,\n",
    "    activation_name=Dropdown(\n",
    "        options=['Sigmoid', 'Tanh', 'ReLU', 'Step'],\n",
    "        value='Sigmoid',\n",
    "        description='Activation:'\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Straight Rules After Warping\n",
    "\n",
    "Now for the **powerful insight**:\n",
    "\n",
    "Suppose after applying an activation, we classify points using a very simple rule:\n",
    "\n",
    "**\"activated_x₁ + activated_x₂ > threshold\"**\n",
    "\n",
    "- That rule is **linear** (straight line) in the activated space\n",
    "- But because activation **bent the coordinates**, the boundary looks **curved** in the original space!\n",
    "\n",
    "This is the key: **simple rules after warping = flexible boundaries before warping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_rule_after_activation(points, threshold):\n",
    "    \"\"\"\n",
    "    Apply rule: activated_x1 + activated_x2 > threshold\n",
    "    \"\"\"\n",
    "    return points[:, 0] + points[:, 1] > threshold\n",
    "\n",
    "def visualize_curved_boundary(activation_name, threshold):\n",
    "    \"\"\"\n",
    "    Show how a linear rule in activated space looks curved in original space.\n",
    "    \"\"\"\n",
    "    # Select function\n",
    "    funcs = {\n",
    "        'Sigmoid': sigmoid,\n",
    "        'Tanh': tanh,\n",
    "        'ReLU': relu\n",
    "    }\n",
    "    \n",
    "    if activation_name not in funcs:\n",
    "        print(\"Step creates too much compression - try Sigmoid, Tanh, or ReLU!\")\n",
    "        return\n",
    "    \n",
    "    func = funcs[activation_name]\n",
    "    \n",
    "    # Create a grid of points\n",
    "    x1 = np.linspace(-3, 3, 60)\n",
    "    x2 = np.linspace(-3, 3, 60)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    points_original = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "    \n",
    "    # Apply activation\n",
    "    points_activated = apply_activation_2d(points_original, func)\n",
    "    \n",
    "    # Apply linear rule in activated space\n",
    "    classification = linear_rule_after_activation(points_activated, threshold)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), dpi=100)\n",
    "    \n",
    "    # LEFT: Original space\n",
    "    ax1.scatter(points_original[classification, 0], \n",
    "               points_original[classification, 1],\n",
    "               c='red', s=10, alpha=0.3, label='Class A')\n",
    "    ax1.scatter(points_original[~classification, 0], \n",
    "               points_original[~classification, 1],\n",
    "               c='blue', s=10, alpha=0.3, label='Class B')\n",
    "    \n",
    "    ax1.set_xlabel('x₁', fontsize=13)\n",
    "    ax1.set_ylabel('x₂', fontsize=13)\n",
    "    ax1.set_title('ORIGINAL Space: Boundary Looks Curved!', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(fontsize=11, markerscale=3)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_xlim(-3, 3)\n",
    "    ax1.set_ylim(-3, 3)\n",
    "    \n",
    "    # RIGHT: Activated space\n",
    "    ax2.scatter(points_activated[classification, 0], \n",
    "               points_activated[classification, 1],\n",
    "               c='red', s=10, alpha=0.3, label='Class A')\n",
    "    ax2.scatter(points_activated[~classification, 0], \n",
    "               points_activated[~classification, 1],\n",
    "               c='blue', s=10, alpha=0.3, label='Class B')\n",
    "    \n",
    "    # Draw the decision boundary line\n",
    "    if activation_name == 'Sigmoid':\n",
    "        y1_line = np.linspace(0, 1, 100)\n",
    "        xlim, ylim = (0, 1), (0, 1)\n",
    "    elif activation_name == 'Tanh':\n",
    "        y1_line = np.linspace(-1, 1, 100)\n",
    "        xlim, ylim = (-1, 1), (-1, 1)\n",
    "    else:  # ReLU\n",
    "        y1_line = np.linspace(0, 3, 100)\n",
    "        xlim, ylim = (0, 3), (0, 3)\n",
    "    \n",
    "    y2_line = threshold - y1_line\n",
    "    ax2.plot(y1_line, y2_line, 'green', linewidth=3, label=f'Rule: y₁+y₂={threshold:.1f}')\n",
    "    \n",
    "    ax2.set_xlabel(f'{activation_name}(x₁)', fontsize=13)\n",
    "    ax2.set_ylabel(f'{activation_name}(x₂)', fontsize=13)\n",
    "    ax2.set_title('ACTIVATED Space: Boundary is Straight!', fontsize=13, fontweight='bold')\n",
    "    ax2.legend(fontsize=11, markerscale=3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_xlim(xlim)\n",
    "    ax2.set_ylim(ylim)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nKey Insight:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  RIGHT: Boundary is STRAIGHT in activated space\")\n",
    "    print(f\"  LEFT: Same boundary looks CURVED in original space\")\n",
    "    print(f\"\\n  Because {activation_name} warped the space!\")\n",
    "    print(f\"  Simple linear rule after activation = flexible boundary before\")\n",
    "\n",
    "# Interactive widget\n",
    "widgets.interact(\n",
    "    visualize_curved_boundary,\n",
    "    activation_name=Dropdown(\n",
    "        options=['Sigmoid', 'Tanh', 'ReLU'],\n",
    "        value='Sigmoid',\n",
    "        description='Activation:'\n",
    "    ),\n",
    "    threshold=FloatSlider(\n",
    "        min=-0.5, max=2.0, step=0.1, value=0.5,\n",
    "        description='Threshold:',\n",
    "        continuous_update=False\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Questions for Your Answer Sheet\n\n**Q4.** In your own words, what happens to very large positive and very large negative inputs for sigmoid and tanh?\n\n**Q5.** Which activation function changes most rapidly near x = 0? How can you tell from the graph?\n\n**Q6.** For the sigmoid activation, what happens to points that were far away from the origin (large |x₁| or |x₂|)? Where do they end up in the activated space?\n\n**Q7.** For ReLU, what happens to points where x₁ or x₂ is negative? What shape do you see in the activated space?\n\n**Q8.** Which activation warps the grid the most (makes it look least like a square), and how can you tell?\n\n**Q9.** When you look at the original space (left plot in Section 5), does the boundary between the two colors look straight or curved?\n\n**Q10.** In the activated space (right plot in Section 5), what does the boundary look like?\n\n**Q11.** Explain, in one or two sentences, how activation functions help us build more flexible decision rules even if the rule itself is linear after activation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. **Answer Q4-Q11** on your answer sheet\n2. **Return to the LMS** and continue to Module 2\n3. In Module 2, you'll learn more details about each activation function and their properties!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}