# DATA 1010 — Lab 11 Student Handout
## Human-AI Collaboration for Creative Problem Solving

---

## What You'll Learn Today

In this lab, you'll discover a fundamental truth about AI systems:

> **AI is excellent at generating content, but struggles to evaluate its own work.**

Through hands-on experimentation, you will:
- Use AI to generate creative content
- Apply explicit rubrics to evaluate quality
- Discover that AI self-evaluation is unreliable
- Guide AI revision with specific feedback
- Develop principles for responsible human-AI collaboration

**Core Insight:** Generation is easy, evaluation is hard—and humans must provide the judgment.

---

## Lab Structure

| Module | Title | Time | Type |
|--------|-------|------|------|
| Module 0 | Setup and Role Assignment | 5 min | Prelab |
| Module 1 | Generate (Stage 1) | 15-20 min | In-class |
| Module 2 | Evaluate (Stage 2) | 20 min | In-class |
| Module 3 | Revise & Compare (Stage 3) | 20-25 min | In-class |
| Module 4 | Reflection and Synthesis | 10 min | In-class |
| **Total** | | **70-80 min** | |

---

## Working in Groups

### Group Size
Work in groups of **2-4 people**. This lab works best with collaborative discussion.

### Suggested Roles (Flexible)

**Driver:**
- Interacts with the AI system (ChatGPT, Claude, Gemini, etc.)
- Copies prompts and pastes responses
- Reads AI outputs aloud to the group

**Analyst:**
- Records data in the notebooks
- Scores outputs using rubrics
- Takes notes on observations

**Note:** Roles can rotate! Everyone should participate in discussion and decision-making.

### Shared Resources
- **Group Code:** All members use the same code to generate consistent prompts
- **AI System:** Any free LLM works (ChatGPT, Claude, Gemini)
- **Notebook:** One person shares screen, everyone participates

---

## Key Concepts

### Generation vs. Evaluation

**Generation** is the process of creating content:
- AI excels at this—fast, fluent, following patterns
- Can produce drafts, brainstorm ideas, iterate quickly
- **Easy for AI**

**Evaluation** is the process of judging quality:
- Requires defining standards ("good" vs. "bad")
- Involves subjective judgment, context, audience fit
- **Hard for AI, requires human oversight**

### Rubrics

A **rubric** is an explicit set of criteria for evaluating quality.

Example criteria:
- **Clarity:** Is it easy to understand?
- **Audience Fit:** Is it appropriate for the target audience?
- **Accuracy:** Is the information correct?

Rubrics make "good" **concrete and measurable** instead of vague.

### AI Self-Evaluation

When AI "evaluates" its own output, it:
- Uses pattern matching (similar to generation)
- Can't genuinely assess quality like a human
- Often shows **overconfidence** (scores itself higher than deserved) or **underconfidence** (misses its own strengths)

**Key Limitation:** AI cannot reliably predict its own quality.

### Human-AI Collaboration

The most effective use of AI:
- **AI generates** → **Human evaluates** → **AI revises** → **Human judges**
- Humans maintain oversight and set standards
- AI acts as a tool, not an authority

---

## AI Use Policy for This Lab

### What You WILL Do:
✓ Use AI to generate creative content (Module 1)
✓ Ask AI to evaluate its own work (Module 2)
✓ Guide AI to revise based on feedback (Module 3)
✓ Record all AI outputs exactly as given
✓ Apply your own human judgment using rubrics

### What You WILL NOT Do:
✗ Use AI to write your reflection answers (Modules 2-4 questions)
✗ Use AI to generate scores for yourself (human judgment required)
✗ Edit or "improve" AI outputs before recording them
✗ Cherry-pick multiple AI attempts—use the first response

### Why This Matters:
This lab is about **understanding AI's limits**. Cherry-picking the best AI output or letting AI evaluate itself defeats the learning objective.

---

## Module-by-Module Breakdown

### Module 0: Setup and Role Assignment (5 min)

**Type:** Prelab setup

**Learning Objectives:**
- Establish group identity and generate unique scenario
- Understand the lab's three-stage workflow
- Form initial hypotheses about AI self-evaluation

**What You'll Do:**
1. Enter your group code (any positive integer)
2. Generate a deterministic creative prompt from 8 possible families
3. Review your unique scenario
4. Assign Driver and Analyst roles
5. Answer pre-lab questions about evaluation

**Key Concepts:**
- **Deterministic generation:** Same group code always produces the same prompt
- **Prompt families:** 8 types of creative tasks (Science Explainer, PSA, Museum Panel, Product Pitch, Infographic, Narrative, Analogy, Debate)
- **Rubric recommendation:** Each family maps to a suggested evaluation rubric

**Questions:**
- **Q1:** What makes content "good"? Can AI judge its own work?
- **Q2:** Predict: Will AI's self-evaluation be accurate? Why/why not?

---

### Module 1: Generate (Stage 1) (15-20 min)

**Type:** In-class experimentation

**Learning Objectives:**
- Experience AI's content generation capabilities firsthand
- Form initial impressions about quality
- Identify obvious strengths and weaknesses
- Prepare for systematic evaluation in Module 2

**What You'll Do:**
1. Review your group's unique prompt
2. Copy the prompt to an AI system (ChatGPT, Claude, Gemini, etc.)
3. Record the AI's complete output
4. Give a "first impression" rating (1-5 scale)
5. Identify 2 strengths and 2 weaknesses

**Key Concepts:**
- **Hybrid approach:** Notebooks provide structure; you interact with AI via web browsers
- **No code execution:** All interactivity through widgets and text entry
- **Model-agnostic:** Works with any LLM (free versions fine)

**Important Notes:**
- Use AI's **first response**—don't regenerate or ask for revisions yet
- Copy the **complete output**—don't edit or summarize
- If AI asks clarifying questions instead of generating, note that

**Questions:**
- **Q3:** What was your first impression rating (1-5) and why?
- **Q4:** What are the 2 biggest strengths of the AI's output?
- **Q5:** What are the 2 biggest weaknesses or areas for improvement?
- **Q6:** How would you verify if this output is actually "good"?

---

### Module 2: Evaluate (Stage 2) (20 min)

**Type:** In-class evaluation

**Learning Objectives:**
- Apply explicit rubrics to evaluate content quality
- Compare human evaluation vs. AI self-evaluation
- Discover where AI's self-assessment is accurate or inaccurate
- Understand that "good" requires clear, measurable standards

**What You'll Do:**
1. Select a rubric that fits your content type (3 options)
2. Score the AI's output using the rubric (human judgment)
3. Ask the AI to score its OWN work using the same rubric
4. Compare: Where do you agree? Where do you disagree?
5. Identify the 2 weakest areas for revision in Module 3

**The Three Rubrics:**

**1. General Communication** (5 criteria)
Best for: Science Explainers, Museum Panels, Educational Analogies
- Clarity
- Audience Fit
- Structure
- Specificity
- Accuracy

**2. Persuasion/Campaign** (5 criteria)
Best for: PSAs, Product Pitches, Debate Positions
- Message Focus
- Evidence/Support
- Call to Action
- Tone Fit
- Ethics

**3. Creative/Narrative** (4 criteria)
Best for: Short Narratives, Creative Content
- Engagement
- Coherence
- Creativity
- Constraint Adherence

**Key Concepts:**
- **Scoring scale:** 1 (Weak) → 3 (Good) → 5 (Excellent)
- **Human vs. AI evaluation:** Comparing your judgment to AI's self-scores
- **Overconfidence:** AI scoring itself higher than deserved
- **Underconfidence:** AI scoring itself lower than deserved

**Questions:**
- **Q7:** Which rubric did you choose? Why is it the best fit?
- **Q8:** Score the original artifact (criterion by criterion, human judgment)
- **Q9:** What scores did AI give itself? Record AI's reasoning
- **Q10:** Which 2 weaknesses did AI identify?
- **Q11:** Where do you most disagree with AI's self-evaluation?

---

### Module 3: Revise & Compare (Stage 3) (20-25 min)

**Type:** In-class revision

**Learning Objectives:**
- Use rubric feedback to guide AI revision
- Evaluate whether explicit criteria help AI improve
- Identify trade-offs in revision (gains vs. new problems)
- Compare original vs. revised quality systematically
- Understand AI as a revision tool (not an authority)

**What You'll Do:**
1. Review the 2 weakest criteria from Module 2
2. Create a targeted revision prompt
3. Get the AI to revise its work
4. Score the revised version using the same rubric
5. Compare: Did it improve? Where? Did new problems appear?
6. Make an overall judgment: original or revised?

**Effective Revision Prompts:**

**Good approach:**
- Be specific about what to improve
- Reference the rubric criteria by name
- Give concrete examples of what's missing

Example: "Please revise your response to improve **Specificity** by adding concrete examples and data, and improve **Audience Fit** by using simpler language for middle school students."

**Less effective:**
- "Make it better"
- "Fix the problems"
- Vague, non-actionable feedback

**Key Concepts:**
- **Targeted revision:** Focusing on specific weaknesses
- **Iterative improvement:** Generate → Evaluate → Revise cycle
- **Trade-offs:** Improving one criterion may worsen another
- **Human judgment:** You decide if the revision is actually better

**Questions:**
- **Q12:** What 2 criteria were weakest? What revision prompt did you use?
- **Q13:** Did the revised version improve? How? (comparison table)
- **Q14:** Did revision introduce any new problems?
- **Q15:** Overall judgment: Original or Revised better? Why?
- **Q16 (Optional):** If AI scored its revision, was it more/less accurate?
- **Q17:** What would you change if you revised again?

---

### Module 4: Reflection and Synthesis (10 min)

**Type:** In-class reflection

**Learning Objectives:**
- Synthesize findings from all three stages
- Connect lab insights to real-world AI failures
- Articulate when AI is a good collaborator vs. when humans must lead
- Develop personal principles for responsible AI collaboration

**What You'll Do:**
1. Review your findings from Modules 1-3
2. Read case studies about AI evaluation failures
3. Complete reflection questions
4. (Optional) Extension activities

**Case Studies:**

**1. Academic Plagiarism Detection Failure**
- AI plagiarism detector falsely accused students of cheating
- System was overconfident in wrong conclusions
- Human review revealed errors

**2. AI-Generated Peer Review**
- AI review tools gave high scores to deliberately nonsensical academic paper
- Systems evaluated surface features (jargon) over substance (coherent ideas)
- Human reviewers immediately recognized it as gibberish

**3. Creative Writing Competition Judging**
- AI judges penalized creativity and rewarded conventionality
- Couldn't evaluate subjective qualities like "voice" or "originality"
- Human judges' rankings diverged significantly

**Key Insights:**
- AI excels at generation and revision (with guidance)
- AI struggles with evaluation, judgment, and subjective quality
- Explicit standards (rubrics) improve collaboration
- Humans must maintain oversight and authority

**Questions:**
- **Q18:** Where did you most disagree with AI's self-evaluation? Why?
- **Q19:** What did AI do well as a creative assistant?
- **Q20:** What should AI never be solely responsible for?
- **Q21:** Would you trust AI to grade student essays? Why/why not?
- **Q22:** How will you use AI as a collaborator in the future? (3-5 principles)

**Optional Extensions:**
- **Q23:** Swap rubrics with another group. Do scores change?
- **Q24:** Try a different AI model. How do results compare?

---

## Before You Submit

Make sure you have:

- [ ] Completed all 5 modules (0-4)
- [ ] Answered all core questions (Q1-Q22)
- [ ] Recorded AI outputs exactly as generated (Module 1 and 3)
- [ ] Completed human scoring using the rubric (Module 2 and 3)
- [ ] Compared human vs. AI evaluation (Module 2)
- [ ] Compared original vs. revised versions (Module 3)
- [ ] Reflected on real-world implications (Module 4)
- [ ] Developed personal collaboration principles (Q22)
- [ ] Saved all JSON data files from each module

---

## Key Takeaways

### What You Discovered:

1. **Generation is easier than evaluation**
   - AI can produce content quickly and fluently
   - But assessing quality requires human judgment and standards

2. **AI self-evaluation is unreliable**
   - AI cannot accurately judge its own quality
   - Confidence doesn't equal correctness
   - Human-AI disagreement is common and expected

3. **Explicit standards improve collaboration**
   - Rubrics make "good" concrete and measurable
   - Clear feedback enables better AI revision
   - Iteration works: Generate → Evaluate → Revise

4. **Humans must maintain oversight**
   - AI is a tool, not an authority
   - Critical judgment, ethics, and creativity require human involvement
   - The future is human-AI collaboration, not AI replacement

### Connections to Previous Labs:

**Lab 1-2 (Optimization):**
- Evaluation provides the "error signal" for improvement
- Lab 11: AI can optimize toward a target but needs humans to define "good"

**Lab 4 (Neural Networks):**
- Training requires labeled data—humans provide ground truth
- Lab 11: AI learns patterns but doesn't inherently know quality

**Lab 6 (Explainability):**
- AI explanations help but don't replace judgment
- Lab 11: Understanding how AI works ≠ trusting it blindly

**Lab 10 (Hallucination & Self-Assessment):**
- Lab 10: AI can't predict when it will fail (self-assessment)
- Lab 11: AI can't evaluate when its output is good (self-evaluation)
- Both explore limits of AI meta-cognition

### Real-World Applications:

This lab prepares you to:
- ✓ Use AI productively as a creative collaborator
- ✓ Maintain critical oversight and judgment
- ✓ Apply structured evaluation methods (rubrics)
- ✓ Iterate effectively with AI assistance
- ✓ Recognize when AI should and shouldn't be trusted

---

## Final Thought

> **The future of AI collaboration is not "AI replaces humans" but "humans guide AI toward human-defined standards of quality."**

AI is an excellent tool for generation and revision when paired with human evaluation and judgment. Your role is to provide the standards, judgment, and oversight that AI cannot provide for itself.

---

**Good luck and enjoy the lab!**
