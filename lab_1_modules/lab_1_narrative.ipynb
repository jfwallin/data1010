{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Narrative Content\n",
    "\n",
    "This notebook contains all the explanatory/narrative text from the original lab.\n",
    "Use this to extract text for LMS pages.\n",
    "\n",
    "This is NOT given to students - it's for instructor reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 0:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA 1010 ‚Äì Lab 1: Models, Errors, Loss, Optimization, and Learning\n",
    "\n",
    "**Course:** DATA 1010 ‚Äì Artificial Intelligence in Action  \n",
    "**Lab 1 Theme:** How machines measure error, optimize models, and ‚Äúlearn‚Äù from data.  \n",
    "\n",
    "You will work in **small groups** for this lab. One person should share their screen and run the notebook; everyone should be involved in discussion and decisions.\n",
    "\n",
    "This lab has two main goals:\n",
    "\n",
    "1. **Conceptual:** Understand how we measure error, what ‚Äúloss‚Äù means, and how optimization finds better models.\n",
    "2. **Practical:** Get comfortable running and modifying code in Google Colab.\n",
    "\n",
    "You will record your answers to lab questions on a **separate handout or LMS assignment**, *not* inside this notebook.\n"
   ],
   "metadata": {
    "id": "e7IhdoZv1g6N"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 1:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AI Use for This Lab\n",
    "\n",
    "You **may** use AI tools (e.g., ChatGPT, Copilot) to:\n",
    "\n",
    "- Help you understand Python error messages.\n",
    "- Explain what a particular line of code is doing.\n",
    "- Suggest ways to debug when something breaks.\n",
    "\n",
    "You **may not** use AI tools to:\n",
    "\n",
    "- Generate full answers to the conceptual questions on your lab handout.\n",
    "- Choose parameter values for you (e.g., ‚Äúwhat slope should I use?‚Äù).\n",
    "- Run the entire lab for you without group discussion.\n",
    "\n",
    "If you use an AI tool, note **what you used** and **how you used it** in your lab writeup.\n"
   ],
   "metadata": {
    "id": "_bBy4vNb1moJ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 2:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# The Prelab\n",
    "## Complete sections 0 and 1 before class!"
   ],
   "metadata": {
    "id": "xogKWAliMkDv"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 3:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Getting Started in Google Colab\n",
    "\n",
    "Google Colab lets you run Python code in your browser without installing anything.\n",
    "\n",
    "A **cell** is a block of code or text. To run a code cell:\n",
    "\n",
    "1. Click on the cell.\n",
    "2. Press `Shift+Enter` (or click the ‚ñ∂ button on the left).\n",
    "\n",
    "Let‚Äôs test that your Colab environment works.\n"
   ],
   "metadata": {
    "id": "qLp55BSh1rUW"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 5:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.1 Simple Python Variables\n",
    "\n",
    "In this course, we‚Äôll often change **parameters** (numbers that control models) and watch how the output changes.\n",
    "\n",
    "Run the code cell below, then:\n",
    "\n",
    "1. Change the values of `x` and `y`.\n",
    "2. Run the cell again.\n",
    "3. Watch how `z` changes.\n",
    "\n",
    "On your lab handout, you‚Äôll answer questions about what you observe.\n"
   ],
   "metadata": {
    "id": "eHljPtzT197N"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 6:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "l1_SLCKQSnqu"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 8:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.2 Lists, Loops, and a Simple Plot\n",
    "\n",
    "Later in the lab, we‚Äôll work with **many data points** at once.\n",
    "This example shows:\n",
    "\n",
    "- a list of x-values,\n",
    "- computing y-values with a simple rule,\n",
    "- and plotting them.\n",
    "\n",
    "You don‚Äôt need to memorize the syntax; just get used to the idea that:\n",
    "\n",
    "> Python can compute values for many x‚Äôs and plot them all at once.\n"
   ],
   "metadata": {
    "id": "qSsnK7e12DAf"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 10:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Group Code and Randomized Experiments\n",
    "\n",
    "Different groups will see **different data and hidden functions**, but your group will see the **same** setup every time you re-run the notebook.\n",
    "\n",
    "To do this, we‚Äôll use a **group code**:\n",
    "\n",
    "- Choose an integer as a group code (e.g., 1234).\n",
    "- Enter it in the cell below.\n",
    "- This code controls the random number generator.\n",
    "- At the end of the lab, we‚Äôll reveal the exact parameters for your group, and you‚Äôll use them in your writeup.\n"
   ],
   "metadata": {
    "id": "7vgnv76O2UOr"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 13:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What Do We Mean by a *Model*?\n",
    "\n",
    "In this course, the word **model** has a very specific meaning.\n",
    "\n",
    "A **model** is a rule, formula, or pattern that **makes predictions** based on input values.  A model isn't \"reality,\", but it attempts to closely capture how a set of data behaves.\n",
    "\n",
    "### Examples of simple models:\n",
    "\n",
    "- A straight line:  \n",
    "  `prediction = m * x + b`\n",
    "- A rule like ‚Äúevery hour the temperature drops by 2 degrees.‚Äù\n",
    "- A computer program that takes an image and predicts what object is in it.\n",
    "\n",
    "A model has **parameters** ‚Äî numbers that control how it behaves.\n",
    "\n",
    "- For a line, the parameters are the **slope** `m` and **intercept** `b`.\n",
    "- In more complicated models (like neural networks), there can be millions of parameters.\n",
    "\n",
    "### What models do\n",
    "\n",
    "A model:\n",
    "\n",
    "1. **Takes input**  \n",
    "   (for example, a value of `x`)\n",
    "\n",
    "2. **Produces a prediction**  \n",
    "   (for example, a predicted value of `y`)\n",
    "\n",
    "3. **Can be adjusted**  \n",
    "   by changing its parameters\n",
    "\n",
    "### Why we adjust models\n",
    "\n",
    "If a model‚Äôs predictions do not match the data very well, we adjust the parameters to make it better.\n",
    "\n",
    "But the model does *not* know whether its guesses look ‚Äúright.‚Äù  \n",
    "It only receives one piece of information: **the total error** (loss) based on the differences between prediction and data.\n",
    "\n",
    "This is why we spend so much time defining error and loss ‚Äî these numbers tell the model how to improve.\n",
    "\n",
    "### In short:\n",
    "\n",
    "A **model** is a system that:\n",
    "\n",
    "- makes predictions,\n",
    "- depends on adjustable parameters, and\n",
    "- is improved by reducing its total error on data.\n",
    "\n",
    "This idea is the foundation of both simple curve fitting **and** modern AI systems.\n"
   ],
   "metadata": {
    "id": "SpO_bFpGLSf9"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 14:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understanding Error and Total Error (Loss)\n",
    "\n",
    "Before we start fitting models, we need a clear idea of **what ‚Äúerror‚Äù means**.\n",
    "\n",
    "### **1.1 Error at a Single Data Point**\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "- a true value from data: `actual`,  \n",
    "- a predicted value from our model: `predicted`.\n",
    "\n",
    "The **error** at that point is simply the difference:\n",
    "\n",
    "\n",
    "Positive error means the model predicted too *low*.  \n",
    "Negative error means the model predicted too *high*.\n",
    "\n",
    "### **1.2 Why We Don‚Äôt Just Add These Errors**\n",
    "\n",
    "If we add all the raw errors for a dataset, something bad happens:\n",
    "\n",
    "- some errors are positive,\n",
    "- some are negative,\n",
    "- and they can cancel each other out even if the model is doing a terrible job.\n",
    "\n",
    "Example:  \n",
    "If one point is off by +5 and another is off by ‚Äì5, the total would look like ‚Äú0‚Äù,  \n",
    "even though both points are badly wrong.\n",
    "\n",
    "We don‚Äôt want cancellations ‚Äî we want a measure of **how wrong** the model is overall.\n",
    "\n",
    "### **1.3 Squaring Errors to Make Them All Positive**\n",
    "\n",
    "To fix this, we **square** each individual error:\n",
    "\n",
    "squared_error = (actual ‚Äì predicted)^2\n",
    "\n",
    "\n",
    "This has three important effects:\n",
    "\n",
    "- All errors become **positive**, so nothing cancels out.\n",
    "- Larger mistakes become **much more noticeable** (a big miss matters more than a small one).\n",
    "- The math works out nicely later when we do optimization.\n",
    "\n",
    "### **1.4 Total Error (also called Loss)**\n",
    "\n",
    "To measure how well a model fits *all* the data points, we combine the squared errors:\n",
    "total_error = squared_error_1 + squared_error_2 + squared_error_3 + ...\n",
    "\n",
    "\n",
    "This is often called the **loss**.\n",
    "\n",
    "A **good** model has a **small** total error.  \n",
    "A **bad** model has a **large** total error.\n",
    "\n",
    "### **1.5 Why We Use Total Error in Optimization**\n",
    "\n",
    "When we‚Äôre adjusting model parameters (like slope and intercept), the only thing the computer ‚Äúlooks at‚Äù is the **total error**. It doesn‚Äôt see the data points or the line shape the way a person does.\n",
    "\n",
    "The optimizer tries different values, checks the total error, and tries to reduce that number.  \n",
    "\n",
    "This is why understanding *how* we compute error is essential ‚Äî it becomes the entire ‚Äúsignal‚Äù that guides learning.\n",
    "\n"
   ],
   "metadata": {
    "id": "USRHaCv_Kn4m"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 15:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# The in class lab\n",
    "## Work on the cells 2 through 6 with in your lab group"
   ],
   "metadata": {
    "id": "7rGdWKbJMHBZ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 16:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Error and Loss ‚Äì Global Error with Visible Line\n",
    "\n",
    "First, we‚Äôll **show** you the true line and compute its global error (loss) for your noisy data.\n",
    "\n",
    "- Each point has some **error**: the vertical distance between the point and the line.\n",
    "- The **global error** (or **loss**) is the sum of the squared errors across all points.\n",
    "\n",
    "Run the next cell and look at:\n",
    "\n",
    "- The scatter of data points.\n",
    "- The true line.\n",
    "- The printed **Sum of Squared Errors (SSE)**.\n",
    "\n",
    "üëâ On your handout, answer:\n",
    "\n",
    "**Q1.** In your own words, what does ‚Äúglobal error‚Äù or ‚Äúloss‚Äù measure in this plot?  \n",
    "**Q2.** If we changed the slope or intercept of the line, how would that change the loss?\n"
   ],
   "metadata": {
    "id": "6FHkjYXX2ra1"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 18:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Error and Loss ‚Äì Exploring Fits with a Hidden True Line\n",
    "\n",
    "Now we‚Äôll **hide** the true line and let you experiment with your own line.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Use sliders to choose a **slope** and **intercept**.\n",
    "- See your line plotted with the data.\n",
    "- See the **global error (SSE)** for your line.\n",
    "- See **residuals**: vertical lines showing the error at each point.\n",
    "- Get **‚Äúwarmer/colder‚Äù** feedback comparing your current error to your previous one.\n",
    "- See a **history table** of your recent attempts.\n",
    "\n",
    "üëâ On your handout, you will answer questions about:\n",
    "\n",
    "- How local (point-wise) errors relate to global error.\n",
    "- Whether you can have a small global error even if some points have large errors.\n"
   ],
   "metadata": {
    "id": "UCrJ7Phi21eS"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 20:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Local vs Global Error ‚Äì Reflection\n",
    "\n",
    "Use the sliders for a while and try to make the **global error (SSE)** as small as you can.\n",
    "\n",
    "üëâ On your handout, answer:\n",
    "\n",
    "**Q3.** How do the residual lines (the dashed vertical lines) help you understand the **local error** at each point?  \n",
    "\n",
    "**Q4.** Can you make the global error small even if a few points still have relatively large errors? Describe a situation where this happens and why.  \n",
    "\n",
    "**Q5.** Suppose you add one very extreme outlier point far away from the others. Predict how this will affect the best-fit line and the global error.\n"
   ],
   "metadata": {
    "id": "nfO2r3VP32q_"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 21:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Optimizing Only on Global Error (MSE) ‚Äì Parameter Space Game\n",
    "\n",
    "In the last section, you could see both:\n",
    "\n",
    "- the **data points**,\n",
    "- and the **line** you were fitting,\n",
    "\n",
    "plus local residuals and the global error (SSE).\n",
    "\n",
    "In this section, we will **hide everything except the global error**.\n",
    "\n",
    "We will work in **parameter space**:\n",
    "\n",
    "- The horizontal axis will be the slope **m**.\n",
    "- The vertical axis will be the intercept **b**.\n",
    "- Each guess (m, b) corresponds to a line `y = m x + b` that could be fit to the data.\n",
    "\n",
    "For each guess, the computer will:\n",
    "\n",
    "1. Compute the **Mean Squared Error (MSE)** between your line and the data.\n",
    "2. Add your guess (m, b, MSE) to a **table**.\n",
    "3. Plot your guesses as **points in the (m, b) plane**, with the **color** of the most recent point showing its MSE (lower is better).\n",
    "\n",
    "You will **not** see:\n",
    "\n",
    "- the underlying data points,\n",
    "- the true line,\n",
    "- or the full error surface,\n",
    "\n",
    "until the very end.\n",
    "\n",
    "**Your goal:**  \n",
    "Use only the global error (MSE) and the history of your guesses to move toward a good fit (a small MSE). This is directly analogous to what optimization algorithms do in machine learning: they ‚Äúsee‚Äù only the loss and adjust parameters to reduce it.\n",
    "\n",
    "When your group feels you have done enough exploration, click the **‚ÄúDone‚Äù** button. Then the notebook will reveal:\n",
    "\n",
    "- a **color map** of MSE over the (m, b) grid,\n",
    "- your guess history overlaid,\n",
    "- and the location of the actual minimum.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "5fWQV3avCPVB"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 23:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Reflection on Optimizing Global Error\n",
    "\n",
    "In this game, you only saw the **global error (MSE)** and the history of your guesses in parameter space.\n",
    "\n",
    "üëâ On your handout, answer:\n",
    "\n",
    "**Q6.** Describe how your guesses for (m, b) moved over time. Did you follow any systematic strategy (e.g., ‚Äúmove m a bit, then adjust b‚Äù, or ‚Äúsearch in a grid‚Äù, etc.)?\n",
    "\n",
    "**Q7.** Look at the MSE landscape plot. How close was your best guess to:\n",
    "- the approximate global minimum on the grid, and  \n",
    "- the least-squares solution from the data?\n",
    "\n",
    "What does this tell you about optimizing *only* based on the global error?\n",
    "\n",
    "**Q8.** In our earlier line-fitting exercise, you could see the data, the line, and the residuals. In this game, you only saw the MSE. How is this situation similar to how many machine learning models are trained, where the algorithm only sees a **loss value** and not the ‚Äúright answer‚Äù in a human-readable way?\n"
   ],
   "metadata": {
    "id": "pE--Xeo4CPeV"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 24:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "2Ry_thKMCPhs"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 25:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Optimization Game with a Hidden Function (Bounded, No Function Plot)\n",
    "\n",
    "In this game, there is a **hidden function** `f(x)`.  This is not just a linear curve, but a more complicated function.  Finding the minimum value doesn't depend on just fitting a line.  We can use the same ideas to fit **ANY** curve.\n",
    "\n",
    "- You will choose values of **x** in a given range (for this lab: from **-10 to 10**).\n",
    "- The computer will tell you **f(x)** at your chosen x.\n",
    "- Your goal is to find an x that gives a **small** value of f(x) (ideally near the minimum).\n",
    "\n",
    "Important:\n",
    "\n",
    "- You will **not** see the full curve of the function, only your guesses.\n",
    "- The valid range is **-10 ‚â§ x ‚â§ 10**. Values outside this range will be rejected.\n",
    "- Type **999** to stop when you are done experimenting.\n",
    "\n",
    "The notebook will keep a running **scatter plot** of your guesses and a **table** of recent attempts, updating them in place as you go.  You should use the table and the graph to get the best value.\n"
   ],
   "metadata": {
    "id": "zPvN03vK35zI"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 28:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Optimization Reflection\n",
    "\n",
    "After you‚Äôve tried several values of x and looked at your plots and history:\n",
    "\n",
    "üëâ On your handout, answer:\n",
    "\n",
    "**Q9.** What strategies did your group use to choose new values of x within the allowed range?  \n",
    "\n",
    "**Q10.** How did the ‚Äúwarmer/colder‚Äù feedback influence your choices?  \n",
    "\n",
    "**Q11.** Imagine that even the scatter plot of your guesses was hidden, and you only saw the table of (x, f(x)). Would you still be able to find a good minimum? How?\n"
   ],
   "metadata": {
    "id": "6auLtDle4hRM"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 30:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Finding a Mountain Peak ‚Äì Optimization Beyond Line Fitting\n",
    "\n",
    "So far, our optimization examples involved **fitting a line** to data:\n",
    "\n",
    "- Parameters: slope *m* and intercept *b*.\n",
    "- Goal: choose (*m*, *b*) to make the **global error** (loss) small.\n",
    "\n",
    "But optimization and learning are **not just about line fitting**.\n",
    "\n",
    "In many problems, we want to:\n",
    "\n",
    "- **maximize** something (e.g., profit, altitude, reward), or  \n",
    "- **minimize** something (e.g., cost, time, error),\n",
    "\n",
    "over a space of possible choices.\n",
    "\n",
    "In this activity, imagine you are exploring a **mountain range**:\n",
    "\n",
    "- For any coordinate *(x, y)*, there is an **altitude** `h(x, y)`.\n",
    "- You can ‚Äúvisit‚Äù points in the landscape and measure the altitude, but:\n",
    "  - you **cannot** see the whole mountain range,\n",
    "  - you only see the locations you choose and their altitudes.\n",
    "\n",
    "However, the landscape is tricky:\n",
    "\n",
    "- There is **not just one peak**, but **several peaks**.\n",
    "- Some peaks are higher than others.\n",
    "\n",
    "Your job is to use **samples of altitude** to:\n",
    "\n",
    "- search for high regions,\n",
    "- try to find the **highest peak** you can,\n",
    "- and notice the difference between **local peaks** and a **global peak**.\n",
    "\n",
    "At the end, we‚Äôll reveal the full mountain landscape and compare:\n",
    "\n",
    "- your best-found peak,\n",
    "- the true global peak,\n",
    "- and the other local peaks you might have visited.\n"
   ],
   "metadata": {
    "id": "RGdZSoOIHEaZ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 32:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Local vs Global Peaks ‚Äì Reflection\n",
    "\n",
    "In this activity, you only saw the **altitude** at the points your group chose, plus a color-coded scatter of your samples. The full mountain landscape (with multiple peaks) only appeared at the end.\n",
    "\n",
    "üëâ On your handout, answer:\n",
    "\n",
    "**Q12.** Describe your group‚Äôs strategy for choosing new (x, y) locations. How did you decide where to sample next after finding a high point?  \n",
    "\n",
    "**Q13.** Look at the revealed landscape. How many local peaks can you see? Did your group spend most of its time near one peak, or did you explore multiple regions?  \n",
    "\n",
    "**Q14.** Compare your best sample to the true global peak shown on the plot. Were you close to the global maximum, or did you end up stuck near a local maximum?  \n",
    "\n",
    "**Q15.** Explain how this mountain-peak search is similar to what happens in machine learning when an algorithm is trying to optimize a loss function that has many ‚Äúbumps‚Äù (local minima or maxima). What risks does a model face if it only explores one region of the loss landscape?\n",
    "\n",
    "This example shows that **optimization and learning** are not just about fitting lines. The same ideas apply to many problems where we are searching over a landscape (parameter space, positions, strategies) using only local feedback, not the full picture.\n"
   ],
   "metadata": {
    "id": "NlF5nr4rHYSq"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n**Original Cell 34:**\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. From Error and Optimization to Learning\n",
    "\n",
    "You‚Äôve seen three ideas in this lab:\n",
    "\n",
    "1. **Error at a point** (residual): how far a prediction is from the actual value.\n",
    "2. **Global loss** (SSE): sum of squared errors over all data points.\n",
    "3. **Optimization**: choosing parameters to make the maximize or minimize a value.\n",
    "\n",
    "In a very simplified view, many machine learning systems follow this pattern:\n",
    "\n",
    "```text\n",
    "Data ‚Üí Model ‚Üí Error ‚Üí Loss ‚Üí Optimization ‚Üí Updated Model ‚Üí Predictions\n",
    "6"
   ],
   "metadata": {
    "id": "GuxSmlNe4qZH"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}