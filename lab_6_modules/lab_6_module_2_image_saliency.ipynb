{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6, Module 2: Image Saliency with MobileNetV2\n",
    "\n",
    "**Estimated time:** 20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: From Words to Pixels**\n",
    "\n",
    "In **Module 1**, you learned how to find which **words** matter most for text predictions.\n",
    "\n",
    "Now let's ask the same question for images:\n",
    "\n",
    "> When a model classifies an image as \"Golden Retriever\", **which pixels** drove that decision?\n",
    "\n",
    "### **Why This Matters:**\n",
    "\n",
    "Imagine a medical AI that analyzes X-rays for lung cancer. If it achieves 95% accuracy, that's impressive‚Äîbut:\n",
    "- **Is it looking at the lung tissue?** ‚úì Good\n",
    "- **Or is it focusing on the hospital logo in the corner?** ‚úó Spurious correlation\n",
    "\n",
    "Saliency maps let us **see what the model sees**‚Äîrevealing both successes and failures.\n",
    "\n",
    "---\n",
    "\n",
    "# üìò **How Image Saliency Works**\n",
    "\n",
    "Unlike text (where we could just remove words), we can't simply \"delete\" pixels. Instead, we use **gradients**‚Äîremember them from Lab 2?\n",
    "\n",
    "### **The Core Idea:**\n",
    "\n",
    "Recall from **Lab 2** that gradients measure **sensitivity to changes**:\n",
    "- When training, we compute gradients with respect to **weights** (how to change parameters)\n",
    "- For saliency, we compute gradients with respect to **input pixels** (which pixels affect the output most)\n",
    "\n",
    "### **The Math (Intuition Only, No Calculus Required):**\n",
    "\n",
    "```\n",
    "Saliency(pixel) = How much would the class score change if we changed this pixel slightly?\n",
    "```\n",
    "\n",
    "- **High gradient** ‚Üí small pixel change ‚Üí big prediction change ‚Üí **important pixel**\n",
    "- **Low gradient** ‚Üí small pixel change ‚Üí no prediction change ‚Üí **unimportant pixel**\n",
    "\n",
    "### **Visual Example:**\n",
    "\n",
    "For an image of a dog:\n",
    "- **Ears, snout, eyes:** High saliency (distinctive dog features)\n",
    "- **Grass, sky background:** Low saliency (doesn't help identify \"dog\")\n",
    "\n",
    "---\n",
    "\n",
    "## üß± **Setup: Loading a Pre-Trained Image Classifier**\n",
    "\n",
    "We'll use **MobileNetV2**, a lightweight image classifier:\n",
    "- Pre-trained on **ImageNet** (1.4 million images, 1000 classes)\n",
    "- Recognizes objects like dogs, cats, vehicles, food, etc.\n",
    "- Only **14 MB** in size (perfect for Colab!)\n",
    "- Runs on CPU in ~2 seconds per image\n",
    "\n",
    "This is a production-quality model used in mobile apps and edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow matplotlib numpy pillow -q\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "print(\"‚úì Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2\n",
    "print(\"Loading MobileNetV2 model (this may take a few seconds to download)...\")\n",
    "model = MobileNetV2(weights='imagenet', include_top=True)\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "print(f\"Model can recognize {model.output_shape[1]} different object classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üñºÔ∏è **Loading and Classifying Images**\n",
    "\n",
    "First, let's see how the model classifies images. We'll start with some example images.\n",
    "\n",
    "### **Option 1: Load from URL (Built-in Examples)**\n",
    "\n",
    "We'll provide some sample images for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image_from_url(url, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load an image from URL and preprocess for MobileNetV2.\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # Download image\n",
    "    with urllib.request.urlopen(url) as url_response:\n",
    "        img_data = url_response.read()\n",
    "    \n",
    "    # Load as PIL Image\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "    img = img.convert('RGB')  # Ensure RGB format\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert to array and preprocess\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    return img, img_array\n",
    "\n",
    "def classify_image(img_array, model, top_k=3):\n",
    "    \"\"\"\n",
    "    Classify an image and return top predictions.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    decoded = decode_predictions(predictions, top=top_k)[0]\n",
    "    return decoded\n",
    "\n",
    "print(\"‚úì Image loading functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example Image 1: Golden Retriever**\n",
    "\n",
    "Let's start with a dog image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dog image from a public URL\n",
    "dog_url = \"https://hips.hearstapps.com/hmg-prod/images/golden-retriever-royalty-free-image-506756303-1560962726.jpg\"\n",
    "print(\"Loading dog image...\")\n",
    "dog_img, dog_array = load_and_preprocess_image_from_url(dog_url)\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(dog_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image: Golden Retriever\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Classify\n",
    "predictions = classify_image(dog_array, model)\n",
    "print(\"\\nTop 3 predictions:\")\n",
    "for i, (imagenet_id, label, score) in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {label}: {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî• **Computing Saliency Maps**\n",
    "\n",
    "Now let's compute which pixels matter most for the model's prediction.\n",
    "\n",
    "We'll use **TensorFlow's GradientTape**‚Äîa tool that automatically computes gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(img_array, model, class_idx):\n",
    "    \"\"\"\n",
    "    Compute saliency map using gradient √ó input method.\n",
    "    \n",
    "    Args:\n",
    "        img_array: Preprocessed image (1, 224, 224, 3)\n",
    "        model: Trained Keras model\n",
    "        class_idx: Index of target class\n",
    "    \n",
    "    Returns:\n",
    "        saliency_map: 2D array showing pixel importance\n",
    "    \"\"\"\n",
    "    # Convert to tensor\n",
    "    img_tensor = tf.convert_to_tensor(img_array)\n",
    "    \n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img_tensor)\n",
    "        predictions = model(img_tensor)\n",
    "        target_class_score = predictions[0, class_idx]\n",
    "    \n",
    "    # Get gradients of target class score with respect to input image\n",
    "    gradients = tape.gradient(target_class_score, img_tensor)\n",
    "    \n",
    "    # Compute saliency as maximum absolute gradient across color channels\n",
    "    saliency = tf.reduce_max(tf.abs(gradients), axis=-1)\n",
    "    saliency = saliency.numpy()[0]  # Remove batch dimension\n",
    "    \n",
    "    # Normalize to [0, 1] for visualization\n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
    "    \n",
    "    return saliency\n",
    "\n",
    "print(\"‚úì Saliency computation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing Saliency Maps**\n",
    "\n",
    "Let's create a function to show:\n",
    "1. Original image\n",
    "2. Saliency heatmap\n",
    "3. Overlay (saliency on top of original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_saliency(original_img, img_array, model, class_idx, class_label):\n",
    "    \"\"\"\n",
    "    Visualize saliency map with 3 views: original, heatmap, overlay.\n",
    "    \"\"\"\n",
    "    # Compute saliency\n",
    "    saliency_map = compute_saliency_map(img_array, model, class_idx)\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Original image\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # 2. Saliency heatmap\n",
    "    im = axes[1].imshow(saliency_map, cmap='hot', interpolation='bilinear')\n",
    "    axes[1].set_title(f\"Saliency Map\\n(Prediction: {class_label})\", fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 3. Overlay\n",
    "    axes[2].imshow(original_img)\n",
    "    axes[2].imshow(saliency_map, cmap='hot', alpha=0.5, interpolation='bilinear')\n",
    "    axes[2].set_title(\"Overlay (Original + Saliency)\", fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Saliency map computed for class '{class_label}'\")\n",
    "    print(f\"  Red/bright regions = high saliency (important for prediction)\")\n",
    "    print(f\"  Blue/dark regions = low saliency (not important)\")\n",
    "\n",
    "print(\"‚úì Visualization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **Example 1: Dog Saliency**\n",
    "\n",
    "Let's see which pixels matter for recognizing the Golden Retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top prediction\n",
    "predictions = classify_image(dog_array, model)\n",
    "top_class_id = np.argmax(model.predict(dog_array, verbose=0)[0])\n",
    "top_class_label = predictions[0][1]\n",
    "\n",
    "# Visualize saliency\n",
    "visualize_saliency(dog_img, dog_array, model, top_class_id, top_class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What to Notice:**\n",
    "\n",
    "Look at the saliency map and overlay:\n",
    "- **Dog's face (ears, snout, eyes)** ‚Üí Likely HIGH saliency (bright red/yellow)\n",
    "- **Fur texture** ‚Üí Moderate saliency\n",
    "- **Background (grass, sky)** ‚Üí LOW saliency (dark blue)\n",
    "\n",
    "**This makes sense!** The model focuses on distinctive dog features, not the background.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 9 (Observation)**\n",
    "\n",
    "**Q9.** For the dog image, which parts of the image had the highest saliency? Does this make sense for recognizing a dog?\n",
    "\n",
    "*Look at the red/bright regions in the heatmap. Are they on the dog's body or the background? Which specific features (ears, face, legs, etc.) are highlighted?*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Example 2: Handwritten Digit**\n",
    "\n",
    "Let's try a different type of image‚Äîa handwritten digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST digit (we'll use a sample from the web)\n",
    "digit_url = \"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamples.png\"\n",
    "\n",
    "print(\"Loading digit image...\")\n",
    "# Note: This URL shows multiple digits. For demo, we'll use a simple digit image\n",
    "# In practice, you'd crop a single digit. For now, let's use a simpler approach:\n",
    "\n",
    "# Create a simple handwritten-style 8\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Create a simple digit image\n",
    "digit_img = Image.new('RGB', (224, 224), color='white')\n",
    "draw = ImageDraw.Draw(digit_img)\n",
    "\n",
    "# Draw a large \"8\" shape\n",
    "draw.ellipse([60, 40, 164, 120], outline='black', width=8)\n",
    "draw.ellipse([60, 104, 164, 184], outline='black', width=8)\n",
    "\n",
    "# Preprocess\n",
    "digit_array = image.img_to_array(digit_img)\n",
    "digit_array = np.expand_dims(digit_array, axis=0)\n",
    "digit_array = preprocess_input(digit_array)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(digit_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image: Handwritten '8'\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Classify\n",
    "predictions = classify_image(digit_array, model)\n",
    "print(\"\\nTop 3 predictions:\")\n",
    "for i, (imagenet_id, label, score) in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {label}: {score*100:.2f}%\")\n",
    "\n",
    "print(\"\\nNote: MobileNetV2 wasn't trained on digits, so predictions may be unexpected!\")\n",
    "print(\"But we can still compute saliency to see which pixels matter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize saliency for digit\n",
    "top_class_id = np.argmax(model.predict(digit_array, verbose=0)[0])\n",
    "predictions = classify_image(digit_array, model)\n",
    "top_class_label = predictions[0][1]\n",
    "\n",
    "visualize_saliency(digit_img, digit_array, model, top_class_id, top_class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question 10 (Observation)**\n",
    "\n",
    "**Q10.** For the handwritten digit, which parts had high saliency? Why would those regions be important?\n",
    "\n",
    "*Look at where the bright regions are. Are they on the loops? The edges? What do these regions tell you about what the model \"sees\"?*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ **Upload Your Own Image!**\n",
    "\n",
    "Now it's your turn! Upload any image and see:\n",
    "1. What the model classifies it as\n",
    "2. Which pixels drove that classification\n",
    "\n",
    "### **Instructions:**\n",
    "1. Run the cell below\n",
    "2. Click \"Choose Files\" and select an image from your computer\n",
    "3. The image will be classified and saliency will be computed automatically\n",
    "\n",
    "**Tips:**\n",
    "- Try photos of animals, objects, vehicles, food\n",
    "- MobileNetV2 recognizes 1000 classes from ImageNet\n",
    "- Works best with clear, centered objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image upload widget for Colab\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def process_uploaded_image():\n",
    "    \"\"\"\n",
    "    Handle image upload and process it.\n",
    "    \"\"\"\n",
    "    print(\"Please upload an image (JPG, PNG, etc.):\\n\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if not uploaded:\n",
    "        print(\"No file uploaded.\")\n",
    "        return\n",
    "    \n",
    "    # Get the first uploaded file\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    \n",
    "    # Load and preprocess\n",
    "    img = Image.open(io.BytesIO(uploaded[filename]))\n",
    "    img = img.convert('RGB')\n",
    "    img_resized = img.resize((224, 224))\n",
    "    \n",
    "    img_array = image.img_to_array(img_resized)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # Display original\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Your Uploaded Image: {filename}\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    # Classify\n",
    "    predictions = classify_image(img_array, model, top_k=5)\n",
    "    print(\"\\nTop 5 predictions:\")\n",
    "    for i, (imagenet_id, label, score) in enumerate(predictions, 1):\n",
    "        print(f\"{i}. {label}: {score*100:.2f}%\")\n",
    "    \n",
    "    # Compute and visualize saliency\n",
    "    print(\"\\nComputing saliency map...\\n\")\n",
    "    top_class_id = np.argmax(model.predict(img_array, verbose=0)[0])\n",
    "    top_class_label = predictions[0][1]\n",
    "    \n",
    "    visualize_saliency(img_resized, img_array, model, top_class_id, top_class_label)\n",
    "    \n",
    "    return img, predictions\n",
    "\n",
    "# Run the upload process\n",
    "my_image = process_uploaded_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Questions 11-12 (Experimentation)**\n",
    "\n",
    "**Q11.** Upload your own image using the cell above. What object did the model classify it as? What was the top prediction?\n",
    "\n",
    "- **My uploaded image:** (describe it) _____________________\n",
    "- **Top prediction:** _______________\n",
    "- **Confidence:** ______%\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "**Q12.** For your uploaded image, which parts had the highest saliency? Does this reveal how the model recognized the object?\n",
    "\n",
    "*Think about: Did the model focus on the right features? Or did it focus on something unexpected (background, texture, etc.)?*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **When Saliency Reveals Problems**\n",
    "\n",
    "Sometimes saliency maps reveal that models focus on the **wrong features**. This is called **spurious correlation**.\n",
    "\n",
    "### **Real-World Example: Medical Imaging**\n",
    "\n",
    "**Problem:** An AI trained to detect pneumonia from chest X-rays achieved 90% accuracy.\n",
    "\n",
    "**Saliency revealed:** The model focused on:\n",
    "- Hospital logos and watermarks in the corners\n",
    "- Patient positioning markers\n",
    "- Image quality artifacts\n",
    "\n",
    "**Why?** Different hospitals used different imaging equipment. Certain hospitals happened to have more pneumonia cases. The model learned to recognize **hospitals** instead of **lung pathology**!\n",
    "\n",
    "**Solution:** Retrain with more diverse data, remove watermarks, use data augmentation.\n",
    "\n",
    "### **Another Example: Husky vs. Wolf**\n",
    "\n",
    "A famous case study found that a \"Husky vs. Wolf\" classifier focused on:\n",
    "- **Background snow** (wolves photographed in snow)\n",
    "- **Green grass** (huskies photographed in yards)\n",
    "\n",
    "Not on the animals themselves!\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 13 (Critical Thinking)**\n",
    "\n",
    "**Q13.** Looking at the saliency maps you've seen, can you think of a scenario where a model might focus on the \"wrong\" features?\n",
    "\n",
    "*Hint: Think about spurious correlations‚Äîwhen two things appear together by coincidence, not causation.*\n",
    "\n",
    "*Examples: Backgrounds, watermarks, image quality, lighting, etc.*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Comparing Text and Image Saliency**\n",
    "\n",
    "Let's reflect on what's **similar** and **different** between Module 1 (text) and Module 2 (images):\n",
    "\n",
    "| Aspect | Text Saliency (Module 1) | Image Saliency (Module 2) |\n",
    "|--------|--------------------------|---------------------------|\n",
    "| **Input type** | Words in a sentence | Pixels in an image |\n",
    "| **Method** | Masking (remove words) | Gradients (compute sensitivity) |\n",
    "| **Output** | Importance per word | Importance per pixel |\n",
    "| **Visualization** | Bar chart | Heatmap overlay |\n",
    "| **Interpretation** | Which words drive sentiment? | Which pixels drive classification? |\n",
    "\n",
    "### **What's Similar:**\n",
    "- Both measure **sensitivity**: how much the prediction changes when we change the input\n",
    "- Both reveal **which features matter most**\n",
    "- Both help **debug models** and **detect spurious correlations**\n",
    "\n",
    "### **What's Different:**\n",
    "- Text is **discrete** (remove whole words) vs. images are **continuous** (use gradients)\n",
    "- Text has clear **semantic units** (words) vs. pixels have no inherent meaning\n",
    "- Text saliency is **sparse** (few important words) vs. image saliency is **dense** (regions of pixels)\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question 14 (Synthesis)**\n",
    "\n",
    "**Q14.** How is image saliency different from the word saliency you explored in Module 1? What's similar?\n",
    "\n",
    "*Think about: The methods used, the visualizations, what they reveal, and their limitations.*\n",
    "\n",
    "*Record your answer in the Answer Sheet.*\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Module 2 Complete!\n",
    "\n",
    "You've learned:\n",
    "- **How gradient-based saliency works** (sensitivity to pixel changes)\n",
    "- **Which pixels matter for image classification** (objects vs. backgrounds)\n",
    "- **How to compute and visualize saliency heatmaps**\n",
    "- **Why saliency is crucial for debugging** (detecting spurious correlations)\n",
    "- **How to interpret saliency overlays** (red = important, blue = unimportant)\n",
    "\n",
    "**Key Insight:** Saliency reveals **what the model actually looks at**‚Äîsometimes it's the right features, sometimes it's not!\n",
    "\n",
    "**Next up:** Module 3, where you'll explore **tabular saliency** for structured data‚Äîand discover how saliency can reveal bias in features like zip codes and demographics.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
