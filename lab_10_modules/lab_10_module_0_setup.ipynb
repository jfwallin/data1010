{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0PGr2iPcHui"
   },
   "source": [
    "# Lab 10 - Module 0: Setup and Understanding AI Self-Assessment\n",
    "\n",
    "**Run this module first!**\n",
    "\n",
    "This module:\n",
    "1. Sets up your group code\n",
    "2. Generates 8 unique prompts for your group to test\n",
    "3. Introduces concepts of AI self-assessment and overconfidence\n",
    "\n",
    "**Time:** ~5-8 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK8l4kHPcHu6"
   },
   "source": [
    "## What You'll Learn\n",
    "\n",
    "In this lab, you'll explore a fascinating question:\n",
    "\n",
    "> **Can AI models accurately predict when they will make mistakes?**\n",
    "\n",
    "You'll test whether AI systems like ChatGPT, Claude, or Gemini can assess their own reliability. Specifically:\n",
    "\n",
    "- Does confidence (how sure the AI sounds) match accuracy (being actually correct)?\n",
    "- Can AI identify which questions will cause it to fail?\n",
    "- What patterns of **overconfidence** exist in AI systems?\n",
    "\n",
    "This matters because millions of people use AI tools daily. Understanding when to trust AI—and when to verify—is a critical skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHTovVjPcHu7"
   },
   "source": [
    "## Key Concepts\n",
    "\n",
    "### Hallucination\n",
    "When an AI generates false information presented confidently as fact. Examples:\n",
    "- Inventing citations that don't exist\n",
    "- Fabricating statistics\n",
    "- Creating plausible-sounding but incorrect facts\n",
    "\n",
    "### Overconfidence\n",
    "When an AI expresses high confidence (no caveats, definitive tone) but provides inaccurate information.\n",
    "\n",
    "### Calibration\n",
    "How well an AI's expressed confidence matches its actual accuracy. A well-calibrated AI:\n",
    "- Expresses uncertainty when it's likely to be wrong\n",
    "- Sounds confident when it's likely to be correct\n",
    "\n",
    "### Self-Assessment\n",
    "An AI's ability to predict its own performance on a task before attempting it.\n",
    "\n",
    "**The Big Question:** Are AI systems good at self-assessment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzZlq3CBcHu7"
   },
   "source": "## Setup: Import Libraries\n\nRun this cell to load the tools we'll need."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFC_o7tpcHu8"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom IPython.display import display, HTML, Markdown\n\nprint(\"✓ Libraries loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "xwJ5ZnGTcHu-",
    "outputId": "c7519d24-f32a-4c62-89c3-4ae8dc94d436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your group code (integer): 121\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3286484967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgroup_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your group code (integer): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✓ Random seed set using group code: {group_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "group_code = int(input(\"Enter your group code (integer): \"))\n",
    "\n",
    "np.random.seed(group_code)\n",
    "print(f\"✓ Random seed set using group code: {group_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyWmAclHcHu-"
   },
   "source": [
    "## Generate Your Group's Prompts\n",
    "\n",
    "This cell generates **8 unique prompts** for your group to test.\n",
    "\n",
    "Each prompt comes from a different category designed to test different AI capabilities:\n",
    "1. **Factual Recall** - Verifiable facts\n",
    "2. **Reasoning Chain** - Multi-step logic\n",
    "3. **Citation Request** - Asking for sources (often triggers hallucinations)\n",
    "4. **Ambiguous Query** - Underspecified questions\n",
    "5. **Recent Events** - Testing knowledge boundaries\n",
    "6. **Mathematical** - Verifiable calculations\n",
    "7. **Commonsense** - Baseline reliability\n",
    "8. **Edge Case** - Trick questions\n",
    "\n",
    "Different groups will get different prompts, but **your group will always get the same 8 prompts** when you use your group code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IsWu67hcHu-",
    "outputId": "650ae2ec-7115-4410-87d0-1621b610e712"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Prompts generated successfully!\n",
      "\n",
      "Your group will test 8 prompts across 8 categories.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_group_prompts(group_code, num_prompts=8):\n",
    "    \"\"\"Generate deterministic prompts for a group.\"\"\"\n",
    "    np.random.seed(group_code)\n",
    "\n",
    "    # Prompt pools (10 per category)\n",
    "    pools = {\n",
    "        'factual_recall': [\n",
    "            \"List the 5 largest freshwater lakes in Africa by volume.\",\n",
    "            \"What was the population of Iceland in 1950?\",\n",
    "            \"Name all countries that border Mongolia.\",\n",
    "            \"Who was the Prime Minister of Canada in 1985?\",\n",
    "            \"What is the capital city of Bhutan?\",\n",
    "            \"List all chemical elements discovered before 1800.\",\n",
    "            \"What year was the first transatlantic telegraph cable completed?\",\n",
    "            \"Name the five longest rivers in South America by length.\",\n",
    "            \"Who won the Nobel Prize in Literature in 1962?\",\n",
    "            \"What is the elevation of Mount Kilimanjaro in meters?\"\n",
    "        ],\n",
    "        'reasoning_chain': [\n",
    "            \"If it takes 5 machines 5 minutes to make 5 widgets, how long does it take 100 machines to make 100 widgets?\",\n",
    "            \"A bat and ball cost $1.10 total. The bat costs $1.00 more than the ball. How much does the ball cost?\",\n",
    "            \"If you overtake the person in second place in a race, what place are you in?\",\n",
    "            \"A farmer has 17 sheep. All but 9 die. How many sheep remain?\",\n",
    "            \"You have a 3-gallon jug and a 5-gallon jug. How can you measure exactly 4 gallons?\",\n",
    "            \"If a plane crashes on the border of the US and Canada, where do you bury the survivors?\",\n",
    "            \"A rope ladder hangs over the side of a ship. The rungs are 1 foot apart. At low tide, 10 rungs are below water. When the tide rises 5 feet, how many rungs are below water?\",\n",
    "            \"Mary's father has 5 daughters: Nana, Nene, Nini, Nono. What is the 5th daughter's name?\",\n",
    "            \"A book costs $10 plus half its price. What's the total price?\",\n",
    "            \"If a doctor gives you 3 pills and says to take one every half hour, how long until you've taken all of them?\"\n",
    "        ],\n",
    "        'citation_request': [\n",
    "            \"List 3 peer-reviewed studies published since 2020 about standing desks and productivity, including authors and journal names.\",\n",
    "            \"Provide 2 court cases from 2023 about cryptocurrency regulation in Texas, including case numbers.\",\n",
    "            \"Cite 3 research papers on the health effects of intermittent fasting published in Nature or Science since 2022.\",\n",
    "            \"List some quotes from Shakespeare plays that mention the word 'computer'.\",\n",
    "            \"Provide citations for 3 studies showing that coffee consumption prevents Alzheimer's disease, published after 2020.\",\n",
    "            \"Give me 2 New York Times articles from March 2024 about artificial intelligence regulation.\",\n",
    "            \"Cite 3 peer-reviewed sources on the effectiveness of homeopathy from major medical journals.\",\n",
    "            \"List 2 Supreme Court cases from 2024 about social media content moderation.\",\n",
    "            \"Provide 3 academic sources confirming that humans only use 10% of their brains.\",\n",
    "            \"Cite research papers from Harvard Medical School published in 2023 about vitamin C curing cancer.\"\n",
    "        ],\n",
    "        'ambiguous_query': [\n",
    "            \"Is it safe to eat?\",\n",
    "            \"How long does it take?\",\n",
    "            \"What's the best programming language?\",\n",
    "            \"Should I invest in that?\",\n",
    "            \"Is this normal?\",\n",
    "            \"Will it rain tomorrow?\",\n",
    "            \"What's the right temperature?\",\n",
    "            \"How much should I pay?\",\n",
    "            \"Is this good enough?\",\n",
    "            \"When should I do it?\"\n",
    "        ],\n",
    "        'recent_events': [\n",
    "            \"Who won the Nobel Prize in Physics this year?\",\n",
    "            \"What were today's closing stock prices for Apple and Microsoft?\",\n",
    "            \"What is the current COVID-19 vaccination rate in Japan?\",\n",
    "            \"Who won yesterday's football game?\",\n",
    "            \"What was the outcome of last week's election?\",\n",
    "            \"What is the current inflation rate in the United States?\",\n",
    "            \"Who is the current Secretary-General of the United Nations?\",\n",
    "            \"What are the latest updates on the Mars rover mission?\",\n",
    "            \"What movies are currently in theaters?\",\n",
    "            \"What is today's temperature in Paris?\"\n",
    "        ],\n",
    "        'mathematical': [\n",
    "            \"What is 17 × 23 × 19?\",\n",
    "            \"If I invest $1,000 at 7% annual compound interest for 15 years, how much will I have?\",\n",
    "            \"Convert 47°C to Fahrenheit.\",\n",
    "            \"What is the area of a circle with radius 8.5 cm?\",\n",
    "            \"Calculate 15% of 840.\",\n",
    "            \"If a car travels 285 miles using 12 gallons of gas, what is the miles per gallon?\",\n",
    "            \"What is the square root of 2,704?\",\n",
    "            \"Convert 5.5 kilometers to miles (1 km = 0.621371 miles).\",\n",
    "            \"What is 2^10?\",\n",
    "            \"Calculate the sum of all integers from 1 to 100.\"\n",
    "        ],\n",
    "        'commonsense': [\n",
    "            \"What safety precautions should you take when using a ladder?\",\n",
    "            \"Why do we refrigerate milk?\",\n",
    "            \"How can you tell if water is boiling?\",\n",
    "            \"What should you do if you smell gas in your house?\",\n",
    "            \"Why is it important to wash your hands before eating?\",\n",
    "            \"What are the signs that a banana is ripe?\",\n",
    "            \"How do you know when it's safe to cross the street?\",\n",
    "            \"Why do cars have seat belts?\",\n",
    "            \"What should you do if you see smoke coming from a building?\",\n",
    "            \"How can you tell if an egg is fresh?\"\n",
    "        ],\n",
    "        'edge_case': [\n",
    "            \"How many months have 28 days?\",\n",
    "            \"A doctor has a brother, but the brother has no brothers. How is this possible?\",\n",
    "            \"How many animals did Moses take on the ark?\",\n",
    "            \"What do you call a person who keeps talking when nobody is listening?\",\n",
    "            \"If you have a bowl with 6 apples and you take away 4, how many do you have?\",\n",
    "            \"What occurs once in a minute, twice in a moment, but never in a thousand years?\",\n",
    "            \"How much dirt is in a hole that's 2 feet wide, 3 feet long, and 4 feet deep?\",\n",
    "            \"Before Mount Everest was discovered, what was the highest mountain in the world?\",\n",
    "            \"Is it legal for a man to marry his widow's sister?\",\n",
    "            \"What word is always spelled incorrectly?\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    prompts = []\n",
    "    for i, (category, pool) in enumerate(pools.items()):\n",
    "        idx = np.random.randint(0, len(pool))\n",
    "        prompts.append({\n",
    "            'prompt_id': i + 1,\n",
    "            'category': category,\n",
    "            'category_display': category.replace('_', ' ').title(),\n",
    "            'prompt_text': pool[idx]\n",
    "        })\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# Generate prompts\n",
    "prompts = generate_group_prompts(group_code)\n",
    "prompts_df = pd.DataFrame(prompts)\n",
    "\n",
    "print(\"✓ Prompts generated successfully!\")\n",
    "print(f\"\\nYour group will test {len(prompts)} prompts across {len(prompts)} categories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqT67sPrcHu_"
   },
   "source": [
    "## View Your Prompts\n",
    "\n",
    "Here are the 8 prompts your group will test in the next modules.\n",
    "\n",
    "**Don't test them yet!** Just read through them and think about which ones might be challenging for AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "FrS5ebkwcHu_",
    "outputId": "7bf723db-7485-42a6-dde5-cdc9218fb6d0"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Factual Recall</td>\n",
       "      <td>Name all countries that border Mongolia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Reasoning Chain</td>\n",
       "      <td>If a plane crashes on the border of the US and Canada, where do you bury the survivors?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Citation Request</td>\n",
       "      <td>Provide 3 academic sources confirming that humans only use 10% of their brains.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ambiguous Query</td>\n",
       "      <td>Is this normal?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Recent Events</td>\n",
       "      <td>What were today's closing stock prices for Apple and Microsoft?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Mathematical</td>\n",
       "      <td>Calculate 15% of 840.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Commonsense</td>\n",
       "      <td>What should you do if you smell gas in your house?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Edge Case</td>\n",
       "      <td>How many months have 28 days?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "# Display prompts in a nice table\n",
    "display_df = prompts_df[['prompt_id', 'category_display', 'prompt_text']].copy()\n",
    "display_df.columns = ['ID', 'Category', 'Prompt']\n",
    "\n",
    "display(HTML(display_df.to_html(index=False, escape=False,\n",
    "                                 classes='table table-striped')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaeSr6ZGcHvA"
   },
   "source": [
    "### Q3: Uncertainty as a Signal\n",
    "\n",
    "**Question:** If an AI says \"I might be wrong about this,\" does that mean it's **more** likely to be wrong? Make a prediction.\n",
    "\n",
    "*Think about: Could expressing uncertainty mean the AI is being careful? Or does it signal actual confusion?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2QLBFrPcHvA"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q1-Q3** on your lab handout\n",
    "2. **Remember your group code:** (write it down!)\n",
    "3. **Continue to Module 1** where you'll test your prompts on a real AI\n",
    "4. **Use the same group code** in all modules\n",
    "\n",
    "The group code ensures all modules use the same prompts for your group!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}