{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Module 1: Gradient Descent on Hidden Parabola\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Apply gradient descent to 1D function optimization\n",
    "- Understand how learning rate affects convergence\n",
    "- Compare automated GD vs. manual search from Lab 1\n",
    "\n",
    "**Time:** ~15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**IMPORTANT:** Enter the same group code from Lab 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Lab 1\n",
    "\n",
    "In **Lab 1 Module 4**, you manually searched for the minimum of a hidden parabola:\n",
    "- Chose x values with a slider\n",
    "- Got warm/cold feedback\n",
    "- Refined your guesses iteratively\n",
    "\n",
    "**Today:** Gradient descent will do this automatically!\n",
    "\n",
    "Same function, but now:\n",
    "- GD computes the slope (gradient)\n",
    "- GD moves downhill automatically\n",
    "- You control the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Load Group Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatText, Button, IntText, Output, VBox, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "group_code = int(input(\"Enter your group code: \"))\n",
    "np.random.seed(group_code)\n",
    "\n",
    "# Generate same hidden function parameters as Lab 1\n",
    "# Skip line parameters (we don't need them here)\n",
    "_ = np.random.uniform(-3, 3)  # true_m (not used)\n",
    "_ = np.random.uniform(-5, 5)  # true_b (not used)\n",
    "\n",
    "# Hidden function parameters (parabola: f(x) = a(x-b)^2 + c)\n",
    "a = np.random.uniform(0.5, 2.0)\n",
    "b_param = np.random.uniform(-4, 4)\n",
    "c_param = np.random.uniform(-10, 10)\n",
    "\n",
    "def hidden_func(x):\n",
    "    \"\"\"The same hidden parabola from Lab 1 Module 4\"\"\"\n",
    "    return a * (x - b_param)**2 + c_param\n",
    "\n",
    "print(\"✓ Hidden parabola loaded (same as Lab 1 Module 4)\")\n",
    "print(f\"\\nTrue minimum is at x ≈ {b_param:.2f} with f(x) ≈ {c_param:.2f}\")\n",
    "print(\"(This is revealed now for learning purposes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Utilities\n",
    "\n",
    "We'll use simple numerical gradient computation (no calculus required!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, func, h=1e-5):\n",
    "    \"\"\"Compute numerical gradient using central difference\"\"\"\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "def gd_step(x, grad, learning_rate):\n",
    "    \"\"\"Single gradient descent step: new = old - lr * gradient\"\"\"\n",
    "    return x - learning_rate * grad\n",
    "\n",
    "print(\"✓ Gradient descent functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Prediction Question (Answer BEFORE running GD)\n\n**Q3 (PREDICTION):** Before running the interactive tool below, predict:\n\nStarting from x = 0.0, with five different learning rates:\n- **LR = 0.01**: Will this converge quickly or slowly? Will it reach the minimum?\n- **LR = 0.05**: Will this converge faster than 0.01?\n- **LR = 0.4**: Will this converge faster than 0.05? Any risks?\n- **LR = 1.0**: Will this be fastest? Or will it have problems?\n- **LR = 3.0**: What do you expect with such a large learning rate?\n\nWrite your predictions on the answer sheet, then test them below!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Gradient Descent\n",
    "\n",
    "**Instructions:**\n",
    "1. Choose a starting point (x₀)\n",
    "2. Choose a learning rate\n",
    "3. Click \"Run 1 Step\" to see one GD update\n",
    "4. Click \"Run 10 Steps\" to see multiple updates\n",
    "5. Click \"Reset\" to start over with new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# State for interactive GD\ngd_state = {\n    'history': [],\n    'running': False\n}\n\n# Widgets\nx0_input = FloatText(description=\"Starting x:\", value=0.0, step=0.5)\nlr_input = FloatText(description=\"Learning rate:\", value=0.1, step=0.01)\nstep_button = Button(description=\"Run 1 Step\", button_style='info')\nmulti_step_button = Button(description=\"Run 10 Steps\", button_style='success')\nreset_button = Button(description=\"Reset\", button_style='warning')\noutput_area = Output()\n\ndef initialize_gd():\n    \"\"\"Initialize or reset GD state\"\"\"\n    x_start = x0_input.value\n    gd_state['history'] = [{\n        'step': 0,\n        'x': x_start,\n        'f(x)': hidden_func(x_start),\n        'gradient': compute_gradient(x_start, hidden_func),\n        'change': 0.0\n    }]\n    gd_state['running'] = True\n\ndef run_gd_steps(n_steps):\n    \"\"\"Run n steps of gradient descent\"\"\"\n    if not gd_state['running']:\n        initialize_gd()\n    \n    lr = lr_input.value\n    \n    for _ in range(n_steps):\n        current = gd_state['history'][-1]\n        x_current = current['x']\n        grad = compute_gradient(x_current, hidden_func)\n        \n        # Update rule\n        change = -lr * grad\n        x_new = x_current + change\n        \n        gd_state['history'].append({\n            'step': current['step'] + 1,\n            'x': x_new,\n            'f(x)': hidden_func(x_new),\n            'gradient': grad,\n            'change': change\n        })\n\ndef update_display():\n    \"\"\"Update visualization and table\"\"\"\n    with output_area:\n        output_area.clear_output(wait=True)\n        \n        if not gd_state['history']:\n            print(\"Click 'Run 1 Step' or 'Run 10 Steps' to start!\")\n            return\n        \n        # Extract history\n        x_hist = [h['x'] for h in gd_state['history']]\n        f_hist = [h['f(x)'] for h in gd_state['history']]\n        \n        # Plot\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n        \n        # Left: Scatter plot of GD path (no function shown)\n        colors = plt.cm.viridis(np.linspace(0, 1, len(x_hist)))\n        for i in range(len(x_hist)):\n            marker = 'o' if i == 0 else ('*' if i == len(x_hist)-1 else 'o')\n            size = 200 if i == 0 or i == len(x_hist)-1 else 100\n            ax1.scatter(x_hist[i], f_hist[i], c=[colors[i]], marker=marker, \n                       s=size, edgecolors='black', linewidths=1.5, zorder=3)\n        \n        # Add arrows\n        for i in range(len(x_hist) - 1):\n            ax1.annotate('', xy=(x_hist[i+1], f_hist[i+1]), \n                        xytext=(x_hist[i], f_hist[i]),\n                        arrowprops=dict(arrowstyle='->', color='red', lw=1.5, alpha=0.5))\n        \n        ax1.set_xlabel('x', fontsize=11)\n        ax1.set_ylabel('f(x)', fontsize=11)\n        ax1.set_title('Gradient Descent Path', fontsize=12, fontweight='bold')\n        ax1.grid(True, alpha=0.3)\n        \n        # Right: Loss over iterations\n        ax2.plot(range(len(f_hist)), f_hist, 'b-o', linewidth=2, markersize=6)\n        ax2.set_xlabel('Step', fontsize=11)\n        ax2.set_ylabel('f(x)', fontsize=11)\n        ax2.set_title('Function Value Over Time', fontsize=12, fontweight='bold')\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Summary stats\n        print(f\"\\nSteps taken: {len(gd_state['history']) - 1}\")\n        print(f\"Start: x = {x_hist[0]:.4f}, f(x) = {f_hist[0]:.4f}\")\n        print(f\"Current: x = {x_hist[-1]:.4f}, f(x) = {f_hist[-1]:.4f}\")\n        print(f\"Improvement: {f_hist[0] - f_hist[-1]:.4f}\")\n        print(f\"\\nTrue minimum: x ≈ {b_param:.4f}, f(x) ≈ {c_param:.4f}\")\n        print(f\"Distance from minimum: {abs(x_hist[-1] - b_param):.4f}\")\n        \n        # Table of recent steps\n        print(\"\\n\" + \"=\"*70)\n        print(\"Recent Steps:\")\n        df = pd.DataFrame(gd_state['history'][-10:])\n        df_display = df[['step', 'x', 'f(x)', 'gradient', 'change']].copy()\n        for col in ['x', 'f(x)', 'gradient', 'change']:\n            df_display[col] = df_display[col].apply(lambda v: f\"{v:.4f}\")\n        display(df_display)\n\ndef on_step_click(b):\n    run_gd_steps(1)\n    update_display()\n\ndef on_multi_step_click(b):\n    run_gd_steps(10)\n    update_display()\n\ndef on_reset_click(b):\n    gd_state['history'] = []\n    gd_state['running'] = False\n    with output_area:\n        output_area.clear_output()\n        print(\"Reset! Adjust starting point and learning rate, then click 'Run 1 Step'.\")\n\nstep_button.on_click(on_step_click)\nmulti_step_button.on_click(on_multi_step_click)\nreset_button.on_click(on_reset_click)\n\nprint(\"Interactive Gradient Descent\")\nprint(\"=\"*70)\nprint(\"1. Set starting point and learning rate\")\nprint(\"2. Click 'Run 1 Step' to see one GD update\")\nprint(\"3. Click 'Run 10 Steps' to see multiple updates\")\nprint(\"4. Try different learning rates (e.g., 0.01, 0.05, 0.4, 1.0, 3.0)\")\nprint(\"5. Click 'Reset' to start over\\n\")\n\ndisplay(VBox([\n    HBox([x0_input, lr_input]),\n    HBox([step_button, multi_step_button, reset_button]),\n    output_area\n]))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Comparison: Run GD with Five Learning Rates\n\nLet's systematically compare five learning rates starting from the same point."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare five learning rates\nx_start_compare = 0.0\nlearning_rates = [0.01, 0.05, 0.4, 1.0, 3.0]\nmax_steps = 30\n\nresults = {}\n\nfor lr in learning_rates:\n    history_x = [x_start_compare]\n    history_f = [hidden_func(x_start_compare)]\n    \n    x_current = x_start_compare\n    for step in range(max_steps):\n        grad = compute_gradient(x_current, hidden_func)\n        x_current = gd_step(x_current, grad, lr)\n        \n        history_x.append(x_current)\n        history_f.append(hidden_func(x_current))\n    \n    results[lr] = {'x': history_x, 'f': history_f}\n\n# Plot comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n\ncolors_comparison = ['purple', 'blue', 'green', 'orange', 'red']\nlabels = [f'LR = {lr}' for lr in learning_rates]\n\n# Left: Paths in (x, f(x)) space\nfor i, lr in enumerate(learning_rates):\n    ax1.plot(results[lr]['x'], results[lr]['f'], 'o-', \n            color=colors_comparison[i], label=labels[i], alpha=0.7, linewidth=2)\n\nax1.axhline(y=c_param, color='black', linestyle='--', alpha=0.3, label='True minimum')\nax1.set_xlabel('x', fontsize=11)\nax1.set_ylabel('f(x)', fontsize=11)\nax1.set_title('GD Paths for Different Learning Rates', fontsize=12, fontweight='bold')\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\n# Right: Loss over iterations\nfor i, lr in enumerate(learning_rates):\n    ax2.plot(range(len(results[lr]['f'])), results[lr]['f'], 'o-',\n            color=colors_comparison[i], label=labels[i], alpha=0.7, linewidth=2)\n\nax2.axhline(y=c_param, color='black', linestyle='--', alpha=0.3, label='True minimum')\nax2.set_xlabel('Step', fontsize=11)\nax2.set_ylabel('f(x)', fontsize=11)\nax2.set_title('Convergence Comparison', fontsize=12, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary table\nprint(\"\\nComparison Summary:\")\nprint(\"=\"*70)\nsummary_data = []\nfor lr in learning_rates:\n    final_x = results[lr]['x'][-1]\n    final_f = results[lr]['f'][-1]\n    error_from_min = abs(final_x - b_param)\n    summary_data.append({\n        'Learning Rate': lr,\n        'Final x': f\"{final_x:.4f}\",\n        'Final f(x)': f\"{final_f:.4f}\",\n        'Error from true min': f\"{error_from_min:.4f}\"\n    })\n\ndisplay(pd.DataFrame(summary_data))\n\nprint(f\"\\nTrue minimum: x = {b_param:.4f}, f(x) = {c_param:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Questions for Your Answer Sheet\n\n**Q4.** Compare gradient descent to your manual search in Lab 1 Module 4:\n- Which was faster at finding the minimum?\n- Which required more \"attempts\" or \"steps\"?\n- What advantage does GD have over manual guessing?\n\n**Q5.** Based on the visualizations above:\n- How does the step size relate to: (a) the slope (gradient) magnitude, and (b) the learning rate?\n- Why do the steps get smaller as you approach the minimum?\n- What happens with LR = 1.0? Does it converge smoothly?\n- What happens with LR = 3.0? Can you explain this behavior?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Reveal the Full Function\n",
    "\n",
    "In Lab 1, you never saw the underlying function. Now let's reveal it and see how GD navigated it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual function with GD paths overlaid\n",
    "x_plot = np.linspace(-10, 10, 300)\n",
    "f_plot = hidden_func(x_plot)\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "plt.plot(x_plot, f_plot, 'b-', linewidth=2, alpha=0.3, label='Hidden function (revealed!)')\n",
    "\n",
    "# Overlay GD paths\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    plt.plot(results[lr]['x'], results[lr]['f'], 'o-', \n",
    "            color=colors_comparison[i], label=f'GD path (LR={lr})', \n",
    "            alpha=0.8, linewidth=2, markersize=5)\n",
    "\n",
    "# Mark true minimum\n",
    "plt.scatter([b_param], [c_param], c='red', marker='*', s=500, \n",
    "           edgecolors='black', linewidths=2, zorder=10, label='True minimum')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Revealed: The Hidden Parabola with GD Paths', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-10, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This is the function you searched manually in Lab 1!\")\n",
    "print(f\"Formula: f(x) = {a:.3f} × (x - {b_param:.3f})² + {c_param:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q3, Q4, Q5** on your answer sheet\n",
    "2. **Return to the LMS** and continue to Module 2\n",
    "3. In Module 2, you'll apply GD to 2D parameter space (line fitting from Lab 1)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}