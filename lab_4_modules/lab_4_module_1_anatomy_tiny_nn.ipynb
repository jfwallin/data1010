{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Module 1: Anatomy of a Tiny Neural Network\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the architecture of a simple 2-2-1 neural network\n",
    "- Count and identify all network parameters\n",
    "- See how hidden neurons transform data into a new representation\n",
    "- **Discover that XOR becomes linearly separable in hidden space**\n",
    "- Manually tune parameters to solve XOR (appreciate why automatic training is needed!)\n",
    "\n",
    "**Time:** ~15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**Remember from Module 0:** You manually added a third dimension (x‚ÇÉ = x‚ÇÅ √ó x‚ÇÇ) that made XOR separable. A flat plane in 3D became a curved boundary in 2D.\n",
    "\n",
    "**Today's Big Idea:** Hidden layers do this automatically! Each hidden neuron creates a new \"dimension\" - and the network learns which transformations help solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network Architecture: The 2-2-1 Network\n",
    "\n",
    "We'll build a tiny neural network with:\n",
    "- **Input layer:** 2 neurons (x‚ÇÅ, x‚ÇÇ)\n",
    "- **Hidden layer:** 2 neurons (h‚ÇÅ, h‚ÇÇ) with sigmoid activation\n",
    "- **Output layer:** 1 neuron (output) with sigmoid activation\n",
    "\n",
    "```\n",
    "    x‚ÇÅ ‚îÄ‚îÄ‚îê\n",
    "         ‚îú‚îÄ‚Üí h‚ÇÅ ‚îÄ‚îê\n",
    "    x‚ÇÇ ‚îÄ‚îÄ‚î§        ‚îú‚îÄ‚Üí output\n",
    "         ‚îú‚îÄ‚Üí h‚ÇÇ ‚îÄ‚îò\n",
    "    x‚ÇÅ ‚îÄ‚îÄ‚îò\n",
    "    x‚ÇÇ\n",
    "```\n",
    "\n",
    "### Connection to What You Know:\n",
    "\n",
    "**From Lab 3:** Each hidden neuron (h‚ÇÅ and h‚ÇÇ) is a **perceptron** with sigmoid activation!\n",
    "- h‚ÇÅ has its own weights and bias: w‚ÇÅ‚ÇÅ, w‚ÇÅ‚ÇÇ, b‚ÇÅ\n",
    "- h‚ÇÇ has its own weights and bias: w‚ÇÇ‚ÇÅ, w‚ÇÇ‚ÇÇ, b‚ÇÇ\n",
    "\n",
    "**From Module 0:** Instead of manually adding x‚ÇÉ = x‚ÇÅ √ó x‚ÇÇ, the network creates h‚ÇÅ and h‚ÇÇ automatically!\n",
    "- h‚ÇÅ and h‚ÇÇ are like new \"features\" computed from x‚ÇÅ and x‚ÇÇ\n",
    "- The output layer makes decisions in this new (h‚ÇÅ, h‚ÇÇ) space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Load XOR Data and Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport ipywidgets as widgets\nfrom ipywidgets import interact, FloatSlider, Button, VBox, HBox, HTML, Output, Layout, Dropdown\nfrom IPython.display import display, clear_output\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Function to create different XOR datasets\ndef create_xor_dataset(dataset_type='perfect'):\n    \"\"\"\n    Create different versions of XOR data.\n    \n    Args:\n        dataset_type: 'perfect' (4 perfect points), 'clean' (tight clusters), \n                      or 'noisy' (Gaussian clouds)\n    \"\"\"\n    if dataset_type == 'perfect':\n        # Perfect XOR - just 4 points\n        X = np.array([\n            [-1.5, -1.5],  # Class 0\n            [1.5, 1.5],    # Class 0\n            [-1.5, 1.5],   # Class 1\n            [1.5, -1.5]    # Class 1\n        ])\n        y = np.array([0, 0, 1, 1])\n        \n        # Replicate points to make visualization clearer\n        X = np.repeat(X, 25, axis=0)\n        y = np.repeat(y, 25)\n        # Add tiny noise for visual variety\n        X = X + np.random.randn(len(X), 2) * 0.05\n        \n    elif dataset_type == 'clean':\n        # Clean XOR - tight clusters with minimal noise\n        n_per_corner = 25\n        corners_class0 = [[-1.5, -1.5], [1.5, 1.5]]\n        corners_class1 = [[-1.5, 1.5], [1.5, -1.5]]\n        \n        X_class0 = []\n        X_class1 = []\n        \n        for corner in corners_class0:\n            points = np.random.randn(n_per_corner, 2) * 0.1 + corner  # Very tight\n            X_class0.append(points)\n        \n        for corner in corners_class1:\n            points = np.random.randn(n_per_corner, 2) * 0.1 + corner  # Very tight\n            X_class1.append(points)\n        \n        X_class0 = np.vstack(X_class0)\n        X_class1 = np.vstack(X_class1)\n        \n        X = np.vstack([X_class0, X_class1])\n        y = np.hstack([np.zeros(len(X_class0)), np.ones(len(X_class1))])\n        \n    else:  # 'noisy'\n        # Noisy XOR - Gaussian clouds (original)\n        n_per_corner = 25\n        corners_class0 = [[-1.5, -1.5], [1.5, 1.5]]\n        corners_class1 = [[-1.5, 1.5], [1.5, -1.5]]\n        \n        X_class0 = []\n        X_class1 = []\n        \n        for corner in corners_class0:\n            points = np.random.randn(n_per_corner, 2) * 0.3 + corner  # Noisy\n            X_class0.append(points)\n        \n        for corner in corners_class1:\n            points = np.random.randn(n_per_corner, 2) * 0.3 + corner  # Noisy\n            X_class1.append(points)\n        \n        X_class0 = np.vstack(X_class0)\n        X_class1 = np.vstack(X_class1)\n        \n        X = np.vstack([X_class0, X_class1])\n        y = np.hstack([np.zeros(len(X_class0)), np.ones(len(X_class1))])\n    \n    return X, y\n\n# Create initial dataset (start with clean version)\nX_xor, y_xor = create_xor_dataset('clean')\ncurrent_dataset_type = 'clean'\n\nprint(\"‚úì XOR dataset created!\")\nprint(f\"  Total points: {len(X_xor)}\")\nprint(f\"  Class 0 (blue): {np.sum(y_xor==0)} points\")\nprint(f\"  Class 1 (red): {np.sum(y_xor==1)} points\")\nprint(\"\\nYou can switch between dataset types in the interactive section below!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "# Define the 2-2-1 network\n",
    "class TinyNetwork:\n",
    "    \"\"\"A tiny 2-2-1 neural network for XOR.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Hidden layer 1 parameters\n",
    "        self.w11 = 0.0\n",
    "        self.w12 = 0.0\n",
    "        self.b1 = 0.0\n",
    "        \n",
    "        # Hidden layer 2 parameters\n",
    "        self.w21 = 0.0\n",
    "        self.w22 = 0.0\n",
    "        self.b2 = 0.0\n",
    "        \n",
    "        # Output layer parameters\n",
    "        self.w_out1 = 0.0\n",
    "        self.w_out2 = 0.0\n",
    "        self.b_out = 0.0\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \n",
    "        Returns:\n",
    "            output: final prediction (0 to 1)\n",
    "            h1: hidden neuron 1 activation (0 to 1)\n",
    "            h2: hidden neuron 2 activation (0 to 1)\n",
    "        \"\"\"\n",
    "        # Hidden layer activations\n",
    "        z1 = self.w11 * x1 + self.w12 * x2 + self.b1\n",
    "        h1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = self.w21 * x1 + self.w22 * x2 + self.b2\n",
    "        h2 = sigmoid(z2)\n",
    "        \n",
    "        # Output layer activation\n",
    "        z_out = self.w_out1 * h1 + self.w_out2 * h2 + self.b_out\n",
    "        output = sigmoid(z_out)\n",
    "        \n",
    "        return output, h1, h2\n",
    "    \n",
    "    def predict_batch(self, X):\n",
    "        \"\"\"Make predictions for a batch of inputs.\"\"\"\n",
    "        predictions = []\n",
    "        h1_vals = []\n",
    "        h2_vals = []\n",
    "        \n",
    "        for x in X:\n",
    "            out, h1, h2 = self.forward(x[0], x[1])\n",
    "            predictions.append(out)\n",
    "            h1_vals.append(h1)\n",
    "            h2_vals.append(h2)\n",
    "        \n",
    "        return np.array(predictions), np.array(h1_vals), np.array(h2_vals)\n",
    "\n",
    "# Create network instance\n",
    "network = TinyNetwork()\n",
    "\n",
    "print(\"\\n‚úì Network architecture defined!\")\n",
    "print(\"\\nNetwork has:\")\n",
    "print(\"  Input layer: 2 neurons (x‚ÇÅ, x‚ÇÇ)\")\n",
    "print(\"  Hidden layer: 2 neurons (h‚ÇÅ, h‚ÇÇ) with sigmoid activation\")\n",
    "print(\"  Output layer: 1 neuron with sigmoid activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Counting Exercise\n",
    "\n",
    "Before we start tuning, let's count how many parameters this network has!\n",
    "\n",
    "**Hidden Neuron 1 (h‚ÇÅ):**\n",
    "- w‚ÇÅ‚ÇÅ (weight from x‚ÇÅ to h‚ÇÅ)\n",
    "- w‚ÇÅ‚ÇÇ (weight from x‚ÇÇ to h‚ÇÅ)\n",
    "- b‚ÇÅ (bias for h‚ÇÅ)\n",
    "- **Total: 3 parameters**\n",
    "\n",
    "**Hidden Neuron 2 (h‚ÇÇ):**\n",
    "- w‚ÇÇ‚ÇÅ (weight from x‚ÇÅ to h‚ÇÇ)\n",
    "- w‚ÇÇ‚ÇÇ (weight from x‚ÇÇ to h‚ÇÇ)\n",
    "- b‚ÇÇ (bias for h‚ÇÇ)\n",
    "- **Total: 3 parameters**\n",
    "\n",
    "**Output Neuron:**\n",
    "- w_out1 (weight from h‚ÇÅ to output)\n",
    "- w_out2 (weight from h‚ÇÇ to output)\n",
    "- b_out (bias for output)\n",
    "- **Total: 3 parameters**\n",
    "\n",
    "**Grand Total: 9 parameters**\n",
    "\n",
    "You're about to manually tune all 9 of these to solve XOR. This will help you appreciate why automatic training is so valuable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Interactive Network Builder: Solve XOR by Hand!\n\n### üéØ Your Goal\n\n**IMPORTANT:** A 2-2-1 network **cannot** create a clean nonlinear boundary in the original (x‚ÇÅ, x‚ÇÇ) space with only 2 hidden neurons. That would require more neurons!\n\nInstead, your network solves XOR by:\n1. **Transforming** the data from (x‚ÇÅ, x‚ÇÇ) space into a new **(h‚ÇÅ, h‚ÇÇ) hidden space**\n2. **Separating** the transformed data with a simple straight line in hidden space\n3. This simple separation in hidden space **looks complex** when projected back to input space\n\n### What Success Looks Like:\n\nAdjust the 9 sliders until you see:\n- **Bottom panels:** Each hidden neuron (H1, H2) creates a meaningful split\n- **Top-right panel (MOST IMPORTANT):** XOR becomes linearly separable in (h‚ÇÅ, h‚ÇÇ) space! ‚≠ê\n- **Top-left panel:** The resulting boundary in input space (complex is normal and expected!)\n\n### Understanding Dataset Types:\n\n**üìä You can choose from three datasets:**\n\n1. **Clean XOR (Recommended Start):** Tight clusters with minimal noise\n   - **Goal:** ~99-100% accuracy achievable\n   - Perfect for learning the concept!\n   \n2. **Noisy XOR (Realistic):** Gaussian clouds with overlap\n   - **Goal:** ~85% accuracy maximum\n   - Shows how real data behaves - perfect separation is impossible!\n   - The hidden space will show approximate separation\n   \n3. **Perfect XOR:** Just 4 points at corners\n   - **Goal:** 100% accuracy\n   - Easiest to understand, but unrealistic\n\n### Target Pattern for Hidden Space:\n\nYou want the **top-right panel** to show something like this:\n\n```\n    h‚ÇÇ\n    ‚Üë\n  1 ‚îÇ  red     blue\n    ‚îÇ   x        .\n    ‚îÇ   x        .\n0.5 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (green line)\n    ‚îÇ   .        x\n    ‚îÇ   .        x\n  0 ‚îÇ blue      red\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí h‚ÇÅ\n      0  0.5    1\n```\n\nWhen XOR forms linearly separable clusters in (h‚ÇÅ, h‚ÇÇ) space, a simple straight line (green) can separate them!\n\n**üí° Key Insight:** With noisy data, the clusters may overlap slightly in hidden space - this is normal and realistic!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization function for four panels\ndef plot_network_state(network, X, y):\n    \"\"\"Plot the four-panel visualization of network state.\"\"\"\n    \n    # Get predictions and hidden activations\n    predictions, h1_vals, h2_vals = network.predict_batch(X)\n    accuracy = np.mean((predictions > 0.5).astype(int) == y) * 100\n    \n    # Create figure with constrained_layout to avoid tight_layout warnings\n    fig = plt.figure(figsize=(16, 14), dpi=90, constrained_layout=True)\n    gs = GridSpec(2, 2, figure=fig)\n    \n    # Create mesh for decision boundaries\n    x_min, x_max = -2.5, 2.5\n    y_min, y_max = -2.5, 2.5\n    h = 0.05\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    \n    # Get network outputs for mesh\n    Z_out, Z_h1, Z_h2 = network.predict_batch(mesh_points)\n    Z_out = Z_out.reshape(xx.shape)\n    Z_h1 = Z_h1.reshape(xx.shape)\n    Z_h2 = Z_h2.reshape(xx.shape)\n    \n    # --- TOP-LEFT: Input Space Decision Boundary ---\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.contourf(xx, yy, Z_out, levels=20, alpha=0.3, cmap='RdBu_r')\n    ax1.contour(xx, yy, Z_out, levels=[0.5], colors='green', linewidths=3)\n    ax1.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=80, alpha=0.7,\n               edgecolors='k', linewidths=1.5, label='Class 0')\n    ax1.scatter(X[y==1, 0], X[y==1, 1], c='red', s=80, alpha=0.7,\n               edgecolors='k', linewidths=1.5, label='Class 1')\n    ax1.set_xlim(x_min, x_max)\n    ax1.set_ylim(y_min, y_max)\n    ax1.set_xlabel('x‚ÇÅ', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('x‚ÇÇ', fontsize=12, fontweight='bold')\n    ax1.set_title('Input Space (x‚ÇÅ, x‚ÇÇ)\\nFinal Decision Boundary\\n(Complex is OK!)',\n                 fontsize=11, fontweight='bold')\n    ax1.legend(loc='upper right', fontsize=9)\n    ax1.grid(True, alpha=0.3)\n    ax1.set_aspect('equal')\n    \n    # --- TOP-RIGHT: Hidden Space (THE KEY PLOT!) ---\n    ax2 = fig.add_subplot(gs[0, 1])\n    # Plot transformed data points\n    ax2.scatter(h1_vals[y==0], h2_vals[y==0], c='blue', s=100, alpha=0.8,\n               edgecolors='k', linewidths=2, label='Class 0', zorder=3)\n    ax2.scatter(h1_vals[y==1], h2_vals[y==1], c='red', s=100, alpha=0.8,\n               edgecolors='k', linewidths=2, label='Class 1', zorder=3)\n    \n    # Draw output layer's decision boundary in hidden space\n    if abs(network.w_out2) > 0.01:  # Avoid division by zero\n        h1_line = np.linspace(-0.1, 1.1, 100)\n        # Decision boundary: w_out1*h1 + w_out2*h2 + b_out = 0\n        h2_line = -(network.w_out1 * h1_line + network.b_out) / network.w_out2\n        # Only plot where h2 is in reasonable range\n        valid_mask = (h2_line >= -0.1) & (h2_line <= 1.1)\n        ax2.plot(h1_line[valid_mask], h2_line[valid_mask], 'g-', linewidth=4,\n                label='Output Decision Line', zorder=2, alpha=0.8)\n    \n    ax2.set_xlim(-0.1, 1.1)\n    ax2.set_ylim(-0.1, 1.1)\n    ax2.set_xlabel('h‚ÇÅ (Hidden Neuron 1)', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('h‚ÇÇ (Hidden Neuron 2)', fontsize=12, fontweight='bold')\n    ax2.set_title('‚≠ê HIDDEN SPACE (h‚ÇÅ, h‚ÇÇ) ‚≠ê\\nYour Main Goal: Linear Separation Here!\\n(This is where the magic happens!)',\n                 fontsize=11, fontweight='bold', color='darkgreen')\n    ax2.legend(loc='upper right', fontsize=9)\n    ax2.grid(True, alpha=0.3)\n    ax2.set_aspect('equal')\n    # Add border to emphasize importance\n    for spine in ax2.spines.values():\n        spine.set_edgecolor('darkgreen')\n        spine.set_linewidth(3)\n    \n    # --- BOTTOM-LEFT: Hidden Neuron 1 Boundary ---\n    ax3 = fig.add_subplot(gs[1, 0])\n    ax3.contourf(xx, yy, Z_h1, levels=20, alpha=0.4, cmap='Purples')\n    ax3.contour(xx, yy, Z_h1, levels=[0.5], colors='purple', linewidths=3)\n    ax3.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=60, alpha=0.6,\n               edgecolors='k', linewidths=1)\n    ax3.scatter(X[y==1, 0], X[y==1, 1], c='red', s=60, alpha=0.6,\n               edgecolors='k', linewidths=1)\n    ax3.set_xlim(x_min, x_max)\n    ax3.set_ylim(y_min, y_max)\n    ax3.set_xlabel('x‚ÇÅ', fontsize=12, fontweight='bold')\n    ax3.set_ylabel('x‚ÇÇ', fontsize=12, fontweight='bold')\n    ax3.set_title('Hidden Neuron 1 (h‚ÇÅ)\\nWhat does H1 separate?',\n                 fontsize=11, fontweight='bold')\n    ax3.grid(True, alpha=0.3)\n    ax3.set_aspect('equal')\n    \n    # --- BOTTOM-RIGHT: Hidden Neuron 2 Boundary ---\n    ax4 = fig.add_subplot(gs[1, 1])\n    ax4.contourf(xx, yy, Z_h2, levels=20, alpha=0.4, cmap='Oranges')\n    ax4.contour(xx, yy, Z_h2, levels=[0.5], colors='orange', linewidths=3)\n    ax4.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=60, alpha=0.6,\n               edgecolors='k', linewidths=1)\n    ax4.scatter(X[y==1, 0], X[y==1, 1], c='red', s=60, alpha=0.6,\n               edgecolors='k', linewidths=1)\n    ax4.set_xlim(x_min, x_max)\n    ax4.set_ylim(y_min, y_max)\n    ax4.set_xlabel('x‚ÇÅ', fontsize=12, fontweight='bold')\n    ax4.set_ylabel('x‚ÇÇ', fontsize=12, fontweight='bold')\n    ax4.set_title('Hidden Neuron 2 (h‚ÇÇ)\\nWhat does H2 separate?',\n                 fontsize=11, fontweight='bold')\n    ax4.grid(True, alpha=0.3)\n    ax4.set_aspect('equal')\n    \n    plt.show()\n    \n    return accuracy\n\nprint(\"‚úì Visualization functions ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the interactive network tuning interface\n\n# Global variable to track current dataset\ncurrent_dataset_type = 'clean'\n\n# Accuracy display and guidance\naccuracy_html = HTML(value=\"<h3 style='text-align:center;'>Current Accuracy: ---%</h3>\")\nguidance_html = HTML(value=\"<div style='background:#e8f0fe; padding:15px; border-radius:8px; margin:10px 0;'><p style='margin:0;'>Adjust the sliders below to tune your network!</p></div>\")\n\n# Dataset selector\ndataset_dropdown = Dropdown(\n    options=[\n        ('Clean XOR (easy - near-perfect separation possible)', 'clean'),\n        ('Noisy XOR (realistic - approximate separation)', 'noisy'),\n        ('Perfect XOR (4 points - 100% possible)', 'perfect')\n    ],\n    value='clean',\n    description='Dataset:',\n    style={'description_width': '80px'},\n    layout=Layout(width='600px')\n)\n\n# Create sliders for all 9 parameters\nslider_layout = Layout(width='400px')\nslider_style = {'description_width': '120px'}\n\n# Hidden Neuron 1 sliders\nw11_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                         description='H1: w‚ÇÅ‚ÇÅ', layout=slider_layout, style=slider_style)\nw12_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                         description='H1: w‚ÇÅ‚ÇÇ', layout=slider_layout, style=slider_style)\nb1_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                        description='H1: b‚ÇÅ', layout=slider_layout, style=slider_style)\n\n# Hidden Neuron 2 sliders\nw21_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                         description='H2: w‚ÇÇ‚ÇÅ', layout=slider_layout, style=slider_style)\nw22_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                         description='H2: w‚ÇÇ‚ÇÇ', layout=slider_layout, style=slider_style)\nb2_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                        description='H2: b‚ÇÇ', layout=slider_layout, style=slider_style)\n\n# Output layer sliders\nw_out1_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                            description='Out: w_out1', layout=slider_layout, style=slider_style)\nw_out2_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                            description='Out: w_out2', layout=slider_layout, style=slider_style)\nb_out_slider = FloatSlider(value=0, min=-10, max=10, step=0.5,\n                           description='Out: b_out', layout=slider_layout, style=slider_style)\n\n# Buttons\nupdate_btn = Button(\n    description='üîÑ Update Network',\n    button_style='primary',\n    layout=Layout(width='220px', height='50px'),\n    tooltip='Click to see your network\\'s performance'\n)\n\nexample_btn = Button(\n    description='üìñ Load Example',\n    button_style='success',\n    layout=Layout(width='220px', height='50px'),\n    tooltip='Load a simple working solution'\n)\n\nperfect_solution_btn = Button(\n    description='üí° Perfect Solution',\n    button_style='danger',\n    layout=Layout(width='220px', height='50px'),\n    tooltip='Load the hard-to-find perfect XOR solution (try to find it yourself first!)'\n)\n\nreset_btn = Button(\n    description='‚Ü∫ Reset Parameters',\n    button_style='warning',\n    layout=Layout(width='220px', height='50px'),\n    tooltip='Reset all parameters to zero'\n)\n\nreset_sim_btn = Button(\n    description='üîÑ Reset Simulation',\n    button_style='',\n    layout=Layout(width='220px', height='50px'),\n    tooltip='Clear output and start fresh'\n)\n\n# Output area for plots\nplot_output = Output()\n\ndef update_network(btn):\n    \"\"\"Update network parameters and redraw visualization.\"\"\"\n    global X_xor, y_xor, current_dataset_type\n    \n    # Update network parameters from sliders\n    network.w11 = w11_slider.value\n    network.w12 = w12_slider.value\n    network.b1 = b1_slider.value\n    \n    network.w21 = w21_slider.value\n    network.w22 = w22_slider.value\n    network.b2 = b2_slider.value\n    \n    network.w_out1 = w_out1_slider.value\n    network.w_out2 = w_out2_slider.value\n    network.b_out = b_out_slider.value\n    \n    # Redraw plot\n    with plot_output:\n        clear_output(wait=True)\n        accuracy = plot_network_state(network, X_xor, y_xor)\n    \n    # Update accuracy display\n    accuracy_html.value = f\"<h3 style='text-align:center; color:#1967d2;'>Current Accuracy: {accuracy:.1f}%</h3>\"\n    \n    # Update guidance based on accuracy AND dataset type\n    if current_dataset_type == 'noisy':\n        # Noisy dataset - different expectations\n        if accuracy >= 85:\n            guidance_html.value = \"\"\"\n            <div style='background:#d4edda; border-left:5px solid #28a745; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#155724; margin-top:0;'>üéâ Excellent! You found a good solution!</h4>\n                <p style='color:#155724; margin:5px 0;'>\n                    <b>Look at the top-right panel (h‚ÇÅ, h‚ÇÇ):</b><br/>\n                    ‚Ä¢ The noisy XOR data has been transformed into hidden space<br/>\n                    ‚Ä¢ The clusters overlap slightly due to noise - perfect separation is impossible!<br/>\n                    ‚Ä¢ The green line finds the best approximate boundary<br/>\n                    ‚Ä¢ This is realistic - real data is always noisy!\n                </p>\n                <p style='color:#155724; margin:5px 0;'>\n                    üí° Try switching to \"Clean XOR\" or \"Perfect XOR\" dataset to see perfect separation!\n                </p>\n            </div>\n            \"\"\"\n        elif accuracy >= 70:\n            guidance_html.value = \"\"\"\n            <div style='background:#fff3cd; border-left:5px solid #ffc107; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#856404; margin-top:0;'>üìà Good progress!</h4>\n                <p style='color:#856404; margin:5px 0;'>\n                    <b>Note:</b> This noisy dataset cannot be perfectly separated!<br/>\n                    ‚Ä¢ Goal: ~85% accuracy (not 100%!)<br/>\n                    ‚Ä¢ The Gaussian noise creates overlapping regions<br/>\n                    ‚Ä¢ Focus on getting clean separation in the top-right hidden space panel\n                </p>\n            </div>\n            \"\"\"\n        else:\n            guidance_html.value = \"\"\"\n            <div style='background:#f8d7da; border-left:5px solid #dc3545; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#721c24; margin-top:0;'>üí° Strategy Guide</h4>\n                <p style='color:#721c24; margin:5px 0;'>\n                    <b>With noisy data, aim for ~85% accuracy (not 100%!)</b>\n                </p>\n                <p style='color:#721c24; margin:5px 0;'>\n                    Try exploring different parameter combinations, or switch to a cleaner dataset!\n                </p>\n            </div>\n            \"\"\"\n    else:\n        # Clean or perfect dataset - can achieve high accuracy\n        if accuracy >= 99:\n            guidance_html.value = \"\"\"\n            <div style='background:#d4edda; border-left:5px solid #28a745; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#155724; margin-top:0;'>üéâ AMAZING! You solved XOR!</h4>\n                <p style='color:#155724; margin:5px 0;'>\n                    <b>Look at the top-right panel (h‚ÇÅ, h‚ÇÇ):</b><br/>\n                    ‚Ä¢ XOR has been transformed into two linearly separable clusters<br/>\n                    ‚Ä¢ The green line is a simple straight boundary in hidden space<br/>\n                    ‚Ä¢ This is exactly what hidden layers do - create new representations!\n                </p>\n                <p style='color:#155724; margin:5px 0;'>\n                    Try tweaking parameters slightly to see how robust your solution is!<br/>\n                    Or switch to \"Noisy XOR\" to see how real data behaves!\n                </p>\n            </div>\n            \"\"\"\n        elif accuracy >= 90:\n            guidance_html.value = \"\"\"\n            <div style='background:#fff3cd; border-left:5px solid #ffc107; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#856404; margin-top:0;'>üéØ Very Close!</h4>\n                <p style='color:#856404; margin:5px 0;'>\n                    <b>Strategy:</b><br/>\n                    ‚Ä¢ Your hidden neurons are working well!<br/>\n                    ‚Ä¢ Fine-tune the parameters to get that last bit of accuracy<br/>\n                    ‚Ä¢ Watch the green line in the top-right panel - it should cleanly separate the clusters\n                </p>\n            </div>\n            \"\"\"\n        elif accuracy >= 75:\n            guidance_html.value = \"\"\"\n            <div style='background:#cce5ff; border-left:5px solid #004085; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#004085; margin-top:0;'>üìà Good Progress!</h4>\n                <p style='color:#004085; margin:5px 0;'>\n                    <b>Strategy:</b><br/>\n                    ‚Ä¢ Check the bottom panels: Are H1 and H2 creating useful splits?<br/>\n                    ‚Ä¢ Experiment with different parameter combinations<br/>\n                    ‚Ä¢ Try making the weights larger or smaller<br/>\n                    ‚Ä¢ Perfect separation IS possible - keep exploring!\n                </p>\n            </div>\n            \"\"\"\n        else:\n            guidance_html.value = \"\"\"\n            <div style='background:#f8d7da; border-left:5px solid #dc3545; padding:15px; border-radius:8px; margin:10px 0;'>\n                <h4 style='color:#721c24; margin-top:0;'>üí° Keep Exploring!</h4>\n                <p style='color:#721c24; margin:5px 0;'>\n                    XOR is a challenging problem to solve manually with 9 parameters.<br/>\n                    ‚Ä¢ Try different combinations of weights and biases<br/>\n                    ‚Ä¢ Watch how the hidden space (top-right panel) changes<br/>\n                    ‚Ä¢ Perfect separation IS achievable, but hard to find!<br/>\n                    ‚Ä¢ This is why automatic training is so valuable!\n                </p>\n                <p style='color:#721c24; margin:5px 0; font-style:italic;'>\n                    Feeling stuck? Click \"Load Example\" for a simple (non-perfect) solution,<br/>\n                    or \"Perfect Solution\" to see the hard-to-find configuration that achieves 100%!\n                </p>\n            </div>\n            \"\"\"\n\ndef change_dataset(change):\n    \"\"\"Handle dataset change from dropdown.\"\"\"\n    global X_xor, y_xor, current_dataset_type\n    \n    current_dataset_type = change['new']\n    X_xor, y_xor = create_xor_dataset(current_dataset_type)\n    \n    # Update network visualization\n    update_network(None)\n    \n    # Show info about the dataset\n    if current_dataset_type == 'perfect':\n        info_msg = \"‚úì Switched to Perfect XOR (4 points) - 100% accuracy is achievable!\"\n    elif current_dataset_type == 'clean':\n        info_msg = \"‚úì Switched to Clean XOR (tight clusters) - near-perfect separation possible!\"\n    else:\n        info_msg = \"‚úì Switched to Noisy XOR (Gaussian clouds) - expect ~85% accuracy max!\"\n    \n    print(info_msg)\n\ndef load_example(btn):\n    \"\"\"Load simple example solution (not perfect, but understandable).\"\"\"\n    # Set sliders to simple, interpretable values\n    w11_slider.value = 5\n    w12_slider.value = 0\n    b1_slider.value = 0\n    \n    w21_slider.value = 0\n    w22_slider.value = 5\n    b2_slider.value = 0\n    \n    w_out1_slider.value = 5\n    w_out2_slider.value = 5\n    b_out_slider.value = -7\n    \n    # Update network\n    update_network(None)\n    \n    # Show explanation\n    guidance_html.value = \"\"\"\n    <div style='background:#e7f3ff; border-left:5px solid #2196F3; padding:15px; border-radius:8px; margin:10px 0;'>\n        <h4 style='color:#0d47a1; margin-top:0;'>üìñ Simple Example Solution Loaded!</h4>\n        <p style='color:#0d47a1; margin:5px 0;'>\n            <b>Hidden Neuron 1 (H1):</b> w‚ÇÅ‚ÇÅ=5, w‚ÇÅ‚ÇÇ=0, b‚ÇÅ=0<br/>\n            ‚Ä¢ Creates vertical boundary at x‚ÇÅ=0<br/>\n            ‚Ä¢ Separates left points from right points\n        </p>\n        <p style='color:#0d47a1; margin:5px 0;'>\n            <b>Hidden Neuron 2 (H2):</b> w‚ÇÇ‚ÇÅ=0, w‚ÇÇ‚ÇÇ=5, b‚ÇÇ=0<br/>\n            ‚Ä¢ Creates horizontal boundary at x‚ÇÇ=0<br/>\n            ‚Ä¢ Separates bottom points from top points\n        </p>\n        <p style='color:#0d47a1; margin:5px 0;'>\n            <b>Output Layer:</b> w_out1=5, w_out2=5, b_out=-7<br/>\n            ‚Ä¢ Attempts to combine h‚ÇÅ and h‚ÇÇ logically\n        </p>\n        <p style='color:#0d47a1; margin:5px 0; font-weight:bold;'>\n            This is an INTUITIVE solution, but it doesn't achieve 100% on XOR!<br/>\n            It shows a simple, understandable approach.\n        </p>\n        <p style='color:#0d47a1; margin:5px 0; font-style:italic;'>\n            A perfect XOR solution exists, but uses non-obvious parameters.<br/>\n            Click \"Perfect Solution\" to see it (after exploring on your own!).\n        </p>\n    </div>\n    \"\"\"\n\ndef load_perfect_solution(btn):\n    \"\"\"Load the hard-to-find perfect XOR solution.\"\"\"\n    # Show warning first\n    guidance_html.value = \"\"\"\n    <div style='background:#fff3cd; border-left:5px solid #ffc107; padding:15px; border-radius:8px; margin:10px 0;'>\n        <h4 style='color:#856404; margin-top:0;'>‚ö†Ô∏è Are you sure?</h4>\n        <p style='color:#856404; margin:5px 0;'>\n            This will load a perfect XOR solution that achieves 100% accuracy.<br/>\n            <b>It's MUCH more valuable to struggle and explore first!</b>\n        </p>\n        <p style='color:#856404; margin:5px 0;'>\n            The struggle helps you understand:<br/>\n            ‚Ä¢ How hard it is to find good parameters manually<br/>\n            ‚Ä¢ Why the parameter space is so complex<br/>\n            ‚Ä¢ Why we need automatic training (Module 2!)\n        </p>\n        <p style='color:#856404; margin:5px 0; font-weight:bold;'>\n            Click \"Perfect Solution\" again to confirm you want to see it.\n        </p>\n    </div>\n    \"\"\"\n    \n    # Change button behavior to actually load on second click\n    if perfect_solution_btn.description == 'üí° Perfect Solution':\n        perfect_solution_btn.description = '‚ö†Ô∏è Confirm Load'\n        perfect_solution_btn.button_style = 'warning'\n    else:\n        # Actually load the perfect solution\n        w11_slider.value = -10\n        w12_slider.value = -10\n        b1_slider.value = -10\n        \n        w21_slider.value = -10\n        w22_slider.value = -10\n        b2_slider.value = 5\n        \n        w_out1_slider.value = -10\n        w_out2_slider.value = 10\n        b_out_slider.value = -5\n        \n        # Update network\n        update_network(None)\n        \n        # Reset button\n        perfect_solution_btn.description = 'üí° Perfect Solution'\n        perfect_solution_btn.button_style = 'danger'\n        \n        # Show explanation\n        guidance_html.value = \"\"\"\n        <div style='background:#f3e5f5; border-left:5px solid #9c27b0; padding:15px; border-radius:8px; margin:10px 0;'>\n            <h4 style='color:#6a1b9a; margin-top:0;'>üí° Perfect XOR Solution Loaded!</h4>\n            <p style='color:#6a1b9a; margin:5px 0;'>\n                <b>This solution achieves 100% accuracy on perfect XOR!</b>\n            </p>\n            <p style='color:#6a1b9a; margin:5px 0;'>\n                <b>Hidden Neuron 1 (H1):</b> w‚ÇÅ‚ÇÅ=-10, w‚ÇÅ‚ÇÇ=-10, b‚ÇÅ=-10<br/>\n                ‚Ä¢ Detects when BOTH x‚ÇÅ AND x‚ÇÇ are strongly negative<br/>\n                ‚Ä¢ Activates high (‚âà1) only for bottom-left corner\n            </p>\n            <p style='color:#6a1b9a; margin:5px 0;'>\n                <b>Hidden Neuron 2 (H2):</b> w‚ÇÇ‚ÇÅ=-10, w‚ÇÇ‚ÇÇ=-10, b‚ÇÇ=+5<br/>\n                ‚Ä¢ Similar to H1 but with positive bias<br/>\n                ‚Ä¢ Creates different activation pattern\n            </p>\n            <p style='color:#6a1b9a; margin:5px 0;'>\n                <b>Output Layer:</b> w_out1=-10, w_out2=+10, b_out=-5<br/>\n                ‚Ä¢ Uses NEGATIVE weight on H1, POSITIVE on H2<br/>\n                ‚Ä¢ This creates the XOR logic in hidden space!\n            </p>\n            <p style='color:#6a1b9a; margin:5px 0; font-weight:bold;'>\n                Look at the top-right panel: Perfect linear separation in hidden space!\n            </p>\n            <p style='color:#6a1b9a; margin:5px 0;'>\n                <b>Key Insight:</b> This solution is NON-OBVIOUS!<br/>\n                ‚Ä¢ Uses all negative weights in hidden layer<br/>\n                ‚Ä¢ Uses opposite signs in output layer<br/>\n                ‚Ä¢ You would never find this by intuition alone!<br/>\n                <br/>\n                <b>This is why we need automatic training with gradient descent!</b>\n            </p>\n        </div>\n        \"\"\"\n\ndef reset_parameters(btn):\n    \"\"\"Reset all parameters to zero.\"\"\"\n    w11_slider.value = 0\n    w12_slider.value = 0\n    b1_slider.value = 0\n    w21_slider.value = 0\n    w22_slider.value = 0\n    b2_slider.value = 0\n    w_out1_slider.value = 0\n    w_out2_slider.value = 0\n    b_out_slider.value = 0\n    \n    # Reset perfect solution button\n    perfect_solution_btn.description = 'üí° Perfect Solution'\n    perfect_solution_btn.button_style = 'danger'\n    \n    update_network(None)\n\ndef reset_simulation(btn):\n    \"\"\"Reset the entire simulation - clear output and parameters.\"\"\"\n    # Reset all sliders to zero\n    w11_slider.value = 0\n    w12_slider.value = 0\n    b1_slider.value = 0\n    w21_slider.value = 0\n    w22_slider.value = 0\n    b2_slider.value = 0\n    w_out1_slider.value = 0\n    w_out2_slider.value = 0\n    b_out_slider.value = 0\n    \n    # Reset network\n    network.w11 = 0\n    network.w12 = 0\n    network.b1 = 0\n    network.w21 = 0\n    network.w22 = 0\n    network.b2 = 0\n    network.w_out1 = 0\n    network.w_out2 = 0\n    network.b_out = 0\n    \n    # Reset perfect solution button\n    perfect_solution_btn.description = 'üí° Perfect Solution'\n    perfect_solution_btn.button_style = 'danger'\n    \n    # Clear output\n    with plot_output:\n        clear_output(wait=True)\n    \n    # Reset displays\n    accuracy_html.value = \"<h3 style='text-align:center;'>Current Accuracy: ---%</h3>\"\n    guidance_html.value = \"<div style='background:#e8f0fe; padding:15px; border-radius:8px; margin:10px 0;'><p style='margin:0;'>Adjust the sliders below to tune your network, then click 'Update Network'!</p></div>\"\n\n# Connect buttons to functions\nupdate_btn.on_click(update_network)\nexample_btn.on_click(load_example)\nperfect_solution_btn.on_click(load_perfect_solution)\nreset_btn.on_click(reset_parameters)\nreset_sim_btn.on_click(reset_simulation)\ndataset_dropdown.observe(change_dataset, names='value')\n\n# Layout the interface\nprint(\"\\n\" + \"=\"*80)\nprint(\"INTERACTIVE NETWORK BUILDER\")\nprint(\"=\"*80)\nprint(\"\\nInstructions:\")\nprint(\"1. Choose a dataset type from the dropdown\")\nprint(\"2. Adjust the 9 sliders below to tune your network\")\nprint(\"3. Click 'Update Network' to see the results\")\nprint(\"4. Focus on the TOP-RIGHT panel - that's where XOR should become separable!\")\nprint(\"5. Try to find a solution yourself before using the buttons!\")\nprint(\"6. 'Load Example' shows a simple (imperfect) solution\")\nprint(\"7. 'Perfect Solution' shows the hard-to-find 100% solution (try last!)\")\nprint(\"\\n\" + \"=\"*80)\n\ndisplay(HTML(\"<h4 style='margin-top:10px; color:#1967d2;'>üìä Choose Your Dataset:</h4>\"))\ndisplay(dataset_dropdown)\ndisplay(HTML(\"<p style='color:#5f6368; font-size:13px; margin:5px 0 15px 0;'><b>Tip:</b> Start with 'Clean XOR' to learn the concept!</p>\"))\n\ndisplay(accuracy_html)\ndisplay(guidance_html)\n\ndisplay(HTML(\"<h4 style='margin-top:20px; color:#1967d2;'>‚öôÔ∏è Hidden Neuron 1 Parameters:</h4>\"))\ndisplay(VBox([w11_slider, w12_slider, b1_slider]))\n\ndisplay(HTML(\"<h4 style='margin-top:20px; color:#1967d2;'>‚öôÔ∏è Hidden Neuron 2 Parameters:</h4>\"))\ndisplay(VBox([w21_slider, w22_slider, b2_slider]))\n\ndisplay(HTML(\"<h4 style='margin-top:20px; color:#1967d2;'>‚öôÔ∏è Output Layer Parameters:</h4>\"))\ndisplay(VBox([w_out1_slider, w_out2_slider, b_out_slider]))\n\ndisplay(HTML(\"<div style='margin:20px 0;'></div>\"))\n# Three rows of buttons\ndisplay(HBox([update_btn, example_btn]))\ndisplay(HTML(\"<div style='margin:10px 0;'></div>\"))\ndisplay(HBox([perfect_solution_btn]))\ndisplay(HTML(\"<div style='margin:10px 0;'></div>\"))\ndisplay(HBox([reset_btn, reset_sim_btn]))\ndisplay(HTML(\"<div style='margin:20px 0;'></div>\"))\n\ndisplay(plot_output)\n\n# Show initial state\nupdate_network(None)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding What Happened\n",
    "\n",
    "### What Did You Just Do?\n",
    "\n",
    "**The Manual Approach (You):**\n",
    "1. Adjusted 9 sliders by hand\n",
    "2. Tried to make hidden space linearly separable\n",
    "3. Saw how two perceptrons can combine to solve XOR\n",
    "\n",
    "**Why This Was Hard:**\n",
    "- 9-dimensional parameter space to search\n",
    "- Non-obvious which direction to adjust\n",
    "- Trial and error with only visual feedback\n",
    "- Imagine doing this with 1000 parameters... or 1 million!\n",
    "\n",
    "### The Key Insight: Hidden Space Transformation\n",
    "\n",
    "**From Module 0, you learned:**\n",
    "- Manually adding x‚ÇÉ = x‚ÇÅ √ó x‚ÇÇ made XOR separable\n",
    "- A flat plane in 3D became a curved boundary in 2D\n",
    "\n",
    "**Now in Module 1, you discovered:**\n",
    "- Hidden neurons create h‚ÇÅ and h‚ÇÇ automatically (no manual feature engineering!)\n",
    "- The network transforms (x‚ÇÅ, x‚ÇÇ) ‚Üí (h‚ÇÅ, h‚ÇÇ)\n",
    "- **XOR becomes linearly separable in this new (h‚ÇÅ, h‚ÇÇ) space**\n",
    "- A simple straight line in hidden space = complex boundary in input space\n",
    "\n",
    "### Connection to Lab 3:\n",
    "\n",
    "Remember perceptrons from Lab 3? Each hidden neuron **IS** a perceptron!\n",
    "- H1 = perceptron with weights (w‚ÇÅ‚ÇÅ, w‚ÇÅ‚ÇÇ) and bias b‚ÇÅ\n",
    "- H2 = perceptron with weights (w‚ÇÇ‚ÇÅ, w‚ÇÇ‚ÇÇ) and bias b‚ÇÇ\n",
    "- Output = perceptron that takes (h‚ÇÅ, h‚ÇÇ) as input\n",
    "\n",
    "**Two perceptrons + one output perceptron = solves XOR!**\n",
    "\n",
    "This is why a single perceptron couldn't solve XOR in Lab 3, but a network can.\n",
    "\n",
    "### Coming Next (Module 2):\n",
    "\n",
    "**Automatic training does this for you!**\n",
    "- Gradient descent finds good parameters automatically\n",
    "- Scales to millions of parameters\n",
    "- You just saw **WHY** hidden layers work\n",
    "- Next you'll see **HOW** they learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Solution Explained\n",
    "\n",
    "If you loaded the example solution (or found your own!), let's understand what each part does:\n",
    "\n",
    "### Hidden Neuron 1 (H1):\n",
    "**Parameters:** w‚ÇÅ‚ÇÅ=5, w‚ÇÅ‚ÇÇ=0, b‚ÇÅ=0\n",
    "\n",
    "**What it computes:**\n",
    "```\n",
    "z‚ÇÅ = 5¬∑x‚ÇÅ + 0¬∑x‚ÇÇ + 0 = 5¬∑x‚ÇÅ\n",
    "h‚ÇÅ = sigmoid(5¬∑x‚ÇÅ)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Creates a **vertical boundary** at x‚ÇÅ = 0\n",
    "- When x‚ÇÅ < 0 (left side): h‚ÇÅ ‚âà 0\n",
    "- When x‚ÇÅ > 0 (right side): h‚ÇÅ ‚âà 1\n",
    "- **Effect:** Separates left points from right points\n",
    "\n",
    "### Hidden Neuron 2 (H2):\n",
    "**Parameters:** w‚ÇÇ‚ÇÅ=0, w‚ÇÇ‚ÇÇ=5, b‚ÇÇ=0\n",
    "\n",
    "**What it computes:**\n",
    "```\n",
    "z‚ÇÇ = 0¬∑x‚ÇÅ + 5¬∑x‚ÇÇ + 0 = 5¬∑x‚ÇÇ\n",
    "h‚ÇÇ = sigmoid(5¬∑x‚ÇÇ)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Creates a **horizontal boundary** at x‚ÇÇ = 0\n",
    "- When x‚ÇÇ < 0 (bottom): h‚ÇÇ ‚âà 0\n",
    "- When x‚ÇÇ > 0 (top): h‚ÇÇ ‚âà 1\n",
    "- **Effect:** Separates bottom points from top points\n",
    "\n",
    "### Output Neuron:\n",
    "**Parameters:** w_out1=5, w_out2=5, b_out=-7\n",
    "\n",
    "**What it computes:**\n",
    "```\n",
    "z_out = 5¬∑h‚ÇÅ + 5¬∑h‚ÇÇ - 7\n",
    "output = sigmoid(5¬∑h‚ÇÅ + 5¬∑h‚ÇÇ - 7)\n",
    "```\n",
    "\n",
    "**What it does (XOR logic):**\n",
    "- Bottom-left (x‚ÇÅ<0, x‚ÇÇ<0): h‚ÇÅ‚âà0, h‚ÇÇ‚âà0 ‚Üí z_out ‚âà -7 ‚Üí output‚âà0 ‚úì (Class 0)\n",
    "- Top-right (x‚ÇÅ>0, x‚ÇÇ>0): h‚ÇÅ‚âà1, h‚ÇÇ‚âà1 ‚Üí z_out ‚âà +3 ‚Üí output‚âà1... wait!\n",
    "\n",
    "Actually, let's recalculate more carefully:\n",
    "\n",
    "| Corner | x‚ÇÅ | x‚ÇÇ | h‚ÇÅ | h‚ÇÇ | z_out = 5h‚ÇÅ+5h‚ÇÇ-7 | output | True Label |\n",
    "|--------|----|----|----|----|-------------------|--------|------------|\n",
    "| Bottom-left | -1.5 | -1.5 | ~0 | ~0 | -7 | ~0 | 0 ‚úì |\n",
    "| Top-right | +1.5 | +1.5 | ~1 | ~1 | +3 | ~1 | 0 ‚úó |\n",
    "| Top-left | -1.5 | +1.5 | ~0 | ~1 | -2 | ~0.1 | 1 ‚úó |\n",
    "| Bottom-right | +1.5 | -1.5 | ~1 | ~0 | -2 | ~0.1 | 1 ‚úó |\n",
    "\n",
    "Hmm, that's not quite right! Let me give you a better example solution:\n",
    "\n",
    "### Better Example Solution:\n",
    "\n",
    "**H1:** w‚ÇÅ‚ÇÅ=5, w‚ÇÅ‚ÇÇ=5, b‚ÇÅ=-4 (creates diagonal boundary: x‚ÇÅ+x‚ÇÇ=0.8)\n",
    "**H2:** w‚ÇÇ‚ÇÅ=5, w‚ÇÇ‚ÇÇ=-5, b‚ÇÇ=0 (creates diagonal boundary: x‚ÇÅ-x‚ÇÇ=0)\n",
    "**Output:** w_out1=5, w_out2=5, b_out=-7\n",
    "\n",
    "The key insight is that **many solutions exist!** The network can discover different transformations of the hidden space that make XOR separable.\n",
    "\n",
    "### The Hidden Space Magic:\n",
    "\n",
    "No matter which solution you found, the pattern is the same:\n",
    "1. **Hidden neurons create new dimensions** (h‚ÇÅ, h‚ÇÇ)\n",
    "2. **XOR data transforms** from messy in (x‚ÇÅ, x‚ÇÇ) to separable in (h‚ÇÅ, h‚ÇÇ)\n",
    "3. **Output layer draws simple line** in hidden space\n",
    "4. **This maps back** to complex boundary in input space\n",
    "\n",
    "**This is the fundamental reason why neural networks work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways from Module 1\n",
    "\n",
    "### 1. Hidden Layers Create New Dimensions\n",
    "- Just like you manually added x‚ÇÉ=x‚ÇÅ√óx‚ÇÇ in Module 0\n",
    "- Hidden neurons create h‚ÇÅ, h‚ÇÇ automatically during training\n",
    "- These new dimensions make the problem solvable\n",
    "- **The transformation is learned, not hand-designed!**\n",
    "\n",
    "### 2. Separation Happens in Hidden Space\n",
    "- **Not in input space!** The input boundary will be complex\n",
    "- The key transformation is (x‚ÇÅ, x‚ÇÇ) ‚Üí (h‚ÇÅ, h‚ÇÇ)\n",
    "- Linear separation in hidden space = complex boundary in input space\n",
    "- **Focus on the hidden representation, not just the final output!**\n",
    "\n",
    "### 3. Each Hidden Neuron is a Perceptron (from Lab 3)\n",
    "- H1 and H2 are both perceptrons with sigmoid activation\n",
    "- Each creates one boundary/transformation\n",
    "- The output layer combines their outputs\n",
    "- **Two perceptrons together can solve what one cannot!**\n",
    "\n",
    "### 4. Manual Tuning Doesn't Scale\n",
    "- 9 parameters was already hard to tune by hand\n",
    "- Modern networks have millions or billions of parameters\n",
    "- Gradient descent does this automatically in Module 2\n",
    "- **Automatic training is not just convenient - it's essential!**\n",
    "\n",
    "### 5. Multiple Solutions Exist\n",
    "- There's no single \"correct\" solution to XOR\n",
    "- Different weight configurations can achieve high accuracy\n",
    "- The network can discover various transformations of hidden space\n",
    "- **This flexibility is a strength of neural networks!**\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Module 2, you'll see gradient descent automatically find these parameters through training. The network will learn the same hidden-space transformation you just discovered - but completely on its own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Your Answer Sheet\n",
    "\n",
    "**Q5.** How many total parameters does the 2-2-1 network have? Break down the count by layer (hidden layer 1, hidden layer 2, output layer).\n",
    "\n",
    "**Q6.** Describe what each hidden neuron (H1 and H2) separated in your solution. What patterns did they detect in the input space? (Refer to the bottom-left and bottom-right panels)\n",
    "\n",
    "**Q7.** Look at the hidden space plot (top-right panel with h‚ÇÅ and h‚ÇÇ axes). Explain how the XOR data was transformed in (h‚ÇÅ, h‚ÇÇ) space and why this transformation made it easier to separate the two classes. How does this relate to what you did manually in Module 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q5-Q7** on your answer sheet\n",
    "2. **Experiment** with different parameter values - try to find alternative solutions!\n",
    "3. **Return to the LMS** and continue to Module 2\n",
    "4. In Module 2, you'll see how gradient descent trains this same 2-2-1 network automatically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}