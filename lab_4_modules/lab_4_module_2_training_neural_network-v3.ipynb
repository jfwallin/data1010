{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Module 2: Training a Neural Network\n",
    "\n",
    "## Learning to Learn: Automatic Weight Optimization with Gradient Descent\n",
    "\n",
    "**Estimated time: 15-20 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Why Train Automatically? (2 min)\n",
    "\n",
    "In Module 1, you manually adjusted **9 sliders** to find weights that could separate XOR data:\n",
    "- 2 hidden neurons × (2 weights + 1 bias) = 6 parameters\n",
    "- 1 output neuron × (2 weights + 1 bias) = 3 parameters\n",
    "- **Total: 9 parameters to tune by hand!**\n",
    "\n",
    "You discovered that finding the right weights is HARD:\n",
    "- The perfect solution exists: `H1=(-10,-10,-10), H2=(-10,-10,5), Out=(-10,10,-5)`\n",
    "- But finding it manually required lots of trial and error\n",
    "- Most random combinations gave terrible results\n",
    "\n",
    "### The Problem Gets Worse Fast\n",
    "\n",
    "Imagine if we had:\n",
    "- **10 hidden neurons**: 31 parameters\n",
    "- **100 hidden neurons**: 301 parameters\n",
    "- **Modern deep networks**: millions of parameters!\n",
    "\n",
    "Manual tuning becomes impossible. We need an **automatic** way to find good weights.\n",
    "\n",
    "### Enter: Gradient Descent\n",
    "\n",
    "Think of the loss function as a landscape:\n",
    "- **High points** = bad weights (high error)\n",
    "- **Low points** = good weights (low error)\n",
    "- **Goal**: Roll downhill to find the lowest point\n",
    "\n",
    "Gradient descent is like **rolling a ball downhill** in this loss landscape:\n",
    "1. Start at a random location (random weights)\n",
    "2. Look around and find which direction is steepest downward (compute gradients)\n",
    "3. Take a small step in that direction (update weights)\n",
    "4. Repeat until you reach a valley bottom (converged!)\n",
    "\n",
    "**The key insight**: Even though the code looks complex (calculating derivatives, chain rule, etc.), it's just these 4 steps over and over. The math handles **9 weights simultaneously**, but the idea is simple: always move downhill!\n",
    "\n",
    "In this module, you'll **watch this process happen in real time**!\n",
    "\n",
    "### Making Training Reliable\n",
    "\n",
    "In this module, we'll see how gradient descent can automatically find good solutions - even for tricky problems like XOR. We'll use:\n",
    "- **Smart initialization**: Trying multiple random starting points and picking the best one\n",
    "- **Momentum**: A better gradient descent algorithm that helps escape local minima\n",
    "- **Flexible learning rates**: Adjusting the step size to balance speed and stability\n",
    "\n",
    "These techniques make training **robust and reliable** across different datasets!\n",
    "\n",
    "### This Module: Compare Algorithms and Datasets\n",
    "\n",
    "In this module, you'll:\n",
    "- **Try different datasets**: See how problem difficulty affects convergence\n",
    "- **Compare algorithms**: Basic gradient descent vs. gradient descent with momentum\n",
    "- **Experiment with learning rates**: Find the sweet spot between speed and stability\n",
    "\n",
    "You'll discover that **better algorithms converge more reliably** across different datasets!\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Setup the Training System (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from ipywidgets import Button, Output, VBox, HBox, Layout, HTML, Checkbox\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid with clipping to prevent overflow.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_xor_dataset(dataset_type='clean', n_per_cluster=25, noise_std=0.2, seed=42):\n    \"\"\"\n    Create XOR-style datasets with different difficulty levels.\n\n    Parameters:\n    - dataset_type: 'corner', 'corner_noisy', 'clean', 'noisy', or 'perfect'\n    - n_per_cluster: number of samples per cluster (default 25)\n    - noise_std: standard deviation of Gaussian noise (default 0.2)\n    - seed: random seed for reproducibility\n\n    Returns:\n    - X: features (n_samples, 2)\n    - y: labels (n_samples,)\n    \"\"\"\n    np.random.seed(seed)\n\n    if dataset_type == 'corner':\n        # One corner vs other three corners (easier - linearly separable)\n        corners = np.array([\n            [-1.0, -1.0],  # BL - Class 0\n            [-1.0, 1.0],   # TL - Class 1\n            [1.0, -1.0],   # BR - Class 1\n            [1.0, 1.0],    # TR - Class 1\n        ])\n        labels = np.array([0, 1, 1, 1])\n        X = np.repeat(corners, n_per_cluster, axis=0)\n        y = np.repeat(labels, n_per_cluster)\n        X = X + np.random.randn(len(X), 2) * noise_std\n\n    elif dataset_type == 'corner_noisy':\n        # One corner vs three with more noise\n        corners = np.array([\n            [-1.0, -1.0],  # BL - Class 0\n            [-1.0, 1.0],   # TL - Class 1\n            [1.0, -1.0],   # BR - Class 1\n            [1.0, 1.0],    # TR - Class 1\n        ])\n        labels = np.array([0, 1, 1, 1])\n        X = np.repeat(corners, n_per_cluster, axis=0)\n        y = np.repeat(labels, n_per_cluster)\n        X = X + np.random.randn(len(X), 2) * (noise_std * 1.5)  # More noise\n\n    elif dataset_type == 'clean':\n        # Standard XOR (moderate difficulty)\n        corners = np.array([\n            [-1.0, -1.0],  # BL - Class 0\n            [1.0, 1.0],    # TR - Class 0\n            [-1.0, 1.0],   # TL - Class 1\n            [1.0, -1.0],   # BR - Class 1\n        ])\n        labels = np.array([0, 0, 1, 1])\n        X = np.repeat(corners, n_per_cluster, axis=0)\n        y = np.repeat(labels, n_per_cluster)\n        X = X + np.random.randn(len(X), 2) * noise_std\n\n    elif dataset_type == 'noisy':\n        # XOR with more overlap (harder)\n        corners = np.array([\n            [-1.0, -1.0],  # BL - Class 0\n            [1.0, 1.0],    # TR - Class 0\n            [-1.0, 1.0],   # TL - Class 1\n            [1.0, -1.0],   # BR - Class 1\n        ])\n        labels = np.array([0, 0, 1, 1])\n        X = np.repeat(corners, n_per_cluster, axis=0)\n        y = np.repeat(labels, n_per_cluster)\n        X = X + np.random.randn(len(X), 2) * (noise_std * 2.0)  # Much more noise\n\n    elif dataset_type == 'perfect':\n        # Minimal noise XOR (easiest)\n        corners = np.array([\n            [-1.0, -1.0],  # BL - Class 0\n            [1.0, 1.0],    # TR - Class 0\n            [-1.0, 1.0],   # TL - Class 1\n            [1.0, -1.0],   # BR - Class 1\n        ])\n        labels = np.array([0, 0, 1, 1])\n        X = np.repeat(corners, n_per_cluster, axis=0)\n        y = np.repeat(labels, n_per_cluster)\n        X = X + np.random.randn(len(X), 2) * 0.05  # Minimal noise\n\n    else:\n        raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n\n    return X, y\n\n# Create initial dataset (will be updated by dropdown)\ncurrent_dataset_type = 'clean'\nX_train, y_train = create_xor_dataset(dataset_type=current_dataset_type)\n\nprint(f\"Dataset created: {len(X_train)} samples\")\nprint(f\"Class 0: {np.sum(y_train==0)} samples\")\nprint(f\"Class 1: {np.sum(y_train==1)} samples\")\nprint(f\"Current dataset: {current_dataset_type}\")\n\n# Visualize\nplt.figure(figsize=(6, 6))\nfor label in [0, 1]:\n    mask = y_train == label\n    color = 'blue' if label == 0 else 'red'\n    plt.scatter(X_train[mask, 0], X_train[mask, 1], c=color, s=30, alpha=0.6, edgecolors='k', linewidths=0.5)\nplt.xlabel('x\\u2081', fontsize=12)\nplt.ylabel('x\\u2082', fontsize=12)\nplt.title(f'Dataset: {current_dataset_type}', fontsize=12, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNetwork class ready!\n"
     ]
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "class TinyNetwork:\n",
    "    \"\"\"2-2-1 neural network for XOR classification.\"\"\"\n",
    "    \n",
    "    '''def __init__(self, weights=None):\n",
    "        if weights is None:\n",
    "            self.w11 = self.w12 = self.b1 = 0.0\n",
    "            self.w21 = self.w22 = self.b2 = 0.0\n",
    "            self.w_out1 = self.w_out2 = self.b_out = 0.0\n",
    "        else:\n",
    "            self.set_weights(weights)'''\n",
    "    \n",
    "    \n",
    "    def __init__(self, weights=None, rng=None, scale=0.5):\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "        if weights is None:\n",
    "            # Small random initialization to break symmetry\n",
    "            w = rng.normal(loc=0.0, scale=scale, size=9)\n",
    "            self.set_weights(w)\n",
    "        else:\n",
    "            self.set_weights(weights)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward pass for a single input.\"\"\"\n",
    "        z1 = self.w11 * x1 + self.w12 * x2 + self.b1\n",
    "        h1 = tanh(z1)\n",
    "        z2 = self.w21 * x1 + self.w22 * x2 + self.b2\n",
    "        h2 = tanh(z2)\n",
    "        z_out = self.w_out1 * h1 + self.w_out2 * h2 + self.b_out\n",
    "        output = sigmoid(z_out)\n",
    "        return output, h1, h2\n",
    "    \n",
    "    def predict_batch(self, X):\n",
    "        \"\"\"Forward pass for batch of inputs.\"\"\"\n",
    "        predictions = []\n",
    "        h1_vals = []\n",
    "        h2_vals = []\n",
    "        for x in X:\n",
    "            out, h1, h2 = self.forward(x[0], x[1])\n",
    "            predictions.append(out)\n",
    "            h1_vals.append(h1)\n",
    "            h2_vals.append(h2)\n",
    "        return np.array(predictions), np.array(h1_vals), np.array(h2_vals)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"Get all weights as a list.\"\"\"\n",
    "        return [self.w11, self.w12, self.b1, self.w21, self.w22,\n",
    "                self.b2, self.w_out1, self.w_out2, self.b_out]\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Set all weights from a list.\"\"\"\n",
    "        self.w11, self.w12, self.b1, self.w21, self.w22, self.b2, \\\n",
    "        self.w_out1, self.w_out2, self.b_out = weights\n",
    "\n",
    "print(\"TinyNetwork class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and accuracy functions ready!\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(network, X, y):\n",
    "    \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "    predictions, _, _ = network.predict_batch(X)\n",
    "    epsilon = 1e-10  # Prevent log(0)\n",
    "    bce = -np.mean(y * np.log(predictions + epsilon) +\n",
    "                   (1 - y) * np.log(1 - predictions + epsilon))\n",
    "    return bce\n",
    "\n",
    "def compute_accuracy(network, X, y):\n",
    "    \"\"\"Classification accuracy.\"\"\"\n",
    "    predictions, _, _ = network.predict_batch(X)\n",
    "    pred_labels = (predictions > 0.5).astype(int)\n",
    "    return np.mean(pred_labels == y)\n",
    "\n",
    "print(\"Loss and accuracy functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computation (backpropagation) ready!\n"
     ]
    }
   ],
   "source": [
    "def compute_gradients(network, X, y):\n",
    "    \"\"\"Analytical backpropagation for 2-2-1 network.\"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Initialize gradient accumulators\n",
    "    grads = {'w11': 0, 'w12': 0, 'b1': 0,\n",
    "             'w21': 0, 'w22': 0, 'b2': 0,\n",
    "             'w_out1': 0, 'w_out2': 0, 'b_out': 0}\n",
    "    \n",
    "    # Accumulate gradients over all samples\n",
    "    for i in range(n_samples):\n",
    "        x1, x2 = X[i]\n",
    "        target = y[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = network.w11 * x1 + network.w12 * x2 + network.b1\n",
    "        h1 = tanh(z1)\n",
    "        z2 = network.w21 * x1 + network.w22 * x2 + network.b2\n",
    "        h2 = tanh(z2)\n",
    "        z_out = network.w_out1 * h1 + network.w_out2 * h2 + network.b_out\n",
    "        y_pred = sigmoid(z_out)\n",
    "        \n",
    "        # Backpropagation\n",
    "        epsilon = 1e-10\n",
    "        dL_dy_pred = -(target / (y_pred + epsilon) - (1 - target) / (1 - y_pred + epsilon))\n",
    "        dy_pred_dz_out = y_pred * (1 - y_pred)\n",
    "        delta_out = dL_dy_pred * dy_pred_dz_out\n",
    "        \n",
    "        # Output layer gradients\n",
    "        grads['w_out1'] += delta_out * h1\n",
    "        grads['w_out2'] += delta_out * h2\n",
    "        grads['b_out'] += delta_out\n",
    "        \n",
    "        # Hidden layer deltas\n",
    "        #delta_h1 = delta_out * network.w_out1 * h1 * (1 - h1)\n",
    "        #delta_h2 = delta_out * network.w_out2 * h2 * (1 - h2)\n",
    "        delta_h1 = delta_out * network.w_out1 * (1 - h1**2)\n",
    "        delta_h2 = delta_out * network.w_out2 * (1 - h2**2)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        grads['w11'] += delta_h1 * x1\n",
    "        grads['w12'] += delta_h1 * x2\n",
    "        grads['b1'] += delta_h1\n",
    "        grads['w21'] += delta_h2 * x1\n",
    "        grads['w22'] += delta_h2 * x2\n",
    "        grads['b2'] += delta_h2\n",
    "    \n",
    "    # Average gradients\n",
    "    for key in grads:\n",
    "        grads[key] /= n_samples\n",
    "    \n",
    "    return [grads['w11'], grads['w12'], grads['b1'],\n",
    "            grads['w21'], grads['w22'], grads['b2'],\n",
    "            grads['w_out1'], grads['w_out2'], grads['b_out']]\n",
    "\n",
    "print(\"Gradient computation (backpropagation) ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network_multistart(X, y, n_trials=10, scale=0.5, verbose=True):\n",
    "    \"\"\"\n",
    "    Try N random initializations and return the one with lowest initial loss.\n",
    "\n",
    "    Parameters:\n",
    "    - X, y: Training data\n",
    "    - n_trials: Number of random initializations to try (default 10)\n",
    "    - scale: Standard deviation for weight initialization (default 0.5)\n",
    "    - verbose: Print progress (default True)\n",
    "\n",
    "    Returns:\n",
    "    - best_network: TinyNetwork with best initial weights\n",
    "    - best_loss: Initial loss of best network\n",
    "    \"\"\"\n",
    "    best_network = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Trying {n_trials} random initializations...\")\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # Create network with random init\n",
    "        network = TinyNetwork(scale=scale)\n",
    "\n",
    "        # Compute initial loss\n",
    "        loss = compute_loss(network, X, y)\n",
    "\n",
    "        if verbose and trial < 5:  # Show first few trials\n",
    "            print(f\"  Trial {trial+1}: Initial loss = {loss:.4f}\")\n",
    "\n",
    "        # Keep if best so far\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_network = network\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[OK] Best initialization: Loss = {best_loss:.4f}\")\n",
    "\n",
    "    return best_network, best_loss\n",
    "\n",
    "print(\"Multi-start initialization function ready!\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step function ready!\n"
     ]
    }
   ],
   "source": [
    "#def train_step(network, X, y, learning_rate):\n",
    "#    \"\"\"Single gradient descent step.\"\"\"\n",
    "#    grads = compute_gradients(network, X, y)\n",
    "#    weights = network.get_weights()\n",
    "#    new_weights = [w - learning_rate * g for w, g in zip(weights, grads)]\n",
    "#    network.set_weights(new_weights)\n",
    "#    loss = compute_loss(network, X, y)\n",
    "#    acc = compute_accuracy(network, X, y)\n",
    "#    return loss, acc\n",
    "\n",
    "\n",
    "def train_step(network, X, y, base_lr, training_state):\n",
    "    grads = compute_gradients(network, X, y)\n",
    "    w = np.array(network.get_weights(), dtype=float)\n",
    "    direction = -np.array(grads)\n",
    "\n",
    "    # Try a few candidate step sizes\n",
    "    lrs = [base_lr, base_lr / 2, base_lr * 2]\n",
    "    best_loss = np.inf\n",
    "    best_w = w\n",
    "\n",
    "    for eta in lrs:\n",
    "        cand_w = w + eta * direction\n",
    "        network.set_weights(cand_w.tolist())\n",
    "        loss = compute_loss(network, X, y)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_w = cand_w\n",
    "\n",
    "    # Commit to the best candidate\n",
    "    network.set_weights(best_w.tolist())\n",
    "    loss = best_loss\n",
    "    acc = compute_accuracy(network, X, y)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training step function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_momentum(network, X, y, base_lr, training_state):\n",
    "    \"\"\"Gradient descent with momentum and line search.\"\"\"\n",
    "    grads = compute_gradients(network, X, y)\n",
    "    w = np.array(network.get_weights(), dtype=float)\n",
    "\n",
    "    # Momentum parameter (typical value: 0.9)\n",
    "    beta = 0.9\n",
    "\n",
    "    # Initialize velocity if first iteration\n",
    "    if 'velocity' not in training_state:\n",
    "        training_state['velocity'] = np.zeros_like(grads)\n",
    "\n",
    "    # Update velocity: v = beta * v + (1-beta) * gradient\n",
    "    training_state['velocity'] = beta * training_state['velocity'] + (1 - beta) * np.array(grads)\n",
    "\n",
    "    # Direction is now based on velocity instead of raw gradient\n",
    "    direction = -training_state['velocity']\n",
    "\n",
    "    # Line search: try 3 learning rates\n",
    "    lrs = [base_lr, base_lr / 2, base_lr * 2]\n",
    "    best_loss = np.inf\n",
    "    best_w = w\n",
    "\n",
    "    for eta in lrs:\n",
    "        cand_w = w + eta * direction\n",
    "        network.set_weights(cand_w.tolist())\n",
    "        loss = compute_loss(network, X, y)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_w = cand_w\n",
    "\n",
    "    # Commit to the best candidate\n",
    "    network.set_weights(best_w.tolist())\n",
    "    loss = best_loss\n",
    "    acc = compute_accuracy(network, X, y)\n",
    "    return loss, acc\n",
    "\n",
    "print(\"Momentum training function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Gradient Descent Works: Simple Rules, Powerful Results\n",
    "\n",
    "The code you see below looks complex, but it follows a few **simple rules**:\n",
    "\n",
    "**The Algorithm (in plain English):**\n",
    "1. **Forward pass**: Calculate prediction from current weights\n",
    "2. **Compute error**: How wrong is the prediction?\n",
    "3. **Backpropagation**: Calculate which direction to adjust each weight\n",
    "4. **Update weights**: Take a small step in that direction\n",
    "5. **Repeat** until error is small enough\n",
    "\n",
    "**The math behind it:**\n",
    "- **Derivatives** tell us which direction makes error smaller\n",
    "- **Chain rule** connects output error back to every weight\n",
    "- **Learning rate** controls step size (too big = unstable, too small = slow)\n",
    "\n",
    "Despite ~100 lines of code, it's just these 5 steps repeated!\n",
    "\n",
    "### Why Multiple Starting Points?\n",
    "\n",
    "The loss landscape has **hills and valleys**. Gradient descent rolls downhill from wherever it starts:\n",
    "- **Good starting point**: Near a deep valley → converges quickly\n",
    "- **Bad starting point**: On a plateau or shallow valley → gets stuck\n",
    "\n",
    "**Multi-start strategy**: Try 10 random starts, pick the one with lowest initial loss. This gives us the best \"head start\" down the hill!\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding Your Controls\n",
    "\n",
    "Before you start training, here's what each control does:\n",
    "\n",
    "### Dataset Selector\n",
    "- **Corner / Corner Noisy**: Easier problem (one corner vs. three corners)\n",
    "- **Clean XOR**: Moderate difficulty (standard XOR problem)\n",
    "- **Noisy XOR**: Harder (lots of overlap between classes)\n",
    "- **Perfect XOR**: Easiest (minimal noise)\n",
    "\n",
    "**Try different datasets to see how problem difficulty affects learning!**\n",
    "\n",
    "### Algorithm Selector\n",
    "- **Basic Gradient Descent**: Uses line search to pick best step size\n",
    "- **Gradient Descent + Momentum**: Adds \"velocity\" to help escape local minima and speed up convergence\n",
    "\n",
    "**Momentum often works better on harder problems!**\n",
    "\n",
    "### Learning Rate\n",
    "- **Slow (0.1)**: Safe but may take many steps\n",
    "- **Moderate (0.3)**: Good default balance\n",
    "- **Fast (0.5)**: Converges quickly but may be unstable\n",
    "- **Very Fast (1.0)**: May overshoot or diverge\n",
    "\n",
    "**Experiment to see the trade-off between speed and stability!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Watch Learning Happen! (8-10 min)\n",
    "\n",
    "Now let's see gradient descent in action. You'll watch the network **learn** to solve XOR automatically.\n",
    "\n",
    "### What You'll See:\n",
    "- **Left panel**: Decision boundary evolving in real-time\n",
    "- **Right panel**: Loss decreasing as the network learns\n",
    "- **Training controls**: Step through learning at your own pace\n",
    "\n",
    "### Smart Initialization Strategy\n",
    "\n",
    "When you click **\"Reset Network\"**, the system tries **10 different random starting points** and automatically picks the one with the lowest initial loss. This is called **multi-start initialization**.\n",
    "\n",
    "**Why this helps:**\n",
    "- The loss landscape is like a bumpy terrain with many hills and valleys\n",
    "- Some random starting points are near deep valleys (good!) → fast convergence\n",
    "- Others are on plateaus or shallow valleys (bad!) → slow or no convergence\n",
    "- By trying multiple starts, we increase the odds of finding a good valley\n",
    "\n",
    "**The intuition:**\n",
    "- Imagine dropping 10 balls randomly on a hilly landscape\n",
    "- Each ball rolls to a different local low point\n",
    "- We pick the ball that found the **deepest valley** to start from\n",
    "- This gives gradient descent the best chance to find a great solution!\n",
    "\n",
    "**Real-world machine learning:**\n",
    "- Training often uses smart initialization strategies (Xavier, He initialization)\n",
    "- Advanced optimizers (Adam, RMSprop) adapt learning rates automatically\n",
    "- Large models sometimes run multiple training sessions and pick the best result\n",
    "\n",
    "**Try this:**\n",
    "1. Train until convergence or until stuck (loss stops decreasing)\n",
    "2. If stuck, click \"Reset Network\" to get a fresh starting point\n",
    "3. Notice how the initial loss varies - some starting points are much better!\n",
    "4. Compare: Does momentum help escape bad starting points?\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_training_state(network, X, y, loss_history, epoch, show_log_scale=True):\n    \"\"\"2-panel visualization: Decision boundary + Loss curve.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Left panel: Decision boundary in input space\n    x_min, x_max = -2.0, 2.0\n    y_min, y_max = -2.0, 2.0\n    h = 0.05\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    \n    Z_out, _, _ = network.predict_batch(mesh_points)\n    Z_out = Z_out.reshape(xx.shape)\n    \n    ax1.contourf(xx, yy, Z_out, levels=20, alpha=0.4, cmap='RdBu_r')\n    ax1.contour(xx, yy, Z_out, levels=[0.5], colors='green', linewidths=3)\n    ax1.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=50, alpha=0.7,\n               edgecolors='k', linewidths=1, label='Class 0')\n    ax1.scatter(X[y==1, 0], X[y==1, 1], c='red', s=50, alpha=0.7,\n               edgecolors='k', linewidths=1, label='Class 1')\n    ax1.set_xlim(x_min, x_max)\n    ax1.set_ylim(y_min, y_max)\n    ax1.set_xlabel('x\\u2081', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('x\\u2082', fontsize=12, fontweight='bold')\n    ax1.set_title(f'Decision Boundary (Epoch {epoch})', fontsize=12, fontweight='bold')\n    ax1.legend(loc='upper right')\n    ax1.grid(True, alpha=0.3)\n    ax1.set_aspect('equal')\n    \n    # Right panel: Loss curve\n    if len(loss_history) > 0:\n        ax2.plot(loss_history, 'b-', linewidth=2)\n        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n        ax2.set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12, fontweight='bold')\n        ax2.set_title('Training Loss Curve', fontsize=12, fontweight='bold')\n        ax2.grid(True, alpha=0.3)\n        \n        if show_log_scale and len(loss_history) > 5:\n            ax2.set_yscale('log')\n            ax2.set_ylabel('Loss (log scale)', fontsize=12, fontweight='bold')\n    else:\n        ax2.text(0.5, 0.5, 'No training yet...', ha='center', va='center',\n                fontsize=14, transform=ax2.transAxes)\n        ax2.set_xlabel('Epoch', fontsize=12)\n        ax2.set_ylabel('Loss', fontsize=12)\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Visualization function ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interface ready!\n"
     ]
    }
   ],
   "source": [
    "# Training state management\n",
    "'''training_state = {\n",
    "    'network': TinyNetwork(CONVERGENT_SEEDS[0]),  # Start with first seed\n",
    "    'epoch': 0,\n",
    "    'loss_history': [],\n",
    "    'learning_rate': 0.1,\n",
    "    'current_seed_idx': 0\n",
    "}'''\n",
    "\n",
    "training_state = {\n",
    "    'network': None,  # Will be initialized with multi-start below\n",
    "    'epoch': 0,\n",
    "    'loss_history': [],\n",
    "    'learning_rate': 0.3,\n",
    "    'algorithm': 'basic',  # 'basic' or 'momentum'\n",
    "    'current_seed_idx': None,\n",
    "}\n",
    "# Status display\n",
    "status_html = HTML(value=\"<h3>Ready to train!</h3>\")\n",
    "plot_output = Output()\n",
    "\n",
    "\n",
    "# Algorithm selector dropdown\n",
    "from ipywidgets import Dropdown\n",
    "\n",
    "algorithm_dropdown = Dropdown(\n",
    "    options=[\n",
    "        ('Basic Gradient Descent', 'basic'),\n",
    "        ('Gradient Descent + Momentum', 'momentum')\n",
    "    ],\n",
    "    value='basic',\n",
    "    description='Algorithm:',\n",
    "    layout=Layout(width='300px')\n",
    ")\n",
    "\n",
    "def on_algorithm_change(change):\n",
    "    training_state['algorithm'] = change['new']\n",
    "    # Reset velocity when switching algorithms\n",
    "    if 'velocity' in training_state:\n",
    "        del training_state['velocity']\n",
    "\n",
    "algorithm_dropdown.observe(on_algorithm_change, names='value')\n",
    "\n",
    "# Learning rate dropdown\n",
    "lr_dropdown = Dropdown(\n",
    "    options=[\n",
    "        ('Slow (0.1)', 0.1),\n",
    "        ('Moderate (0.3) - Default', 0.3),\n",
    "        ('Fast (0.5)', 0.5),\n",
    "        ('Very Fast (1.0)', 1.0)\n",
    "    ],\n",
    "    value=0.3,\n",
    "    description='Learning Rate:',\n",
    "    layout=Layout(width='300px')\n",
    ")\n",
    "\n",
    "def on_lr_change(change):\n",
    "    training_state['learning_rate'] = change['new']\n",
    "\n",
    "lr_dropdown.observe(on_lr_change, names='value')\n",
    "\n",
    "# Dataset selector dropdown\n",
    "dataset_dropdown = Dropdown(\n",
    "    options=[\n",
    "        ('Corner (easier)', 'corner'),\n",
    "        ('Corner Noisy (moderate)', 'corner_noisy'),\n",
    "        ('Clean XOR (moderate)', 'clean'),\n",
    "        ('Noisy XOR (harder)', 'noisy'),\n",
    "        ('Perfect XOR (easiest)', 'perfect')\n",
    "    ],\n",
    "    value='clean',\n",
    "    description='Dataset:',\n",
    "    layout=Layout(width='300px')\n",
    ")\n",
    "\n",
    "def on_dataset_change(change):\n",
    "    global X_train, y_train, current_dataset_type\n",
    "    current_dataset_type = change['new']\n",
    "    X_train, y_train = create_xor_dataset(dataset_type=current_dataset_type)\n",
    "    # Reset training when dataset changes\n",
    "    best_network, _ = initialize_network_multistart(X_train, y_train, n_trials=10, verbose=False)\n",
    "    training_state['network'] = best_network\n",
    "    training_state['epoch'] = 0\n",
    "    training_state['loss_history'] = []\n",
    "    if 'velocity' in training_state:\n",
    "        del training_state['velocity']\n",
    "    update_display()\n",
    "\n",
    "dataset_dropdown.observe(on_dataset_change, names='value')\n",
    "\n",
    "# Buttons\n",
    "train_1_btn = Button(\n",
    "    description='Train 1 Step',\n",
    "    button_style='info',\n",
    "    layout=Layout(width='150px', height='40px')\n",
    ")\n",
    "\n",
    "train_10_btn = Button(\n",
    "    description='Train 10 Steps',\n",
    "    button_style='primary',\n",
    "    layout=Layout(width='150px', height='40px')\n",
    ")\n",
    "\n",
    "train_converge_btn = Button(\n",
    "    description='Train to Convergence',\n",
    "    button_style='success',\n",
    "    layout=Layout(width='180px', height='40px')\n",
    ")\n",
    "\n",
    "reset_btn = Button(\n",
    "    description='Reset Network',\n",
    "    button_style='warning',\n",
    "    layout=Layout(width='150px', height='40px')\n",
    ")\n",
    "\n",
    "def update_display():\n",
    "    \"\"\"Update status and visualization.\"\"\"\n",
    "    epoch = training_state['epoch']\n",
    "    loss = training_state['loss_history'][-1] if training_state['loss_history'] else compute_loss(training_state['network'], X_train, y_train)\n",
    "    acc = compute_accuracy(training_state['network'], X_train, y_train)\n",
    "    \n",
    "    status_html.value = f\"<h3>Epoch {epoch} | Loss: {loss:.6f} | Accuracy: {acc:.2%}</h3>\"\n",
    "    \n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        plot_training_state(training_state['network'], X_train, y_train, \n",
    "                          training_state['loss_history'], epoch)\n",
    "\n",
    "def train_n_steps(n):\n",
    "    \"\"\"Train for n steps using selected algorithm.\"\"\"\n",
    "    algorithm = training_state.get('algorithm', 'basic')\n",
    "\n",
    "    for _ in range(n):\n",
    "        # Dispatch to correct training function\n",
    "        if algorithm == 'momentum':\n",
    "            loss, acc = train_step_momentum(training_state['network'], X_train, y_train,\n",
    "                                          training_state['learning_rate'], training_state)\n",
    "        else:\n",
    "            loss, acc = train_step(training_state['network'], X_train, y_train,\n",
    "                                 training_state['learning_rate'], training_state)\n",
    "\n",
    "        training_state['loss_history'].append(loss)\n",
    "        training_state['epoch'] += 1\n",
    "\n",
    "        # Early stopping if converged\n",
    "        if acc >= 0.99:\n",
    "            break\n",
    "\n",
    "    update_display()\n",
    "\n",
    "def on_train_1(btn):\n",
    "    train_n_steps(1)\n",
    "\n",
    "def on_train_10(btn):\n",
    "    train_n_steps(10)\n",
    "\n",
    "def on_train_converge(btn):\n",
    "    \"\"\"Train until convergence or max 500 epochs using selected algorithm.\"\"\"\n",
    "    algorithm = training_state.get('algorithm', 'basic')\n",
    "    max_epochs = 500\n",
    "\n",
    "    while training_state['epoch'] < max_epochs:\n",
    "        # Dispatch to correct training function\n",
    "        if algorithm == 'momentum':\n",
    "            loss, acc = train_step_momentum(training_state['network'], X_train, y_train,\n",
    "                                          training_state['learning_rate'], training_state)\n",
    "        else:\n",
    "            loss, acc = train_step(training_state['network'], X_train, y_train,\n",
    "                                 training_state['learning_rate'], training_state)\n",
    "\n",
    "        training_state['loss_history'].append(loss)\n",
    "        training_state['epoch'] += 1\n",
    "\n",
    "        if acc >= 0.99 or loss < 0.01:\n",
    "            break\n",
    "\n",
    "    update_display()\n",
    "\n",
    "'''def on_reset(btn):\n",
    "    \"\"\"Reset to new random seed.\"\"\"\n",
    "    training_state['current_seed_idx'] = (training_state['current_seed_idx'] + 1) % len(CONVERGENT_SEEDS)\n",
    "    training_state['network'] = TinyNetwork(CONVERGENT_SEEDS[training_state['current_seed_idx']])\n",
    "    training_state['epoch'] = 0\n",
    "    training_state['loss_history'] = []\n",
    "    update_display()'''\n",
    "\n",
    "def on_reset(btn):\n",
    "    \"\"\"Reset with multi-start initialization (tries multiple random starts).\"\"\"\n",
    "    global X_train, y_train\n",
    "\n",
    "    # Multi-start: try 10 random inits, pick best\n",
    "    status_html.value = \"<h3>&#128260; Finding good starting point (10 random trials)...</h3>\"\n",
    "\n",
    "    best_network, best_loss = initialize_network_multistart(\n",
    "        X_train, y_train,\n",
    "        n_trials=10,\n",
    "        scale=0.5,\n",
    "        verbose=False  # Don't clutter output\n",
    "    )\n",
    "\n",
    "    # Update state\n",
    "    training_state['network'] = best_network\n",
    "    training_state['epoch'] = 0\n",
    "    training_state['loss_history'] = []\n",
    "\n",
    "    # Clear momentum velocity\n",
    "    if 'velocity' in training_state:\n",
    "        del training_state['velocity']\n",
    "\n",
    "    # Show result\n",
    "    acc = compute_accuracy(best_network, X_train, y_train)\n",
    "    status_html.value = f\"<h3>&#10024; Reset! Best init: Loss={best_loss:.4f}, Acc={acc:.2%}</h3>\"\n",
    "\n",
    "    update_display()\n",
    "\n",
    "# Connect buttons\n",
    "train_1_btn.on_click(on_train_1)\n",
    "train_10_btn.on_click(on_train_10)\n",
    "train_converge_btn.on_click(on_train_converge)\n",
    "reset_btn.on_click(on_reset)\n",
    "\n",
    "\n",
    "# Initialize network with multi-start (try 10 random inits, pick best)\n",
    "print(\"Initializing network with multi-start strategy...\")\n",
    "training_state['network'], initial_loss = initialize_network_multistart(\n",
    "    X_train, y_train,\n",
    "    n_trials=10,\n",
    "    scale=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"Network initialized! Starting loss: {initial_loss:.4f}\\n\")\n",
    "\n",
    "print(\"Training interface ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INTERACTIVE TRAINING: WATCH GRADIENT DESCENT LEARN!\n",
      "======================================================================\n",
      "\n",
      "Instructions:\n",
      "  1. Click 'Train 1 Step' to see one gradient descent update\n",
      "  2. Click 'Train 10 Steps' to speed things up\n",
      "  3. Click 'Train to Convergence' to watch it finish automatically\n",
      "  4. Click 'Reset Network' to try a different random starting point\n",
      "\n",
      "Watch the LEFT panel: Decision boundary evolves!\n",
      "Watch the RIGHT panel: Loss decreases!\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287b7208e96347879984203c49494b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Ready to train!</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31cfc45e2814918879a95bd477e4566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Train 1 Step', layout=Layout(height='40px', width='150…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edfda8aa40b43999209f98ef2d9499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the interactive training interface\n",
    "print(\"=\"*70)\n",
    "print(\"INTERACTIVE TRAINING: WATCH GRADIENT DESCENT LEARN!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"  1. Click 'Train 1 Step' to see one gradient descent update\")\n",
    "print(\"  2. Click 'Train 10 Steps' to speed things up\")\n",
    "print(\"  3. Click 'Train to Convergence' to watch it finish automatically\")\n",
    "print(\"  4. Click 'Reset Network' to try a different random starting point\")\n",
    "print(\"\\nWatch the LEFT panel: Decision boundary evolves!\")\n",
    "print(\"Watch the RIGHT panel: Loss decreases!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display(status_html)\n",
    "display(HTML(\"<h4>Training Configuration:</h4>\"))\n",
    "display(HBox([dataset_dropdown, algorithm_dropdown, lr_dropdown]))\n",
    "display(HTML(\"<h4>Training Controls:</h4>\"))\n",
    "display(HBox([train_1_btn, train_10_btn, train_converge_btn, reset_btn]))\n",
    "display(plot_output)\n",
    "\n",
    "# Show initial state\n",
    "update_display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}