{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8, Module 3: Multi-Step Denoising (Reverse Diffusion)\n",
    "\n",
    "**Estimated time:** 12-15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: The Moment of Magic**\n",
    "\n",
    "In Module 2, you trained a model to predict noise at any timestep.\n",
    "\n",
    "Now comes the **exciting part**: using that model to **generate images from pure noise**!\n",
    "\n",
    "In this module, you'll:\n",
    "- Implement the **reverse diffusion algorithm**\n",
    "- Start with random noise and progressively denoise it\n",
    "- Watch digits **emerge from chaos** step by step!\n",
    "- Generate multiple samples to see variation\n",
    "- Compare quality at different step counts (50 vs 100 vs 200 steps)\n",
    "\n",
    "**This is how DALL-E, Midjourney, and Stable Diffusion work‚Äîjust at much larger scale!**\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ **Setup: Import Libraries and Reconstruct Model**\n",
    "\n",
    "**Important:** This module uses the model you trained in Module 2. We have two options:\n",
    "\n",
    "### **Option A:** You just finished Module 2 in this same session\n",
    "- Your model is still in memory\n",
    "- Skip the \"Quick Training\" cell\n",
    "- The model variable `model` is already available\n",
    "\n",
    "### **Option B:** You're starting fresh (new session)\n",
    "- We'll train a quick version (takes ~2-3 minutes)\n",
    "- Run the \"Quick Training\" cell below\n",
    "- This gives you a working model\n",
    "\n",
    "Let's start by setting up!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check if model exists from Module 2\n",
    "try:\n",
    "    test = model\n",
    "    print(\"\\n‚úÖ Model found from Module 2!\")\n",
    "    print(\"   You can skip the 'Quick Training' section below.\")\n",
    "    MODEL_AVAILABLE = True\n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è  No model found from Module 2.\")\n",
    "    print(\"   Run the 'Quick Training' section below to train a model.\")\n",
    "    MODEL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ **Quick Training (Run ONLY if model not available)**\n",
    "\n",
    "If you're starting fresh, run this section to quickly train a denoiser.\n",
    "\n",
    "**If you just finished Module 2**, skip this section!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_AVAILABLE:\n",
    "    print(\"Training a quick denoiser model...\\n\")\n",
    "    print(\"This will take ~2-3 minutes with GPU.\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    (x_train_full, y_train_full), _ = keras.datasets.mnist.load_data()\n",
    "    NUM_CLASSES = 4\n",
    "    mask = y_train_full < NUM_CLASSES\n",
    "    x_train = x_train_full[mask]\n",
    "    \n",
    "    # Downsample to 16x16\n",
    "    x_train_resized = tf.image.resize(x_train[..., np.newaxis], [16, 16]).numpy()\n",
    "    x_train_resized = x_train_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Diffusion schedule\n",
    "    NUM_TIMESTEPS = 200\n",
    "    betas = np.linspace(0.0001, 0.02, NUM_TIMESTEPS).astype('float32')\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = np.cumprod(alphas).astype('float32')\n",
    "    alphas_cumprod_tf = tf.constant(alphas_cumprod, dtype=tf.float32)\n",
    "    \n",
    "    # Build model (simplified U-Net)\n",
    "    def get_timestep_embedding(timesteps, embedding_dim=32):\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
    "        emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n",
    "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
    "        return emb\n",
    "    \n",
    "    # Input layers\n",
    "    noisy_image_input = layers.Input(shape=(16, 16, 1), name='noisy_image')\n",
    "    timestep_input = layers.Input(shape=(), dtype=tf.int32, name='timestep')\n",
    "    \n",
    "    # Timestep embedding\n",
    "    t_emb = layers.Lambda(lambda t: get_timestep_embedding(t, 32))(timestep_input)\n",
    "    t_emb = layers.Dense(64, activation='relu')(t_emb)\n",
    "    t_emb = layers.Dense(64, activation='relu')(t_emb)\n",
    "    t_emb_reshaped = layers.Reshape((1, 1, 64))(t_emb)\n",
    "    t_emb_broadcast = layers.Lambda(lambda x: tf.tile(x, [1, 16, 16, 1]))(t_emb_reshaped)\n",
    "    \n",
    "    # Concatenate\n",
    "    x = layers.Concatenate()([noisy_image_input, t_emb_broadcast])\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    skip1 = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2)(skip1)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    skip2 = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2)(skip2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Concatenate()([x, skip2])\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Concatenate()([x, skip1])\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Output\n",
    "    noise_output = layers.Conv2D(1, 3, padding='same', name='noise_pred')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[noisy_image_input, timestep_input], outputs=noise_output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse')\n",
    "    \n",
    "    print(\"\\n‚úì Model built!\")\n",
    "    \n",
    "    # Training function\n",
    "    def create_training_batch(images, batch_size, alphas_cumprod_tf, num_timesteps):\n",
    "        indices = np.random.randint(0, len(images), batch_size)\n",
    "        batch_images = images[indices]\n",
    "        timesteps = np.random.randint(0, num_timesteps, batch_size)\n",
    "        alpha_bar_t = tf.gather(alphas_cumprod_tf, timesteps)\n",
    "        noise = tf.random.normal(shape=batch_images.shape)\n",
    "        sqrt_alpha_bar_t = tf.sqrt(alpha_bar_t)[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar_t = tf.sqrt(1.0 - alpha_bar_t)[:, None, None, None]\n",
    "        noisy_images = sqrt_alpha_bar_t * batch_images + sqrt_one_minus_alpha_bar_t * noise\n",
    "        return (noisy_images, timesteps), noise\n",
    "    \n",
    "    # Train\n",
    "    EPOCHS = 8\n",
    "    BATCH_SIZE = 128\n",
    "    STEPS_PER_EPOCH = len(x_train_resized) // BATCH_SIZE\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_losses = []\n",
    "        for step in range(STEPS_PER_EPOCH):\n",
    "            (noisy_images, timesteps), noise_targets = create_training_batch(\n",
    "                x_train_resized, BATCH_SIZE, alphas_cumprod_tf, NUM_TIMESTEPS\n",
    "            )\n",
    "            loss = model.train_on_batch([noisy_images, timesteps], noise_targets)\n",
    "            epoch_losses.append(loss)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {np.mean(epoch_losses):.6f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete in {time.time()-start_time:.1f}s!\")\n",
    "    MODEL_AVAILABLE = True\n",
    "else:\n",
    "    print(\"‚úì Skipping training - model already available from Module 2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß **Setup Diffusion Schedule**\n",
    "\n",
    "We need the same schedule from Module 1 for reverse diffusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate diffusion schedule (same as Modules 1 & 2)\n",
    "NUM_TIMESTEPS = 200\n",
    "BETA_START = 0.0001\n",
    "BETA_END = 0.02\n",
    "\n",
    "betas = np.linspace(BETA_START, BETA_END, NUM_TIMESTEPS).astype('float32')\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = np.cumprod(alphas).astype('float32')\n",
    "\n",
    "print(\"‚úÖ Diffusion schedule ready!\")\n",
    "print(f\"   Timesteps: {NUM_TIMESTEPS}\")\n",
    "print(f\"   Beta range: [{BETA_START}, {BETA_END}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé¨ **Implement Reverse Diffusion Algorithm**\n",
    "\n",
    "### **The Reverse Process:**\n",
    "\n",
    "Starting from pure noise at t=200, we gradually denoise:\n",
    "\n",
    "```\n",
    "1. Start: x_200 = pure random noise\n",
    "2. For t = 199, 198, 197, ..., 1, 0:\n",
    "   a. Predict noise: ŒµÃÇ = model(x_t, t)\n",
    "   b. Remove predicted noise (with scaling)\n",
    "   c. Add tiny random noise (except at t=0)\n",
    "   d. Result: x_{t-1} (slightly less noisy)\n",
    "3. End: x_0 = generated digit!\n",
    "```\n",
    "\n",
    "### **Mathematical Formula (Simplified DDPM):**\n",
    "\n",
    "**Symbols:**\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\hat{\\epsilon} \\right) + \\sigma_t \\cdot z$$\n",
    "\n",
    "**Plain English:**\n",
    "> \"Next step = (current - scaled predicted noise) / signal scale + tiny random noise\"\n",
    "\n",
    "**Pseudocode:**\n",
    "```python\n",
    "next_step = (current - noise_removal_amount) / signal_scale + small_random_noise\n",
    "```\n",
    "\n",
    "**Why add tiny noise back?**\n",
    "- Prevents model from \"collapsing\" to one image\n",
    "- Adds stochasticity ‚Üí variety in generated images\n",
    "- Amount of noise decreases over time\n",
    "\n",
    "Let's implement it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion(model, num_steps=200, image_size=16, seed=None):\n",
    "    \"\"\"\n",
    "    Generate an image from pure noise using reverse diffusion.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained denoiser model\n",
    "        num_steps: Number of denoising steps (default 200)\n",
    "        image_size: Size of image to generate\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        generated_image: Final denoised image\n",
    "        trajectory: List of images at each step\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "    \n",
    "    # Start with pure noise\n",
    "    x_t = np.random.randn(1, image_size, image_size, 1).astype('float32')\n",
    "    \n",
    "    # Store trajectory for visualization\n",
    "    trajectory = []\n",
    "    \n",
    "    # Reverse diffusion loop\n",
    "    for t in reversed(range(num_steps)):\n",
    "        # Save current state every 20 steps for visualization\n",
    "        if t % 20 == 0 or t < 10:\n",
    "            trajectory.append((t, x_t.copy()))\n",
    "        \n",
    "        # Predict noise\n",
    "        t_input = np.array([t])\n",
    "        predicted_noise = model.predict([x_t, t_input], verbose=0)\n",
    "        \n",
    "        # Get diffusion parameters\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alphas_cumprod[t]\n",
    "        beta_t = betas[t]\n",
    "        \n",
    "        # Compute denoising step\n",
    "        # x_{t-1} = (1/sqrt(alpha_t)) * (x_t - ((1-alpha_t)/sqrt(1-alpha_bar_t)) * predicted_noise)\n",
    "        noise_removal = (1.0 - alpha_t) / np.sqrt(1.0 - alpha_bar_t)\n",
    "        x_t = (x_t - noise_removal * predicted_noise) / np.sqrt(alpha_t)\n",
    "        \n",
    "        # Add small random noise (except at final step)\n",
    "        if t > 0:\n",
    "            noise_scale = np.sqrt(beta_t)\n",
    "            x_t = x_t + noise_scale * np.random.randn(*x_t.shape).astype('float32')\n",
    "    \n",
    "    # Add final image\n",
    "    trajectory.append((0, x_t.copy()))\n",
    "    \n",
    "    return x_t, trajectory\n",
    "\n",
    "print(\"‚úÖ Reverse diffusion function ready!\")\n",
    "print(\"\\nReady to generate images from noise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® **Generate Your First Image!**\n",
    "\n",
    "Let's start from pure noise and watch a digit emerge!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Generating image from pure noise...\\n\")\n",
    "print(\"This will take ~20-30 seconds for 200 steps.\\n\")\n",
    "\n",
    "# Generate with fixed seed for reproducibility\n",
    "generated_image, trajectory = reverse_diffusion(model, num_steps=200, seed=42)\n",
    "\n",
    "print(\"‚úÖ Generation complete!\\n\")\n",
    "print(f\"Generated {len(trajectory)} snapshots along the way.\")\n",
    "print(f\"\\nFinal image range: [{generated_image.min():.2f}, {generated_image.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì∏ **Visualize the Generation Trajectory**\n",
    "\n",
    "Let's see how the image evolved from noise to digit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 evenly spaced snapshots\n",
    "snapshot_indices = np.linspace(0, len(trajectory)-1, 10, dtype=int)\n",
    "snapshots = [trajectory[i] for i in snapshot_indices]\n",
    "\n",
    "# Visualize trajectory\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (t, img) in enumerate(snapshots):\n",
    "    # Clip and normalize for display\n",
    "    img_display = np.clip(img[0, :, :, 0], -2, 2)\n",
    "    \n",
    "    axes[idx].imshow(img_display, cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[idx].set_title(f't = {t}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    # Color border based on progress\n",
    "    progress = 1.0 - (t / 200)\n",
    "    if progress < 0.3:\n",
    "        color = 'red'  # Early: mostly noise\n",
    "    elif progress < 0.7:\n",
    "        color = 'orange'  # Middle: emerging structure\n",
    "    else:\n",
    "        color = 'green'  # Late: recognizable digit\n",
    "    \n",
    "    for spine in axes[idx].spines.values():\n",
    "        spine.set_edgecolor(color)\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Generation Trajectory: From Pure Noise to Digit', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"  - Early steps (red): Pure noise, no structure\")\n",
    "print(\"  - Middle steps (orange): Vague shapes emerge\")\n",
    "print(\"  - Late steps (green): Clear digit structure\")\n",
    "print(\"  - Final step: Recognizable digit!\")\n",
    "print(\"\\n‚úì The model learned to gradually construct structure from randomness!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Questions Q16-Q17 (Observations)**\n",
    "\n",
    "### **Q16. Predict: At approximately what timestep (t value) does the digit start to become recognizable?**\n",
    "\n",
    "*Look at the trajectory above. When can you first identify what digit it might be?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q17. Looking at the full trajectory, describe how structure emerges. Does it appear suddenly or gradually?**\n",
    "\n",
    "*Think about: Do edges appear first? Does overall shape come before fine details?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üé≤ **Generate Multiple Samples**\n",
    "\n",
    "Let's generate several digits from different random seeds!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≤ Generating 12 different samples...\\n\")\n",
    "print(\"Each starts from different random noise.\\n\")\n",
    "\n",
    "# Generate 12 samples with different seeds\n",
    "num_samples = 12\n",
    "samples = []\n",
    "\n",
    "for seed in range(num_samples):\n",
    "    print(f\"  Generating sample {seed+1}/{num_samples}...\", end='\\r')\n",
    "    generated, _ = reverse_diffusion(model, num_steps=200, seed=seed)\n",
    "    samples.append(generated[0, :, :, 0])\n",
    "\n",
    "print(\"\\n‚úÖ Generated 12 samples!\\n\")\n",
    "\n",
    "# Display grid\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    axes[idx].imshow(np.clip(sample, 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[idx].set_title(f'Sample {idx+1}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('12 Different Digits Generated from Different Random Starting Noise', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key observations:\")\n",
    "print(\"  - Each sample looks different (different random noise ‚Üí different digit)\")\n",
    "print(\"  - Some are clearer than others (stochasticity in process)\")\n",
    "print(\"  - Quality varies (toy model limitations)\")\n",
    "print(f\"  - All are digits 0-3 (model only trained on {MODEL_AVAILABLE and 'those classes' or 'limited classes'})\")\n",
    "print(\"\\n‚úì This demonstrates the generative capability of diffusion models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q18 (Analysis)**\n",
    "\n",
    "### **Q18. Looking at the 12 generated samples, what do you notice about variety and quality? Are all digits equally clear?**\n",
    "\n",
    "*Consider: Are some blurry? Do you see variety? What digits appear most often?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è© **Step Count Comparison**\n",
    "\n",
    "Does using more denoising steps improve quality? Let's find out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è© Comparing different step counts...\\n\")\n",
    "\n",
    "step_counts = [50, 100, 200]\n",
    "results = []\n",
    "\n",
    "for steps in step_counts:\n",
    "    print(f\"  Generating with {steps} steps...\", end='\\r')\n",
    "    # Use same starting noise (seed=42) for fair comparison\n",
    "    generated, _ = reverse_diffusion(model, num_steps=steps, seed=42)\n",
    "    results.append((steps, generated[0, :, :, 0]))\n",
    "\n",
    "print(\"\\n‚úÖ Generated all comparisons!\\n\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for idx, (steps, img) in enumerate(results):\n",
    "    axes[idx].imshow(np.clip(img, 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[idx].set_title(f'{steps} Steps', fontsize=13, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Quality vs. Number of Denoising Steps (Same Starting Noise)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"  - 50 steps: Faster, but may be less detailed\")\n",
    "print(\"  - 100 steps: Good balance of speed and quality\")\n",
    "print(\"  - 200 steps: Most detailed (used during training)\")\n",
    "print(\"\\n‚úì More steps generally = better quality, but diminishing returns\")\n",
    "print(\"   (Real models like DALL-E use 50-100 steps for speed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Questions Q19-Q20 (Critical Thinking)**\n",
    "\n",
    "### **Q19. Compare the 50-step vs 200-step generations. Is the quality difference large or small? Is 200 steps necessary?**\n",
    "\n",
    "*Think about: trade-off between quality and speed. When would you use fewer steps?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q20. Why do you think the generated digits aren't perfect? What limitations does this toy model have?**\n",
    "\n",
    "*Consider: model size, training data, resolution, number of training epochs*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üåâ **Bridge to Module 4: From Toy to Professional Models**\n",
    "\n",
    "### **What You've Accomplished:**\n",
    "- ‚úÖ Trained a toy denoiser (~100k parameters)\n",
    "- ‚úÖ Generated 16√ó16 digits from noise\n",
    "- ‚úÖ Understood reverse diffusion algorithm\n",
    "- ‚úÖ Seen authentic limitations of small models\n",
    "\n",
    "### **What's Coming in Module 4:**\n",
    "- üöÄ Professional diffusion model (millions of parameters)\n",
    "- üé® 32√ó32 RGB images (vs 16√ó16 grayscale)\n",
    "- üèÜ Much higher quality generations\n",
    "- üåç Bridge to DALL-E, Stable Diffusion, Midjourney\n",
    "\n",
    "### **Key Differences:**\n",
    "\n",
    "| Aspect | Your Toy Model | DALL-E / Stable Diffusion |\n",
    "|--------|----------------|---------------------------|\n",
    "| **Parameters** | ~100,000 | Billions |\n",
    "| **Training data** | ~24,000 digits | Billions of images |\n",
    "| **Resolution** | 16√ó16 | 512√ó512 or higher |\n",
    "| **Training time** | 2-3 minutes | Weeks on supercomputers |\n",
    "| **Quality** | Blurry digits | Photorealistic images |\n",
    "| **Control** | Random | Text prompts guide generation |\n",
    "\n",
    "**But the core algorithm is THE SAME!**\n",
    "- Forward diffusion: Add noise progressively\n",
    "- Train denoiser: Predict noise at each timestep\n",
    "- Reverse diffusion: Remove noise step by step\n",
    "- Text conditioning: Guide the denoising process\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Module 3 Complete!\n",
    "\n",
    "You now understand:\n",
    "- **Reverse diffusion algorithm** (iterative denoising)\n",
    "- **Image generation from noise** (pure randomness ‚Üí structured output)\n",
    "- **Trajectory visualization** (how structure emerges gradually)\n",
    "- **Stochasticity and variety** (different seeds ‚Üí different outputs)\n",
    "- **Step count trade-offs** (quality vs speed)\n",
    "- **Toy model limitations** (why professional models need scale)\n",
    "\n",
    "**Key insights:**\n",
    "1. **Structure emerges gradually** - not suddenly!\n",
    "2. **More steps = better quality** - but with diminishing returns\n",
    "3. **Stochasticity creates variety** - essential for diverse generations\n",
    "4. **Scale matters** - billions of parameters >> thousands\n",
    "\n",
    "**Congratulations!** You've implemented the core algorithm that powers DALL-E, Midjourney, and Stable Diffusion!\n",
    "\n",
    "**Ready to see professional quality?**\n",
    "\n",
    "Move on to **Module 4: Pre-Trained Diffusion Model**, where you'll use a pre-trained model to generate CIFAR-10 images and understand how to scale up to text-to-image generation!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
