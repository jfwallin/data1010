{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Module 2: Activation Functions in Detail\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Compare how different activation functions handle the same input\n",
    "- Understand key properties: smoothness, range, and behavior at extremes\n",
    "- Choose appropriate activations for different situations\n",
    "- Connect activation properties to their effects on learning\n",
    "\n",
    "**Time:** ~15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**From Module 1:** You saw how activation functions warp space and create curved boundaries from simple linear rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Module 1\n",
    "\n",
    "In **Module 1**, you explored:\n",
    "- Four activation functions: Sigmoid, Tanh, ReLU, Step\n",
    "- How they warp 2D grids (the \"rubber sheet\" effect)\n",
    "- How straight rules in warped space create curved boundaries in original space\n",
    "\n",
    "**Today:** We'll compare these functions side-by-side and understand when to use each one.\n",
    "\n",
    "**Key question:** If they all \"bend space,\" what makes them different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Load All Four Functions\n",
    "\n",
    "Let's reload the activation functions from Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FloatSlider, interact, VBox, HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def step(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "print(\"âœ“ Activation functions loaded!\")\n",
    "print(\"\\nToday we'll compare their behaviors in detail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Side-by-Side Comparison\n",
    "\n",
    "Let's plot all four activation functions together to see their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), dpi=100)\n",
    "axes = axes.flatten()\n",
    "\n",
    "functions = [\n",
    "    (sigmoid, 'Sigmoid', 'purple'),\n",
    "    (tanh, 'Tanh', 'blue'),\n",
    "    (relu, 'ReLU', 'green'),\n",
    "    (step, 'Step', 'red')\n",
    "]\n",
    "\n",
    "for ax, (func, name, color) in zip(axes, functions):\n",
    "    y = func(x)\n",
    "    ax.plot(x, y, color=color, linewidth=3, label=name)\n",
    "    ax.axhline(0, color='black', linewidth=0.8, alpha=0.3)\n",
    "    ax.axvline(0, color='black', linewidth=0.8, alpha=0.3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('Input (x)', fontsize=12)\n",
    "    ax.set_ylabel('Output', fontsize=12)\n",
    "    ax.set_title(f'{name} Activation', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    \n",
    "    # Set y-limits based on function\n",
    "    if name == 'Sigmoid':\n",
    "        ax.set_ylim(-0.2, 1.2)\n",
    "    elif name == 'Tanh':\n",
    "        ax.set_ylim(-1.2, 1.2)\n",
    "    elif name == 'ReLU':\n",
    "        ax.set_ylim(-0.5, 5.5)\n",
    "    else:  # Step\n",
    "        ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve:\")\n",
    "print(\"  â€¢ Which functions are smooth? Which have sharp corners/jumps?\")\n",
    "print(\"  â€¢ Which functions have bounded outputs (can't go arbitrarily high)?\")\n",
    "print(\"  â€¢ Which functions treat positive and negative inputs differently?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Properties Table\n",
    "\n",
    "Let's organize what we know about each function's properties.\n",
    "\n",
    "| Function | Output Range | Smooth? | Centered at 0? | Kills Negatives? |\n",
    "|----------|--------------|---------|----------------|------------------|\n",
    "| **Sigmoid** | [0, 1] | Yes (S-curve) | No (outputs 0 to 1) | No (maps to ~0) |\n",
    "| **Tanh** | [-1, 1] | Yes (S-curve) | Yes | No (maps to ~-1) |\n",
    "| **ReLU** | [0, âˆž) | No (corner at 0) | No | Yes (outputs 0) |\n",
    "| **Step** | {0, 1} | No (hard jump) | No | Yes (outputs 0) |\n",
    "\n",
    "### What These Properties Mean:\n",
    "\n",
    "**Output Range:**\n",
    "- **Bounded** (Sigmoid, Tanh, Step): Can't output extremely large values\n",
    "- **Unbounded** (ReLU): Can output arbitrarily large values\n",
    "\n",
    "**Smoothness:**\n",
    "- **Smooth** (Sigmoid, Tanh): Gradual changes everywhere\n",
    "- **Has corner** (ReLU): Sharp change in slope at x=0\n",
    "- **Discontinuous** (Step): Sudden jump at x=0\n",
    "\n",
    "**Centered at 0:**\n",
    "- **Yes** (Tanh): Outputs are balanced around zero\n",
    "- **No** (Sigmoid, ReLU, Step): Outputs are always non-negative or biased\n",
    "\n",
    "**Kills Negatives:**\n",
    "- **Yes** (ReLU, Step): Negative inputs â†’ 0 output\n",
    "- **No** (Sigmoid, Tanh): Negative inputs â†’ small but non-zero output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive: Test on the Same Input\n",
    "\n",
    "Adjust the slider to see how each activation function transforms the **same input value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_activations(input_value):\n",
    "    \"\"\"\n",
    "    Show how all four activations transform the same input.\n",
    "    \"\"\"\n",
    "    # Compute outputs\n",
    "    outputs = {\n",
    "        'Sigmoid': sigmoid(np.array([input_value]))[0],\n",
    "        'Tanh': tanh(np.array([input_value]))[0],\n",
    "        'ReLU': relu(np.array([input_value]))[0],\n",
    "        'Step': step(np.array([input_value]))[0]\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "    \n",
    "    names = list(outputs.keys())\n",
    "    values = list(outputs.values())\n",
    "    colors = ['purple', 'blue', 'green', 'red']\n",
    "    \n",
    "    bars = ax.bar(names, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.axhline(0, color='black', linewidth=1, alpha=0.5)\n",
    "    ax.set_ylabel('Output', fontsize=13)\n",
    "    ax.set_title(f'Input = {input_value:.2f} â†’ How Each Activation Responds', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(-1.5, max(5, max(values) + 0.5))\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.1,\n",
    "               f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    print(f\"\\nInput: {input_value:.2f}\")\n",
    "    print(\"=\"*70)\n",
    "    for name, val in outputs.items():\n",
    "        print(f\"  {name:8s} â†’ {val:7.3f}\")\n",
    "    \n",
    "    # Context-specific observations\n",
    "    if input_value < -2:\n",
    "        print(\"\\nðŸ’¡ For very negative inputs:\")\n",
    "        print(\"   Sigmoid â†’ close to 0\")\n",
    "        print(\"   Tanh â†’ close to -1\")\n",
    "        print(\"   ReLU & Step â†’ exactly 0\")\n",
    "    elif input_value > 2:\n",
    "        print(\"\\nðŸ’¡ For very positive inputs:\")\n",
    "        print(\"   Sigmoid â†’ close to 1\")\n",
    "        print(\"   Tanh â†’ close to 1\")\n",
    "        print(\"   ReLU â†’ keeps growing!\")\n",
    "        print(\"   Step â†’ exactly 1\")\n",
    "    elif abs(input_value) < 0.5:\n",
    "        print(\"\\nðŸ’¡ Near zero:\")\n",
    "        print(\"   Sigmoid â‰ˆ 0.5 (halfway)\")\n",
    "        print(\"   Tanh â‰ˆ 0 (centered)\")\n",
    "        print(\"   ReLU â‰ˆ input (if positive)\")\n",
    "        print(\"   Step = 0 or 1 (jumps at exactly 0)\")\n",
    "\n",
    "# Interactive widget\n",
    "interact(\n",
    "    compare_all_activations,\n",
    "    input_value=FloatSlider(\n",
    "        min=-5, max=5, step=0.1, value=0.0,\n",
    "        description='Input:',\n",
    "        continuous_update=False\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When to Use Each Activation?\n",
    "\n",
    "Different activation functions are better for different situations.\n",
    "\n",
    "### Sigmoid:\n",
    "**Best for:** Output layer when you want probabilities (0 to 1)\n",
    "- \"What's the probability this is a cat?\"\n",
    "- Outputs always between 0 and 1\n",
    "- Smooth everywhere\n",
    "\n",
    "**Downside:** Can get \"stuck\" with very large/small inputs (gradient vanishing)\n",
    "\n",
    "### Tanh:\n",
    "**Best for:** Hidden layers in older neural networks\n",
    "- Centered at zero (helps with learning)\n",
    "- Outputs between -1 and 1\n",
    "- Smooth everywhere\n",
    "\n",
    "**Downside:** Also suffers from gradient vanishing at extremes\n",
    "\n",
    "### ReLU:\n",
    "**Best for:** Hidden layers in modern neural networks (most popular!)\n",
    "- Simple and fast to compute\n",
    "- Doesn't saturate for positive values\n",
    "- Helps with learning speed\n",
    "\n",
    "**Downside:** \"Dead neurons\" problem (if a neuron always outputs 0, it stops learning)\n",
    "\n",
    "### Step:\n",
    "**Best for:** Theoretical understanding, old perceptron models\n",
    "- Simplest possible nonlinearity\n",
    "- Clear \"yes/no\" decision\n",
    "\n",
    "**Downside:** Not smooth â†’ can't use gradient descent to learn (no meaningful derivative!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing \"Saturation\"\n",
    "\n",
    "**Saturation** means the output barely changes even when the input changes a lot.\n",
    "\n",
    "This happens with Sigmoid and Tanh at extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate saturation\n",
    "x_saturate = np.linspace(-10, 10, 500)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), dpi=100)\n",
    "\n",
    "# Sigmoid saturation\n",
    "y_sig = sigmoid(x_saturate)\n",
    "ax1.plot(x_saturate, y_sig, 'purple', linewidth=3, label='Sigmoid')\n",
    "ax1.axhspan(0.95, 1.0, alpha=0.2, color='red', label='Saturated (high)')\n",
    "ax1.axhspan(0.0, 0.05, alpha=0.2, color='red')\n",
    "ax1.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('Input', fontsize=12)\n",
    "ax1.set_ylabel('Output', fontsize=12)\n",
    "ax1.set_title('Sigmoid Saturation', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.text(6, 0.5, 'Flat region â†’\\noutput barely changes!', \n",
    "        fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "# Tanh saturation\n",
    "y_tanh = tanh(x_saturate)\n",
    "ax2.plot(x_saturate, y_tanh, 'blue', linewidth=3, label='Tanh')\n",
    "ax2.axhspan(0.95, 1.0, alpha=0.2, color='red', label='Saturated (high)')\n",
    "ax2.axhspan(-1.0, -0.95, alpha=0.2, color='red', label='Saturated (low)')\n",
    "ax2.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('Input', fontsize=12)\n",
    "ax2.set_ylabel('Output', fontsize=12)\n",
    "ax2.set_title('Tanh Saturation', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.text(6, 0, 'Flat region â†’\\noutput barely changes!', \n",
    "        fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaturation Problem:\")\n",
    "print(\"=\"*70)\n",
    "print(\"When input is very large or very small:\")\n",
    "print(\"  â€¢ Output is nearly constant (flat region)\")\n",
    "print(\"  â€¢ Small changes in input â†’ almost no change in output\")\n",
    "print(\"  â€¢ This makes learning slow (gradient â‰ˆ 0)\")\n",
    "print(\"\\nReLU doesn't saturate for positive inputs â†’ faster learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary: Choosing an Activation Function\n",
    "\n",
    "Here's a decision guide:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Is this the OUTPUT layer?           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚             â”‚\n",
    "   YES           NO (hidden layer)\n",
    "    â”‚             â”‚\n",
    "    â–¼             â–¼\n",
    "Need probability?  Use ReLU\n",
    "  â†’ Sigmoid       (most common for modern networks)\n",
    "                  \n",
    "Need range [-1,1]? Alternative: Tanh\n",
    "  â†’ Tanh          (older networks)\n",
    "```\n",
    "\n",
    "**In practice (2020s):**\n",
    "- **Hidden layers:** ReLU (or variants like Leaky ReLU, ELU)\n",
    "- **Output layer:** Depends on task\n",
    "  - Binary classification (0 or 1): Sigmoid\n",
    "  - Multi-class (cat/dog/bird): Softmax (generalization of sigmoid)\n",
    "  - Regression (predict number): No activation (or ReLU if non-negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Your Answer Sheet\n",
    "\n",
    "**Q12.** Which activation function has outputs that are always between 0 and 1? Why might this be useful for a model that outputs probabilities?\n",
    "\n",
    "**Q13.** Which activation function is centered at zero (has negative outputs for negative inputs, positive outputs for positive inputs)?\n",
    "\n",
    "**Q14.** What does \"saturation\" mean? Which activation functions saturate at extreme input values?\n",
    "\n",
    "**Q15.** ReLU is the most popular activation for hidden layers in modern neural networks. Looking at Section 4's interactive tool, what advantage does ReLU have for very large positive inputs compared to Sigmoid or Tanh?\n",
    "\n",
    "**Q16.** Why is the Step function bad for training neural networks with gradient descent? (Hint: Think about smoothness.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q12-Q16** on your answer sheet\n",
    "2. **Return to the LMS** and continue to Module 3\n",
    "3. In Module 3, you'll combine everything: weighted sums â†’ activation â†’ perceptron!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
