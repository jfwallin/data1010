{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Module 4: Real-World Medical Classification - Breast Cancer Dataset\n",
    "\n",
    "## From XOR to Cancer Diagnosis: Binary Classification with High-Dimensional Data\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Apply neural networks to a real medical diagnosis problem\n",
    "- Understand binary classification with many features (30 vs. 4 in Iris)\n",
    "- Experiment with model architecture (layers and units)\n",
    "- Learn that simpler models can work well (don't always need huge networks)\n",
    "- Understand confusion matrices and medical prediction trade-offs\n",
    "\n",
    "**Estimated time:** 15-20 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: Introduction - From Penguins to Medical Diagnosis\n\n**Your ML journey so far:**\n- Module 0-2: Hand-built gradient descent with XOR (2 inputs, binary output)\n- Module 3: Penguin species (4 inputs, 3 classes)\n- Module 4 (now): Breast cancer diagnosis (30 inputs, binary output)\n\n**Today's challenge: High-dimensional medical data**\n\n### The Wisconsin Breast Cancer Dataset\n\nThis dataset contains measurements from **569 breast tumor cell samples**. Each sample is classified as:\n- **Benign (0)**: Not cancerous\n- **Malignant (1)**: Cancerous\n\nEach sample has **30 numeric features** computed from cell nucleus images:\n- Radius, texture, perimeter, area, smoothness, compactness, etc.\n- Mean, standard error, and \"worst\" (largest) values for each measurement\n\n**The Task:** Given these 30 measurements, predict whether the tumor is benign or malignant.\n\n**Connection to XOR:**\n- XOR: 2 inputs ‚Üí binary output (0 or 1)\n- Breast Cancer: 30 inputs ‚Üí binary output (benign or malignant)\n- Same type of problem, but with **15 times more features**!\n\n**Why This Matters:**\n- Real medical diagnosis with high stakes\n- Predictions help doctors make treatment decisions\n- False positives vs. false negatives have different consequences\n- ML accuracy is rarely 100% - we need to understand errors\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Setup and Package Check\n",
    "\n",
    "First, let's make sure we have all the packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package check and installation\n",
    "import sys\n",
    "\n",
    "print(\"Checking packages...\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check/install required packages\n",
    "required_packages = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('tensorflow', 'tensorflow')\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for module_name, package_name in required_packages:\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"‚úì {module_name} is installed\")\n",
    "    except ImportError:\n",
    "        print(f\"‚úó {module_name} not found\")\n",
    "        missing_packages.append(package_name)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nInstalling missing packages: {', '.join(missing_packages)}\")\n",
    "    import subprocess\n",
    "    for package in missing_packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(\"‚úì All packages installed!\")\n",
    "else:\n",
    "    print(\"\\n‚úì All required packages are already installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import all the libraries we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Load and Explore the Dataset\n",
    "\n",
    "Let's load the breast cancer dataset and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset from scikit-learn\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data  # Features: 30 measurements per tumor\n",
    "y = cancer.target  # Labels: 0=Malignant, 1=Benign\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BREAST CANCER DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset shape:\")\n",
    "print(f\"  Features (X): {X.shape} - {X.shape[0]} tumors, {X.shape[1]} measurements each\")\n",
    "print(f\"  Labels (y):   {y.shape} - {y.shape[0]} diagnoses\")\n",
    "\n",
    "print(f\"\\nFeature names (first 10 of {len(cancer.feature_names)}):\")\n",
    "for i, name in enumerate(cancer.feature_names[:10]):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "print(\"  ... and 20 more features\")\n",
    "\n",
    "print(f\"\\nTarget classes:\")\n",
    "benign_count = np.sum(y == 1)\n",
    "malignant_count = np.sum(y == 0)\n",
    "print(f\"  Class 0 (Malignant): {malignant_count} samples ({malignant_count/len(y):.1%})\")\n",
    "print(f\"  Class 1 (Benign):    {benign_count} samples ({benign_count/len(y):.1%})\")\n",
    "\n",
    "print(f\"\\nFeature value ranges (first 3 features):\")\n",
    "for i in range(3):\n",
    "    print(f\"  {cancer.feature_names[i]:30s}: min={X[:, i].min():7.2f}, max={X[:, i].max():7.2f}\")\n",
    "print(\"  ‚Üí Wide range! Scaling will be important.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "**High-dimensional data:**\n",
    "- 30 features (vs. 4 for Iris, 2 for XOR!)\n",
    "- Hard to visualize all dimensions at once\n",
    "- Neural networks can find patterns in high-dimensional spaces\n",
    "\n",
    "**Class balance:**\n",
    "- Roughly 63% benign, 37% malignant\n",
    "- Reasonably balanced (not heavily skewed)\n",
    "\n",
    "**Feature scales:**\n",
    "- Different features have wildly different ranges\n",
    "- StandardScaler will normalize everything to mean=0, std=1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Train/Test Split and Feature Scaling\n",
    "\n",
    "**Why split the data?**\n",
    "- **Training set (80%):** Used to teach the network\n",
    "- **Test set (20%):** Used to evaluate how well it learned (never seen during training)\n",
    "\n",
    "**Why scale features?**\n",
    "- Features have vastly different ranges (e.g., radius: 6-28, area: 143-2500)\n",
    "- Neural networks learn better when all features are on similar scales\n",
    "- StandardScaler transforms each feature to mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (80%) and test (20%) sets\n",
    "# stratify=y ensures each class is proportionally represented in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Training set:   {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set:       {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features to mean=0, std=1\n",
    "# IMPORTANT: Fit scaler on training data only, then apply to both train and test\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling (first feature):\")\n",
    "print(f\"  Before scaling - mean: {X_train[:, 0].mean():.2f}, std: {X_train[:, 0].std():.2f}\")\n",
    "print(f\"  After scaling  - mean: {X_train_scaled[:, 0].mean():.2f}, std: {X_train_scaled[:, 0].std():.2f}\")\n",
    "print(\"\\n‚úì Data prepared for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Baseline Linear Model (No Hidden Layer)\n",
    "\n",
    "**Connection to Module 2:**\n",
    "- Remember XOR couldn't be solved with a linear model (no hidden layer)\n",
    "- But cancer diagnosis might be different!\n",
    "- In **30-dimensional space**, even a linear (straight) boundary might separate the classes well\n",
    "\n",
    "**Model architecture:**\n",
    "- Input: 30 features\n",
    "- Output: 1 unit with sigmoid activation (outputs probability 0-1)\n",
    "- NO hidden layers - just a direct linear transformation\n",
    "\n",
    "This is essentially **logistic regression** - a classic ML baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline linear model (no hidden layer)\n",
    "baseline_model = Sequential([\n",
    "    Dense(1, activation='sigmoid', input_dim=30, name='output')\n",
    "], name='Baseline_Linear_Model')\n",
    "\n",
    "# Compile model\n",
    "# - Adam optimizer: smart gradient descent with momentum\n",
    "# - binary_crossentropy: loss function for binary classification\n",
    "# - accuracy: % of correct predictions\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE LINEAR MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "baseline_model.summary()\n",
    "print(\"\\nüí° This model has NO hidden layers - just 30 inputs ‚Üí 1 output\")\n",
    "print(\"   Total trainable parameters: (30 features √ó 1 output) + 1 bias = 31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training baseline linear model...\\n\")\n",
    "\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    verbose=0  # Suppress detailed output\n",
    ")\n",
    "\n",
    "print(\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss_baseline, test_accuracy_baseline = baseline_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Get predictions for confusion matrix\n",
    "y_pred_baseline = (baseline_model.predict(X_test_scaled, verbose=0) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE LINEAR MODEL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:  {test_accuracy_baseline:.1%}\")\n",
    "print(f\"Test Loss:      {test_loss_baseline:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"\")\n",
    "print(\"                    Predicted\")\n",
    "print(\"                Malignant  Benign\")\n",
    "print(f\"Actual Malignant    {cm_baseline[0,0]:4d}      {cm_baseline[0,1]:4d}\")\n",
    "print(f\"       Benign       {cm_baseline[1,0]:4d}      {cm_baseline[1,1]:4d}\")\n",
    "print(\"\")\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  - True Negatives (correctly identified malignant):  {cm_baseline[0,0]}\")\n",
    "print(f\"  - False Positives (benign predicted as malignant):  {cm_baseline[1,0]}\")\n",
    "print(f\"  - False Negatives (malignant predicted as benign):  {cm_baseline[0,1]}\")\n",
    "print(f\"  - True Positives (correctly identified benign):     {cm_baseline[1,1]}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if test_accuracy_baseline >= 0.95:\n",
    "    print(\"\\n‚úÖ Excellent! Linear model achieves high accuracy on cancer data.\")\n",
    "    print(\"   Why? Even a straight boundary in 30D space can separate classes well!\")\n",
    "elif test_accuracy_baseline >= 0.90:\n",
    "    print(\"\\n‚úì Good baseline performance!\")\n",
    "    print(\"  Let's see if hidden layers can improve this.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Linear model struggles.\")\n",
    "    print(\"  We'll likely need hidden layers for better performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history_baseline.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history_baseline.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Baseline Linear Model: Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history_baseline.history['loss'], label='Training', linewidth=2)\n",
    "ax2.plot(history_baseline.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Baseline Linear Model: Loss', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° What to look for:\")\n",
    "print(\"   - Accuracy increases and stabilizes\")\n",
    "print(\"   - Training and validation curves are close (no major overfitting)\")\n",
    "print(\"   - Loss decreases smoothly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Heatmap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n",
    "\n",
    "# Plot heatmap\n",
    "im = ax.imshow(cm_baseline, cmap='Blues', aspect='auto')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('Count', rotation=-90, va=\"bottom\", fontsize=11)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Malignant', 'Benign'], fontsize=11)\n",
    "ax.set_yticklabels(['Malignant', 'Benign'], fontsize=11)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Baseline Model: Confusion Matrix', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, cm_baseline[i, j],\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Understanding the confusion matrix:\")\n",
    "print(\"   - Diagonal (top-left to bottom-right): Correct predictions\")\n",
    "print(\"   - Off-diagonal: Errors\")\n",
    "print(\"   - Top-right (False Negative): Dangerous! Missed cancer diagnosis\")\n",
    "print(\"   - Bottom-left (False Positive): Concerning! Unnecessary worry/treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Experiment with Model Architecture\n",
    "\n",
    "**Now it's your turn to experiment!**\n",
    "\n",
    "Try different architectures by changing the variables below:\n",
    "- `num_hidden_layers`: How many hidden layers? (Try 0, 1, 2)\n",
    "- `units_per_layer`: How many neurons in each hidden layer? (Try 8, 16, 32, 64)\n",
    "\n",
    "**Questions to explore:**\n",
    "- Does adding hidden layers improve accuracy?\n",
    "- Does more neurons always help?\n",
    "- At what point do you see diminishing returns?\n",
    "- Can you beat the baseline linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ADJUSTABLE PARAMETERS - CHANGE THESE! ==========\n",
    "num_hidden_layers = 1   # Try: 0, 1, 2, 3\n",
    "units_per_layer = 16    # Try: 8, 16, 32, 64\n",
    "# ===========================================================\n",
    "\n",
    "# Build model programmatically based on parameters\n",
    "model = Sequential(name=f'Model_{num_hidden_layers}L_{units_per_layer}U')\n",
    "\n",
    "# Add hidden layers\n",
    "for i in range(num_hidden_layers):\n",
    "    if i == 0:\n",
    "        # First hidden layer needs input_dim specified\n",
    "        model.add(Dense(units_per_layer, activation='relu', input_dim=30, name=f'hidden_{i+1}'))\n",
    "    else:\n",
    "        model.add(Dense(units_per_layer, activation='relu', name=f'hidden_{i+1}'))\n",
    "\n",
    "# Add output layer\n",
    "if num_hidden_layers == 0:\n",
    "    # If no hidden layers, need to specify input_dim on output layer\n",
    "    model.add(Dense(1, activation='sigmoid', input_dim=30, name='output'))\n",
    "else:\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Show architecture\n",
    "print(\"=\"*70)\n",
    "print(f\"MODEL ARCHITECTURE: {num_hidden_layers} Hidden Layers, {units_per_layer} Units Each\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "\n",
    "if num_hidden_layers == 0:\n",
    "    print(\"\\nüí° No hidden layers - this is the same as the baseline!\")\n",
    "else:\n",
    "    total_params = sum([np.prod(w.shape) for w in model.trainable_weights])\n",
    "    print(f\"\\nüí° This model has {num_hidden_layers} hidden layer(s) with {units_per_layer} neurons each\")\n",
    "    print(f\"   Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Your Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(f\"Training model with {num_hidden_layers} hidden layers, {units_per_layer} units each...\\n\")\n",
    "\n",
    "history_custom = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss_custom, test_accuracy_custom = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Get predictions and confusion matrix\n",
    "y_pred_custom = (model.predict(X_test_scaled, verbose=0) > 0.5).astype(int).flatten()\n",
    "cm_custom = confusion_matrix(y_test, y_pred_custom)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: BASELINE vs YOUR CUSTOM MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline (no hidden layer):              {test_accuracy_baseline:.1%}\")\n",
    "print(f\"Your model ({num_hidden_layers} layers, {units_per_layer:2d} units/layer): {test_accuracy_custom:.1%}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement = test_accuracy_custom - test_accuracy_baseline\n",
    "if improvement > 0.02:\n",
    "    print(f\"\\n‚úÖ Your model is better! Improvement: +{improvement:.1%}\")\n",
    "elif improvement > 0:\n",
    "    print(f\"\\n‚úì Slight improvement: +{improvement:.1%}\")\n",
    "elif improvement > -0.02:\n",
    "    print(f\"\\n‚âà Similar performance: {improvement:+.1%}\")\n",
    "    print(\"   The baseline was already very good!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Your model is worse: {improvement:+.1%}\")\n",
    "    print(\"   Sometimes simpler is better!\")\n",
    "\n",
    "print(\"\\nYour Model Confusion Matrix:\")\n",
    "print(\"\")\n",
    "print(\"                    Predicted\")\n",
    "print(\"                Malignant  Benign\")\n",
    "print(f\"Actual Malignant    {cm_custom[0,0]:4d}      {cm_custom[0,1]:4d}\")\n",
    "print(f\"       Benign       {cm_custom[1,0]:4d}      {cm_custom[1,1]:4d}\")\n",
    "print(\"\")\n",
    "\n",
    "# Compare false negatives (most critical in medical diagnosis)\n",
    "fn_baseline = cm_baseline[0, 1]\n",
    "fn_custom = cm_custom[0, 1]\n",
    "print(f\"False Negatives (missed cancers):\")\n",
    "print(f\"  Baseline: {fn_baseline}\")\n",
    "print(f\"  Your model: {fn_custom}\")\n",
    "if fn_custom < fn_baseline:\n",
    "    print(f\"  ‚úÖ Your model missed fewer cancers! ({fn_baseline - fn_custom} fewer)\")\n",
    "elif fn_custom == fn_baseline:\n",
    "    print(f\"  = Same number of missed cancers\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Your model missed more cancers ({fn_custom - fn_baseline} more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Custom Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history_custom.history['accuracy'], label='Training', linewidth=2)\n",
    "ax1.plot(history_custom.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Custom Model ({num_hidden_layers}L, {units_per_layer}U): Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history_custom.history['loss'], label='Training', linewidth=2)\n",
    "ax2.plot(history_custom.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Custom Model ({num_hidden_layers}L, {units_per_layer}U): Loss', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Signs of overfitting:\")\n",
    "print(\"   - Training accuracy much higher than validation\")\n",
    "print(\"   - Validation loss increases while training loss decreases\")\n",
    "print(\"   - Large gap between training and validation curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix heatmap for custom model\n",
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n",
    "\n",
    "# Plot heatmap\n",
    "im = ax.imshow(cm_custom, cmap='Blues', aspect='auto')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('Count', rotation=-90, va=\"bottom\", fontsize=11)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Malignant', 'Benign'], fontsize=11)\n",
    "ax.set_yticklabels(['Malignant', 'Benign'], fontsize=11)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Custom Model ({num_hidden_layers}L, {units_per_layer}U): Confusion Matrix', \n",
    "            fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, cm_custom[i, j],\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 7: Understanding Variability - Running Multiple Experiments\n\n### Why Run Multiple Experiments?\n\n**Remember from Module 3:** Neural networks are stochastic!\n- Random weight initialization leads to different results each run\n- Professional ML practice: run multiple times, report mean ¬± std\n- This is especially important when comparing models\n\n**Let's apply this to medical diagnosis:**\n- Is the baseline model consistently good?\n- Do hidden layers reliably improve performance?\n- How much do false negatives (missed cancers) vary?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Baseline Model with Multiple Runs\n",
    "\n",
    "Let's run the baseline linear model **5 times** and analyze the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline model multiple times\n",
    "num_runs = 5\n",
    "num_epochs = 100\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"RUNNING BASELINE LINEAR MODEL {num_runs} TIMES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWhy? To see how stable the model is across different initializations!\\n\")\n",
    "\n",
    "# Store results\n",
    "baseline_accuracies = []\n",
    "baseline_losses = []\n",
    "baseline_false_negatives = []  # Missed cancers - most critical!\n",
    "baseline_histories = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run+1}/{num_runs}...\", end=\" \")\n",
    "    \n",
    "    # Create NEW model for fresh initialization\n",
    "    model = Sequential([\n",
    "        Dense(1, activation='sigmoid', input_dim=30, name='output')\n",
    "    ], name=f'Baseline_Run_{run+1}')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Get predictions and confusion matrix\n",
    "    y_pred = (model.predict(X_test_scaled, verbose=0) > 0.5).astype(int).flatten()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    false_negatives = cm[0, 1]  # Malignant predicted as benign\n",
    "    \n",
    "    # Store\n",
    "    baseline_accuracies.append(test_acc)\n",
    "    baseline_losses.append(test_loss)\n",
    "    baseline_false_negatives.append(false_negatives)\n",
    "    baseline_histories.append(history)\n",
    "    \n",
    "    print(f\"Acc: {test_acc:.1%}, Loss: {test_loss:.4f}, FN: {false_negatives}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL - STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:        {np.mean(baseline_accuracies):.1%} ¬± {np.std(baseline_accuracies):.1%}\")\n",
    "print(f\"Test Loss:            {np.mean(baseline_losses):.4f} ¬± {np.std(baseline_losses):.4f}\")\n",
    "print(f\"False Negatives:      {np.mean(baseline_false_negatives):.1f} ¬± {np.std(baseline_false_negatives):.2f}\")\n",
    "print(f\"  (Missed cancers - MOST CRITICAL METRIC!)\")\n",
    "print(f\"\\nAccuracy Range:       {np.min(baseline_accuracies):.1%} to {np.max(baseline_accuracies):.1%}\")\n",
    "print(f\"FN Range:             {int(np.min(baseline_false_negatives))} to {int(np.max(baseline_false_negatives))}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if np.std(baseline_accuracies) < 0.02:\n",
    "    print(\"\\n‚úÖ Very consistent results - stable baseline!\")\n",
    "elif np.std(baseline_accuracies) < 0.05:\n",
    "    print(\"\\n‚úì Reasonably consistent results\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è High variability - results depend on initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Custom Model with Multiple Runs\n",
    "\n",
    "Now let's run your custom model (with hidden layers) multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable parameters - CHANGE THESE!\n",
    "num_hidden_layers_exp = 1   # Try: 0, 1, 2\n",
    "units_per_layer_exp = 16    # Try: 8, 16, 32\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"RUNNING CUSTOM MODEL ({num_hidden_layers_exp} layers, {units_per_layer_exp} units) {num_runs} TIMES\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Store results\n",
    "custom_accuracies = []\n",
    "custom_losses = []\n",
    "custom_false_negatives = []\n",
    "custom_histories = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run+1}/{num_runs}...\", end=\" \")\n",
    "    \n",
    "    # Build model\n",
    "    model = Sequential(name=f'Custom_Run_{run+1}')\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(num_hidden_layers_exp):\n",
    "        if i == 0:\n",
    "            model.add(Dense(units_per_layer_exp, activation='relu', input_dim=30, name=f'hidden_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(units_per_layer_exp, activation='relu', name=f'hidden_{i+1}'))\n",
    "    \n",
    "    # Output layer\n",
    "    if num_hidden_layers_exp == 0:\n",
    "        model.add(Dense(1, activation='sigmoid', input_dim=30, name='output'))\n",
    "    else:\n",
    "        model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    y_pred = (model.predict(X_test_scaled, verbose=0) > 0.5).astype(int).flatten()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    false_negatives = cm[0, 1]\n",
    "    \n",
    "    # Store\n",
    "    custom_accuracies.append(test_acc)\n",
    "    custom_losses.append(test_loss)\n",
    "    custom_false_negatives.append(false_negatives)\n",
    "    custom_histories.append(history)\n",
    "    \n",
    "    print(f\"Acc: {test_acc:.1%}, Loss: {test_loss:.4f}, FN: {false_negatives}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"CUSTOM MODEL ({num_hidden_layers_exp} layers, {units_per_layer_exp} units) - STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:        {np.mean(custom_accuracies):.1%} ¬± {np.std(custom_accuracies):.1%}\")\n",
    "print(f\"Test Loss:            {np.mean(custom_losses):.4f} ¬± {np.std(custom_losses):.4f}\")\n",
    "print(f\"False Negatives:      {np.mean(custom_false_negatives):.1f} ¬± {np.std(custom_false_negatives):.2f}\")\n",
    "print(f\"\\nAccuracy Range:       {np.min(custom_accuracies):.1%} to {np.max(custom_accuracies):.1%}\")\n",
    "print(f\"FN Range:             {int(np.min(custom_false_negatives))} to {int(np.max(custom_false_negatives))}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison: Baseline vs Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL COMPARISON: BASELINE vs CUSTOM MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBaseline (no hidden layers):\")\n",
    "print(f\"  Accuracy:         {np.mean(baseline_accuracies):.1%} ¬± {np.std(baseline_accuracies):.1%}\")\n",
    "print(f\"  False Negatives:  {np.mean(baseline_false_negatives):.1f} ¬± {np.std(baseline_false_negatives):.2f}\")\n",
    "\n",
    "print(f\"\\nCustom Model ({num_hidden_layers_exp} layers, {units_per_layer_exp} units):\")\n",
    "print(f\"  Accuracy:         {np.mean(custom_accuracies):.1%} ¬± {np.std(custom_accuracies):.1%}\")\n",
    "print(f\"  False Negatives:  {np.mean(custom_false_negatives):.1f} ¬± {np.std(custom_false_negatives):.2f}\")\n",
    "\n",
    "acc_improvement = np.mean(custom_accuracies) - np.mean(baseline_accuracies)\n",
    "fn_improvement = np.mean(baseline_false_negatives) - np.mean(custom_false_negatives)\n",
    "\n",
    "print(f\"\\nMean Accuracy Improvement:  {acc_improvement:+.1%}\")\n",
    "print(f\"Mean FN Reduction:          {fn_improvement:+.1f} (positive = fewer missed cancers)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Box plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), dpi=100)\n",
    "\n",
    "# Accuracy box plot\n",
    "box_data_acc = [baseline_accuracies, custom_accuracies]\n",
    "labels = ['Baseline\\n(0 layers)', f'Custom\\n({num_hidden_layers_exp}L, {units_per_layer_exp}U)']\n",
    "\n",
    "bp1 = ax1.boxplot(box_data_acc, labels=labels, patch_artist=True, showmeans=True, meanline=True)\n",
    "for patch, color in zip(bp1['boxes'], ['lightblue', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax1.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Accuracy Distribution ({num_runs} runs)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add points\n",
    "for i, data in enumerate(box_data_acc, 1):\n",
    "    x = np.random.normal(i, 0.04, size=len(data))\n",
    "    ax1.scatter(x, data, alpha=0.6, s=60, c='red', edgecolors='black', linewidths=1)\n",
    "\n",
    "# False Negatives box plot\n",
    "box_data_fn = [baseline_false_negatives, custom_false_negatives]\n",
    "\n",
    "bp2 = ax2.boxplot(box_data_fn, labels=labels, patch_artist=True, showmeans=True, meanline=True)\n",
    "for patch, color in zip(bp2['boxes'], ['lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax2.set_ylabel('False Negatives (Missed Cancers)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'False Negatives Distribution ({num_runs} runs)', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add points\n",
    "for i, data in enumerate(box_data_fn, 1):\n",
    "    x = np.random.normal(i, 0.04, size=len(data))\n",
    "    ax2.scatter(x, data, alpha=0.6, s=60, c='red', edgecolors='black', linewidths=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Questions:\")\n",
    "print(\"   - Is the custom model CONSISTENTLY better?\")\n",
    "print(\"   - Do the boxes overlap? (If yes, improvement may not be reliable)\")\n",
    "print(\"   - Which metric matters more: accuracy or false negatives?\")\n",
    "print(\"   - Would you trust this model for medical diagnosis?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all training curves together\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10), dpi=100)\n",
    "\n",
    "# Baseline accuracy\n",
    "for i, history in enumerate(baseline_histories):\n",
    "    ax1.plot(history.history['val_accuracy'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "ax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Baseline: Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Baseline loss\n",
    "for i, history in enumerate(baseline_histories):\n",
    "    ax2.plot(history.history['val_loss'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Validation Loss', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Baseline: Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Custom accuracy\n",
    "for i, history in enumerate(custom_histories):\n",
    "    ax3.plot(history.history['val_accuracy'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "ax3.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Validation Accuracy', fontsize=11, fontweight='bold')\n",
    "ax3.set_title(f'Custom ({num_hidden_layers_exp}L, {units_per_layer_exp}U): Validation Accuracy', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Custom loss\n",
    "for i, history in enumerate(custom_histories):\n",
    "    ax4.plot(history.history['val_loss'], alpha=0.6, linewidth=2, label=f'Run {i+1}')\n",
    "ax4.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Validation Loss', fontsize=11, fontweight='bold')\n",
    "ax4.set_title(f'Custom ({num_hidden_layers_exp}L, {units_per_layer_exp}U): Validation Loss', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° What to observe:\")\n",
    "print(\"   - Do all runs converge to similar final values?\")\n",
    "print(\"   - Are there any outlier runs?\")\n",
    "print(\"   - Does one model show more variability than the other?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights: Stochasticity in Medical ML\n",
    "\n",
    "**What you should have learned:**\n",
    "\n",
    "1. **Results vary even on the same data!**\n",
    "   - Random initialization affects final performance\n",
    "   - Some runs may miss more cancers than others\n",
    "   - This is why clinical ML systems need extensive validation\n",
    "\n",
    "2. **Statistical reporting is essential in medicine**\n",
    "   - Reporting \"97% accuracy\" from one run is misleading\n",
    "   - \"97.2% ¬± 0.8%\" gives a true picture of reliability\n",
    "   - Variability in false negatives is critical - lives are at stake!\n",
    "\n",
    "3. **Model comparison requires statistics**\n",
    "   - If the boxes overlap significantly, the \"improvement\" may not be real\n",
    "   - Need statistical tests (t-test, etc.) to confirm differences\n",
    "   - In medicine, reproducibility is paramount\n",
    "\n",
    "4. **Simpler models can be more stable**\n",
    "   - Baseline model may have lower variance than complex models\n",
    "   - Trade-off: slightly lower mean accuracy but more consistent\n",
    "   - Consistency matters in clinical deployment\n",
    "\n",
    "**Medical Ethics Connection:**\n",
    "- If your model misses 2-4 cancers depending on initialization, that's a problem!\n",
    "- Real medical ML systems:\n",
    "  - Train on much larger datasets (thousands to millions of samples)\n",
    "  - Use ensemble methods (combine multiple models)\n",
    "  - Undergo extensive clinical trials\n",
    "  - Are validated on diverse patient populations\n",
    "\n",
    "**Professional Practice:**\n",
    "- Always run at least 5-10 times (ideally more)\n",
    "- Report mean ¬± std for all metrics\n",
    "- Show distributions (box plots, violin plots)\n",
    "- Consider worst-case runs, not just average\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Record Your Experiments\n",
    "\n",
    "**Instructions:** Run the experiment cells above multiple times with different values for `num_hidden_layers` and `units_per_layer`. Record your results in the table below (on your answer sheet)!\n",
    "\n",
    "### My Experiment Results\n",
    "\n",
    "| Hidden Layers | Units per Layer | Test Accuracy | False Negatives |\n",
    "|--------------|-----------------|---------------|----------------|\n",
    "| 0            | N/A             | ____%        | ____           |\n",
    "| 1            | 8               | ____%        | ____           |\n",
    "| 1            | 16              | ____%        | ____           |\n",
    "| 1            | 32              | ____%        | ____           |\n",
    "| 1            | 64              | ____%        | ____           |\n",
    "| 2            | 16              | ____%        | ____           |\n",
    "| 2            | 32              | ____%        | ____           |\n",
    "\n",
    "**Tip:** Copy this table to your answer sheet and fill it in as you experiment!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 9: Connection to Earlier Labs\n\nLet's connect what you just did to the gradient descent work from Modules 0-2!\n\n### The Journey: From Manual to Automatic\n\n**Module 0 (Dimension Lifting):**\n- You learned that XOR can't be solved in 2D, but CAN in 3D\n- Adding x‚ÇÉ = x‚ÇÅ √ó x‚ÇÇ \"lifted\" the problem to a solvable space\n- **Connection now:** Hidden layers do this automatically! Each hidden neuron creates a new \"dimension\"\n\n**Module 1 (Manual Weight Adjustment):**\n- You manually adjusted 9 weights to solve XOR\n- Saw how changing weights affects decision boundaries\n- **Connection now:** Keras `.fit()` adjusts hundreds of weights automatically!\n\n**Module 2 (Gradient Descent):**\n- You watched gradient descent automatically find good weights\n- Saw momentum speed up convergence\n- Experimented with learning rates\n- **Connection now:**\n  - `.fit()` uses **backpropagation** to compute gradients (same idea!)\n  - **Adam optimizer** is like momentum, but smarter (adaptive learning rates)\n  - All the math you learned still applies!\n\n**Module 3 (Penguin Species):**\n- Applied these ideas to real data (4 features, 3 classes)\n- Saw that simple models can work well\n- **Connection now:** Breast cancer has 30 features instead of 4, but same principles!\n\n### What's The Same?\n1. **Goal:** Find weights that minimize prediction error\n2. **Method:** Gradient descent (computing how to change weights)\n3. **Architecture:** Input ‚Üí Hidden layers ‚Üí Output\n4. **Learning:** Iterative improvement through many epochs\n\n### What's Different?\n1. **Scale:** 30 inputs instead of 2, potentially hundreds of weights instead of 9\n2. **Automation:** No manual weight adjustment, no manual gradient calculation\n3. **Data:** Real medical measurements instead of abstract patterns\n4. **Stakes:** Real diagnosis implications instead of academic exercise\n\n### The Big Picture\n\n```\nModule 0: Learned WHY hidden layers help (dimension lifting)\n         ‚Üì\nModule 1: Saw WHAT weights do (manual adjustment)\n         ‚Üì\nModule 2: Learned HOW to find weights automatically (gradient descent)\n         ‚Üì\nModule 3: Applied to REAL data (penguin species)\n         ‚Üì\nModule 4: Applied to HIGH-STAKES real data (cancer diagnosis)\n         ‚Üì\nSAME FUNDAMENTAL CONCEPTS, INCREASING COMPLEXITY AND REAL-WORLD RELEVANCE!\n```\n\n**The power of abstraction:** The gradient descent you learned on tiny XOR works the same way on medical datasets with millions of parameters!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Key Takeaways\n",
    "\n",
    "### 1. High-Dimensional Data Can Be Surprisingly Easy\n",
    "- 30 features sounds complex, but linear models can work well!\n",
    "- In high-dimensional space, classes are often more separable\n",
    "- \"Curse of dimensionality\" is real, but so is \"blessing of dimensionality\"\n",
    "\n",
    "### 2. Simpler Models Often Suffice\n",
    "- Baseline linear model likely achieved 94-97% accuracy\n",
    "- Adding hidden layers may give only small improvements\n",
    "- Don't assume you need a huge network for every problem!\n",
    "- **Occam's Razor:** Prefer simpler models when they work\n",
    "\n",
    "### 3. Medical ML Has Ethical Implications\n",
    "- **False Positive:** Predicting cancer when there isn't any\n",
    "  - Consequence: Unnecessary stress, additional tests, maybe biopsy\n",
    "- **False Negative:** Missing actual cancer\n",
    "  - Consequence: Delayed treatment, disease progression, worse outcomes\n",
    "- Which is worse? **Context matters!** Doctors use ML as one tool among many.\n",
    "\n",
    "### 4. Accuracy Isn't Everything\n",
    "- 97% sounds great, but that's still ~3 errors per 100 patients\n",
    "- Confusion matrix shows WHERE errors happen\n",
    "- In medicine, the TYPE of error matters as much as the rate\n",
    "\n",
    "### 5. Diminishing Returns with Complexity\n",
    "- Going from 0 to 1 hidden layer: may help a lot (or a little)\n",
    "- Going from 1 to 2 layers: often minimal improvement\n",
    "- More parameters = longer training, more overfitting risk\n",
    "- **Principle:** Add complexity only when justified by performance gain\n",
    "\n",
    "### 6. Real ML Is About Trade-offs\n",
    "- Accuracy vs. interpretability (can we explain predictions?)\n",
    "- Complexity vs. training time\n",
    "- False positives vs. false negatives\n",
    "- Model performance vs. computational cost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reflection Questions\n\nAnswer these on your answer sheet:\n\n**Q1.** How did the baseline linear model perform on breast cancer data? Were you surprised? Why or why not?\n\n**Q2.** Did adding hidden layers significantly improve accuracy? At what architecture did you see diminishing returns?\n\n**Q3.** Looking at your confusion matrices, did you reduce false negatives (missed cancers) with more complex models? Is there a trade-off with false positives?\n\n**Q4.** In medical diagnosis, which error is more concerning: false positive (predicting cancer when there isn't any) or false negative (missing actual cancer)? Explain your reasoning.\n\n**Q5.** Compare Module 3 (Penguins) and Module 4 (Breast Cancer):\n   - Which dataset benefited more from hidden layers?\n   - Why might this be? (Think about feature count and class separability)\n\n**Q6.** Reflect on your journey from Module 0 to now:\n   - What's the connection between manually lifting XOR to 3D (Module 0) and hidden layers in Keras?\n   - How does `.fit()` relate to the gradient descent you saw in Module 2?\n   - What's the SAME between 2-feature XOR and 30-feature cancer diagnosis?\n\n**Q7.** Given that a simple linear model achieves ~95% accuracy, why might doctors still want a more complex model? Why might they prefer the simpler one?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Experiment extensively** - try many combinations of layers and units\n",
    "2. **Record your results** - fill in the experiment table on your answer sheet\n",
    "3. **Analyze patterns** - when does complexity help? When doesn't it?\n",
    "4. **Answer reflection questions** - think deeply about what you learned\n",
    "5. **Return to the LMS** - submit your answer sheet\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the journey from hand-built gradient descent with abstract XOR patterns to real-world medical ML with TensorFlow/Keras!\n",
    "\n",
    "**You now understand:**\n",
    "- ‚úÖ Why hidden layers help (dimension lifting)\n",
    "- ‚úÖ How gradient descent finds weights automatically\n",
    "- ‚úÖ How to build, train, and evaluate neural networks\n",
    "- ‚úÖ How to apply ML to real-world problems\n",
    "- ‚úÖ How to interpret results and understand trade-offs\n",
    "- ‚úÖ The ethical implications of ML in high-stakes domains\n",
    "\n",
    "**The principles you learned apply to:**\n",
    "- Image recognition (computer vision)\n",
    "- Natural language processing (text understanding)\n",
    "- Time series forecasting (stock prices, weather)\n",
    "- Recommender systems (Netflix, Spotify)\n",
    "- And countless other applications!\n",
    "\n",
    "**Great work! üéâ**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}