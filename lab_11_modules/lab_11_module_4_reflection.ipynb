{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 - Module 4: Reflection and Synthesis\n",
    "\n",
    "**Time:** ~10 minutes\n",
    "\n",
    "## Synthesizing Your Findings\n",
    "\n",
    "You've completed the full cycle of human-AI collaboration:\n",
    "1. âœ“ **Generate**: AI created content from your prompt\n",
    "2. âœ“ **Evaluate**: You discovered AI cannot reliably judge its own quality\n",
    "3. âœ“ **Revise**: AI improved when given explicit, rubric-based feedback\n",
    "\n",
    "Now it's time to step back and reflect on what this means for working with AI.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Synthesize findings from all three stages\n",
    "- Connect lab insights to real-world AI failures\n",
    "- Articulate when AI is a good collaborator vs. when humans must lead\n",
    "- Develop personal principles for responsible AI collaboration\n",
    "\n",
    "### What You'll Do\n",
    "\n",
    "1. Review your findings from Modules 1-3\n",
    "2. Read case studies about AI evaluation failures\n",
    "3. Complete reflection questions\n",
    "4. (Optional) Extension activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"âœ“ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all your data\n",
    "group_code = int(input(\"Enter your group code: \"))\n",
    "\n",
    "try:\n",
    "    with open(f'lab11_group_{group_code}_scenario.json', 'r') as f:\n",
    "        scenario = json.load(f)\n",
    "    with open(f'lab11_group_{group_code}_module1.json', 'r') as f:\n",
    "        module1_data = json.load(f)\n",
    "    with open(f'lab11_group_{group_code}_module2.json', 'r') as f:\n",
    "        module2_data = json.load(f)\n",
    "    with open(f'lab11_group_{group_code}_module3.json', 'r') as f:\n",
    "        module3_data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ“ Loaded all data for Group {group_code}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: Could not find all module files for group {group_code}.\")\n",
    "    print(\"Please complete Modules 0-3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Your Journey Through the Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"YOUR LAB 11 JOURNEY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Prompt Family: {scenario['family']}\")\n",
    "print(f\"AI Model Used: {module1_data['model_used']}\")\n",
    "print(f\"Rubric Selected: {module2_data['selected_rubric']}\")\n",
    "print()\n",
    "print(\"STAGE 1: GENERATE\")\n",
    "print(f\"  First Impression Rating: {module1_data['first_impression_rating']}/5\")\n",
    "print()\n",
    "print(\"STAGE 2: EVALUATE\")\n",
    "total_human = sum(module2_data['human_scores'].values())\n",
    "total_ai = sum(module2_data['ai_scores'].values())\n",
    "max_score = len(module2_data['human_scores']) * 5\n",
    "print(f\"  Your Total Score:    {total_human}/{max_score}\")\n",
    "print(f\"  AI's Self-Score:     {total_ai}/{max_score}\")\n",
    "print(f\"  Disagreement:        {total_human - total_ai:+d} points\")\n",
    "print(f\"  Weakest Criteria:    {', '.join(module2_data['weakest_criteria'])}\")\n",
    "print()\n",
    "print(\"STAGE 3: REVISE\")\n",
    "total_revised = sum(module3_data['revised_scores'].values())\n",
    "improvement = total_revised - total_human\n",
    "print(f\"  Revised Total Score: {total_revised}/{max_score} ({improvement:+d})\")\n",
    "print(f\"  Overall Judgment:    {module3_data['overall_judgment']}\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Studies: When AI Evaluation Fails\n",
    "\n",
    "The patterns you observed in your lab work reflect real-world challenges. Here are three documented cases where AI's inability to evaluate quality led to problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 1: Academic Plagiarism Detection Failure\n",
    "\n",
    "**What Happened:**\n",
    "- A university deployed an AI-powered plagiarism detector to flag student essays\n",
    "- The system flagged original student work as \"likely AI-generated\" with high confidence\n",
    "- Students were accused of cheating based on the AI's evaluation\n",
    "- Manual review revealed the AI detector was wrongâ€”the essays were genuinely student-written\n",
    "\n",
    "**The Problem:**\n",
    "- The AI evaluation tool was **overconfident** (gave high probability scores to wrong conclusions)\n",
    "- Humans trusted the AI's judgment without sufficient verification\n",
    "- The system evaluated \"AI-ness\" but couldn't explain **why** specific passages were flagged\n",
    "- Students suffered consequences before human review occurred\n",
    "\n",
    "**Connection to Your Lab:**\n",
    "- Like the AI in your Module 2, this tool couldn't accurately assess quality\n",
    "- Confidence scores didn't correlate with actual accuracy\n",
    "- Human judgment was essential but initially bypassed\n",
    "\n",
    "**Key Lesson:** AI evaluation tools, even those designed specifically for assessment, are unreliable without human oversight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 2: AI-Generated Peer Review\n",
    "\n",
    "**What Happened:**\n",
    "- Researchers submitted a deliberately **nonsense academic paper** (random jargon, no real content) to test AI review systems\n",
    "- Several AI-powered peer review tools gave the paper **high scores** and recommended acceptance\n",
    "- The AI systems praised the paper's \"clarity,\" \"methodological rigor,\" and \"contributions to the field\"\n",
    "- Human reviewers immediately recognized it as gibberish\n",
    "\n",
    "**The Problem:**\n",
    "- AI couldn't distinguish between **surface features** (academic-sounding language) and **actual quality** (coherent ideas)\n",
    "- The systems evaluated form over substance\n",
    "- High confidence scores gave false credibility to nonsense\n",
    "\n",
    "**Connection to Your Lab:**\n",
    "- Similar to how AI might score itself highly on \"Structure\" while missing that content is vague or wrong\n",
    "- AI focuses on patterns (\"sounds academic\") rather than meaning\n",
    "- Your rubric-based human evaluation caught things AI self-evaluation missed\n",
    "\n",
    "**Key Lesson:** AI can recognize stylistic patterns but often fails at deeper quality assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3: Creative Writing Competition Judging\n",
    "\n",
    "**What Happened:**\n",
    "- An online writing contest used AI to evaluate and rank creative short stories\n",
    "- The AI judges consistently gave high scores to stories with:\n",
    "  - Simple vocabulary and sentence structures\n",
    "  - Predictable narrative arcs\n",
    "  - Conventional genre tropes\n",
    "- Original, experimental, or stylistically bold stories scored poorly\n",
    "- Human judges' rankings diverged significantly from AI rankings\n",
    "\n",
    "**The Problem:**\n",
    "- AI penalized **creativity** (what makes writing interesting) while rewarding **conventionality** (what's common in training data)\n",
    "- The system couldn't evaluate subjective aesthetic qualities like \"voice,\" \"emotional impact,\" or \"originality\"\n",
    "- Quantifiable metrics (sentence length, word frequency) don't capture literary quality\n",
    "\n",
    "**Connection to Your Lab:**\n",
    "- If you used the Creative/Narrative rubric, you likely struggled with criteria like \"Engagement\" and \"Creativity\"\n",
    "- AI self-evaluation might miss what makes content **distinctive**\n",
    "- Some qualities require human judgment and taste\n",
    "\n",
    "**Key Lesson:** For subjective, creative domains, AI evaluation is particularly unreliableâ€”human judgment is irreplaceable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When AI Is a Good Collaborator (and When It's Not)\n",
    "\n",
    "Based on your lab experience and these case studies, here's a framework for human-AI collaboration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… AI Excels At:\n",
    "\n",
    "1. **Generating drafts quickly**\n",
    "   - As you saw in Module 1, AI can produce content fast\n",
    "   - Useful for brainstorming, overcoming writer's block, creating first drafts\n",
    "\n",
    "2. **Revising when given specific feedback**\n",
    "   - Module 3 likely showed improvement when you gave clear, rubric-based guidance\n",
    "   - AI can iterate and adjust based on explicit criteria\n",
    "\n",
    "3. **Following structural constraints**\n",
    "   - Word count, format requirements, stylistic guidelines\n",
    "   - AI is good at \"fill-in-the-blank\" tasks with clear rules\n",
    "\n",
    "4. **Offering alternative phrasings**\n",
    "   - Can suggest different ways to express an idea\n",
    "   - Useful for editing and refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âŒ AI Struggles With:\n",
    "\n",
    "1. **Evaluating its own quality**\n",
    "   - Module 2 likely revealed significant disagreements between your judgment and AI's self-scores\n",
    "   - AI cannot reliably tell when it's done well or poorly\n",
    "\n",
    "2. **Judging subjective qualities**\n",
    "   - Creativity, originality, emotional impact, aesthetic appeal\n",
    "   - These require human taste and context\n",
    "\n",
    "3. **Understanding audience needs deeply**\n",
    "   - AI may guess what an audience wants but lacks genuine empathy or cultural context\n",
    "   - Can miss nuance in \"Audience Fit\"\n",
    "\n",
    "4. **Recognizing when to refuse**\n",
    "   - AI generates even when it shouldn't (factual errors, hallucinations, inappropriate content)\n",
    "   - Lacks judgment about when NOT to produce\n",
    "\n",
    "5. **Maintaining ethical standards**\n",
    "   - Can't reliably evaluate \"Ethics\" criterion without human oversight\n",
    "   - May produce manipulative, misleading, or biased content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤ Best Practices for Human-AI Collaboration:\n",
    "\n",
    "1. **Use AI as a first-draft generator, not a final product**\n",
    "   - Start with AI output but assume significant revision needed\n",
    "\n",
    "2. **Apply explicit rubrics and standards**\n",
    "   - As you did in Module 2, define \"good\" clearly before evaluating\n",
    "   - AI works better when criteria are concrete\n",
    "\n",
    "3. **Iterate with human judgment**\n",
    "   - Module 3's cycle: human evaluates â†’ gives specific feedback â†’ AI revises\n",
    "   - Keep human in control of quality standards\n",
    "\n",
    "4. **Never trust AI self-evaluation alone**\n",
    "   - Always verify with independent judgment or external sources\n",
    "   - Especially critical for high-stakes contexts\n",
    "\n",
    "5. **Recognize domain-specific limitations**\n",
    "   - In creative, ethical, or specialized domains, increase human oversight\n",
    "   - AI is a tool, not an expert\n",
    "\n",
    "6. **Document your human contributions**\n",
    "   - In academic or professional work, be transparent about AI use\n",
    "   - Emphasize your evaluation, judgment, and revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "Answer these on your Lab 11 Answer Sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18: Biggest Disagreement (Review)\n",
    "\n",
    "Looking back at Module 2, where did you **most disagree** with the AI's self-evaluation? Why do you think this disagreement occurred?\n",
    "\n",
    "Was it a case where AI:\n",
    "- Overrated itself (gave higher scores than deserved)?\n",
    "- Underrated itself (gave lower scores than deserved)?\n",
    "- Focused on the wrong aspects of quality?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19: AI as a Creative Assistant\n",
    "\n",
    "What did the AI do **well** as a creative assistant in this lab?\n",
    "\n",
    "Consider:\n",
    "- Initial generation in Module 1\n",
    "- Following revision instructions in Module 3\n",
    "- Specific strengths you identified\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20: What AI Should Never Do Alone\n",
    "\n",
    "Based on your experience, what should AI **never be solely responsible for** in a creative or evaluation workflow?\n",
    "\n",
    "List at least 2-3 tasks where human oversight is essential.\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q21: Teacher Scenario\n",
    "\n",
    "Imagine you're a teacher. A colleague suggests using AI to grade student essays automatically, with AI providing scores and feedback.\n",
    "\n",
    "Would you trust this system? Why or why not?\n",
    "\n",
    "If you would use it, what safeguards or human oversight would you require?\n",
    "\n",
    "If you wouldn't use it, what specific risks concern you most?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q22: Personal AI Collaboration Principles\n",
    "\n",
    "How will you use AI as a collaborator in the future?\n",
    "\n",
    "Write 3-5 **personal principles** for responsible AI collaboration based on what you learned in this lab.\n",
    "\n",
    "Format: \"I will...\" or \"I will not...\"\n",
    "\n",
    "Examples:\n",
    "- \"I will always verify AI-generated facts before using them in my work\"\n",
    "- \"I will not trust AI self-evaluation without independent judgment\"\n",
    "- \"I will use explicit rubrics when evaluating AI-generated content\"\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extension Activities\n",
    "\n",
    "If you finish early or want to explore further:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 1: Cross-Group Rubric Swap\n",
    "\n",
    "**Q23 (Optional):** Swap rubrics with another group. Score your AI's output using **their rubric** instead of yours.\n",
    "\n",
    "Do the scores change significantly?\n",
    "Does a different rubric reveal different strengths/weaknesses?\n",
    "What does this tell you about the importance of **defining evaluation criteria**?\n",
    "\n",
    "*(Answer on your answer sheet if completed)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 2: Multi-Model Comparison\n",
    "\n",
    "**Q24 (Optional):** Take your original prompt (from Module 0) and run it on a **different AI model**.\n",
    "\n",
    "For example:\n",
    "- If you used ChatGPT, try Claude or Gemini\n",
    "- If you used Claude, try ChatGPT or Gemini\n",
    "\n",
    "Score the new model's output using the same rubric.\n",
    "\n",
    "Compare:\n",
    "- Which model scored higher? On which criteria?\n",
    "- Did different models have different strengths/weaknesses?\n",
    "- Did their self-evaluations differ in accuracy?\n",
    "\n",
    "*(Answer on your answer sheet if completed)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Accomplished\n",
    "\n",
    "### Complete Lab Journey:\n",
    "\n",
    "âœ… **Module 0:** Generated a unique creative prompt deterministically\n",
    "\n",
    "âœ… **Module 1:** Used AI to generate content; identified initial strengths and weaknesses\n",
    "\n",
    "âœ… **Module 2:** Applied explicit rubrics; discovered AI cannot reliably self-evaluate\n",
    "\n",
    "âœ… **Module 3:** Guided AI revision with rubric feedback; compared improvement systematically\n",
    "\n",
    "âœ… **Module 4:** Synthesized findings; connected to real-world cases; developed collaboration principles\n",
    "\n",
    "### Core Insights:\n",
    "\n",
    "1. **Generation is easier than evaluation**\n",
    "   - AI can produce content quickly\n",
    "   - But assessing quality requires human judgment\n",
    "\n",
    "2. **AI self-evaluation is unreliable**\n",
    "   - Confidence doesn't equal accuracy\n",
    "   - Human-AI disagreement is common and expected\n",
    "\n",
    "3. **Explicit standards improve collaboration**\n",
    "   - Rubrics make \"good\" concrete\n",
    "   - Clear feedback enables better AI revision\n",
    "\n",
    "4. **Humans must maintain oversight**\n",
    "   - AI is a tool, not an authority\n",
    "   - Critical judgment, ethics, and creativity require human involvement\n",
    "\n",
    "### Connections to Previous Labs:\n",
    "\n",
    "**Lab 1-2 (Optimization):**\n",
    "- Evaluation provides the \"error signal\" for improvement\n",
    "- AI can optimize toward a target but needs humans to define what's \"good\"\n",
    "\n",
    "**Lab 4 (Neural Networks):**\n",
    "- Training requires labeled dataâ€”humans must provide ground truth\n",
    "- AI learns from examples but doesn't inherently know quality\n",
    "\n",
    "**Lab 6 (Explainability):**\n",
    "- AI explanations help but don't replace human judgment\n",
    "- Understanding how AI works â‰  trusting it blindly\n",
    "\n",
    "**Lab 10 (Hallucination):**\n",
    "- **Lab 10:** AI can't predict when it will fail (self-assessment)\n",
    "- **Lab 11:** AI can't evaluate when its output is good (self-evaluation)\n",
    "- Both explore limits of AI meta-cognition\n",
    "\n",
    "### Real-World Application:\n",
    "\n",
    "This lab prepares you to:\n",
    "- âœ“ Use AI productively as a creative collaborator\n",
    "- âœ“ Maintain critical oversight and judgment\n",
    "- âœ“ Apply structured evaluation methods (rubrics)\n",
    "- âœ“ Iterate effectively with AI assistance\n",
    "- âœ“ Recognize when AI should and shouldn't be trusted\n",
    "\n",
    "### Final Takeaway:\n",
    "\n",
    "> **AI is an excellent tool for generation and revision when paired with human evaluation and judgment. The future of AI collaboration is not \"AI replaces humans\" but \"humans guide AI toward human-defined standards of quality.\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've completed Lab 11: Human-AI Collaboration for Creative Problem Solving.\n",
    "\n",
    "You now understand:\n",
    "- When to trust AI\n",
    "- When to maintain human oversight\n",
    "- How to collaborate effectively with AI systems\n",
    "- Why evaluation is harderâ€”and more importantâ€”than generation\n",
    "\n",
    "**Thank you for your thoughtful engagement with this lab!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
