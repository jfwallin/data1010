{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 - Module 1: Collecting AI Self-Predictions\n",
    "\n",
    "**Time:** ~15-20 minutes\n",
    "\n",
    "In this module, you'll test your 8 prompts on a real AI model and record:\n",
    "- How confident the AI sounds (tone, caveats, hedging)\n",
    "- Your prediction of whether the response is actually accurate\n",
    "- Specific phrases the AI uses to express certainty or uncertainty\n",
    "\n",
    "**Important:** You will NOT verify accuracy yet‚Äîthat happens in Module 2. For now, just observe and record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML, Markdown\nimport json\nimport os\n\nprint(\"‚úì Libraries loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "source": "# üìÅ Google Drive Setup\n\n**IMPORTANT:** This module loads data from your Google Drive (saved in Module 0).\n\nRun the cell below to mount Google Drive. You may need to authorize access again.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive\nfrom google.colab import drive\n\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive')\n\n# Set lab directory\nLAB_DIR = '/content/drive/MyDrive/DATA1010/Lab10'\n\nprint(\"‚úì Google Drive mounted successfully!\")\nprint(f\"‚úì Lab directory: {LAB_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Your Group Code\n",
    "\n",
    "Use the **same group code** from Module 0 to load your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "group_code = int(input(\"Enter your group code: \"))\n\n# Load prompts from Module 0 (from Google Drive)\nfilename = f\"{LAB_DIR}/lab10_group_{group_code}_prompts.csv\"\n\ntry:\n    prompts_df = pd.read_csv(filename)\n    print(f\"‚úì Loaded {len(prompts_df)} prompts from Google Drive\")\n    print(f\"‚úì Group Code: {group_code}\")\nexcept FileNotFoundError:\n    print(f\"‚ùå ERROR: Could not find {filename}\")\n    print(\"‚ùå Make sure you:\")\n    print(\"   1. Ran Module 0 first\")\n    print(\"   2. Used the same group code\")\n    print(\"   3. Mounted Google Drive (run the cell above)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Instructions\n",
    "\n",
    "### How to Test Each Prompt\n",
    "\n",
    "For each of the 8 prompts, follow these steps:\n",
    "\n",
    "1. **Open a fresh AI chat window**\n",
    "   - Use ChatGPT, Claude, Gemini, or another LLM\n",
    "   - Open a **private/incognito window** for each prompt\n",
    "   - This ensures no context from previous prompts\n",
    "\n",
    "2. **Copy and paste the prompt exactly**\n",
    "   - Don't modify it or add context\n",
    "   - Just paste the prompt text and send\n",
    "\n",
    "3. **Read the AI's complete response**\n",
    "   - Pay attention to tone (confident, cautious, uncertain)\n",
    "   - Note any caveats (\"I might be wrong\", \"Please verify\")\n",
    "   - Look for hedging language (\"typically\", \"generally\", \"often\")\n",
    "\n",
    "4. **Record your observations using the widgets below**\n",
    "   - AI's confidence level (dropdown)\n",
    "   - Your prediction of accuracy (dropdown)\n",
    "   - Specific phrases (text area)\n",
    "\n",
    "5. **Close the window and move to the next prompt**\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "**High confidence signals:**\n",
    "- \"Definitely\", \"certainly\", \"without a doubt\"\n",
    "- No caveats or warnings\n",
    "- Specific details (numbers, dates, names)\n",
    "- Authoritative tone\n",
    "\n",
    "**Uncertainty signals:**\n",
    "- \"I might be wrong\", \"I'm not certain\"\n",
    "- \"You should verify this\", \"Please double-check\"\n",
    "- \"This may vary\", \"typically\", \"generally\"\n",
    "- Refusal to answer\n",
    "\n",
    "### Time Budget\n",
    "\n",
    "Spend about **2 minutes per prompt**:\n",
    "- 30 seconds: Open window and paste prompt\n",
    "- 30 seconds: Read response\n",
    "- 1 minute: Record observations\n",
    "\n",
    "**Total: ~16 minutes for all 8 prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Interface\n",
    "\n",
    "Run this cell to start the interactive data collection process.\n",
    "\n",
    "You'll see each prompt one at a time with dropdowns and text areas to record your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data storage\n",
    "predictions_data = []\n",
    "\n",
    "# Confidence level options\n",
    "confidence_options = [\n",
    "    'Select...',\n",
    "    'No caveats - answered confidently',\n",
    "    'Mild caveats (e.g., \"This might...\", \"Generally...\")',\n",
    "    'Strong caveats (e.g., \"I may be wrong\", \"Please verify\")',\n",
    "    'Refused or heavily qualified the answer'\n",
    "]\n",
    "\n",
    "# Student prediction options\n",
    "prediction_options = [\n",
    "    'Select...',\n",
    "    'Will be accurate',\n",
    "    'Might have minor errors',\n",
    "    'Likely to have major errors',\n",
    "    'Completely failed/refused'\n",
    "]\n",
    "\n",
    "def create_prompt_interface(idx, row):\n",
    "    \"\"\"Create widgets for a single prompt.\"\"\"\n",
    "    \n",
    "    # Display prompt information\n",
    "    print(\"=\"*70)\n",
    "    print(f\"PROMPT #{row['prompt_id']} of {len(prompts_df)}\")\n",
    "    print(f\"Category: {row['category_display']}\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"COPY THIS PROMPT TO YOUR AI:\")\n",
    "    print(\"-\"*70)\n",
    "    print(row['prompt_text'])\n",
    "    print(\"-\"*70)\n",
    "    print()\n",
    "    print(\"After pasting into AI and reading the response, record your observations below:\")\n",
    "    print()\n",
    "    \n",
    "    # Create widgets\n",
    "    confidence_widget = widgets.Dropdown(\n",
    "        options=confidence_options,\n",
    "        value='Select...',\n",
    "        description='AI Confidence:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '650px'}\n",
    "    )\n",
    "    \n",
    "    prediction_widget = widgets.Dropdown(\n",
    "        options=prediction_options,\n",
    "        value='Select...',\n",
    "        description='Your Prediction:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '650px'}\n",
    "    )\n",
    "    \n",
    "    notes_widget = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Optional: Record specific phrases the AI used (e.g., \"I am confident that...\", \"This might vary...\", etc.)',\n",
    "        description='Notes/Quotes:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '650px', 'height': '100px'}\n",
    "    )\n",
    "    \n",
    "    save_button = widgets.Button(\n",
    "        description=f'Save and Continue to Prompt #{row[\"prompt_id\"] + 1}' if idx < len(prompts_df) - 1 else 'Save Final Prompt',\n",
    "        button_style='success',\n",
    "        layout={'width': '300px'}\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_save(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Validate inputs\n",
    "            if confidence_widget.value == 'Select...' or prediction_widget.value == 'Select...':\n",
    "                print(\"‚ö†Ô∏è Please select values for both dropdowns before saving.\")\n",
    "                return\n",
    "            \n",
    "            # Save data\n",
    "            predictions_data.append({\n",
    "                'prompt_id': row['prompt_id'],\n",
    "                'category': row['category'],\n",
    "                'ai_confidence': confidence_widget.value,\n",
    "                'student_prediction': prediction_widget.value,\n",
    "                'notes': notes_widget.value\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì Prompt #{row['prompt_id']} saved!\")\n",
    "            print(f\"Progress: {len(predictions_data)}/{len(prompts_df)} prompts completed\")\n",
    "            \n",
    "            if idx < len(prompts_df) - 1:\n",
    "                print(f\"\\n‚Üí Scroll down for Prompt #{row['prompt_id'] + 1}\")\n",
    "            else:\n",
    "                print(\"\\n‚úì All prompts completed!\")\n",
    "                print(\"‚Üí Scroll down to save your data.\")\n",
    "    \n",
    "    save_button.on_click(on_save)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(confidence_widget)\n",
    "    display(prediction_widget)\n",
    "    display(notes_widget)\n",
    "    display(save_button)\n",
    "    display(output)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "# Display interface for each prompt\n",
    "for idx, row in prompts_df.iterrows():\n",
    "    create_prompt_interface(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your Predictions\n",
    "\n",
    "After completing all 8 prompts above, run this cell to save your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(predictions_data) < len(prompts_df):\n    print(f\"‚ö†Ô∏è WARNING: You've only completed {len(predictions_data)}/{len(prompts_df)} prompts.\")\n    print(\"Please complete all prompts above before saving.\")\nelse:\n    # Create DataFrame\n    predictions_df = pd.DataFrame(predictions_data)\n    \n    # Save to Google Drive\n    predictions_filename = f\"{LAB_DIR}/lab10_group_{group_code}_predictions.csv\"\n    predictions_df.to_csv(predictions_filename, index=False)\n    \n    # Display summary\n    print(\"=\"*70)\n    print(\"‚úì Data Saved Successfully to Google Drive!\")\n    print(\"=\"*70)\n    print(f\"File: {predictions_filename}\")\n    print(f\"Prompts completed: {len(predictions_df)}\")\n    print()\n    print(\"Summary of AI Confidence Levels:\")\n    print(predictions_df['ai_confidence'].value_counts())\n    print()\n    print(\"Summary of Your Predictions:\")\n    print(predictions_df['student_prediction'].value_counts())\n    print(\"=\"*70)\n    print()\n    print(\"Next Steps:\")\n    print(\"1. Answer Q4-Q7 on your lab handout\")\n    print(\"2. Continue to Module 2 to verify actual accuracy\")\n    print(\"3. Use the same group code in Module 2!\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Your Data\n",
    "\n",
    "Optional: View the data you just collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(predictions_data) == len(prompts_df):\n",
    "    display(HTML(predictions_df.to_html(index=False, classes='table table-striped')))\n",
    "else:\n",
    "    print(\"Complete all prompts and save data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Module 1\n",
    "\n",
    "Answer these questions on your lab handout using the data you just collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Uncertainty Language in Prompt #1\n",
    "\n",
    "Looking at Prompt #1, did the AI express any uncertainty or caveats? Quote specific phrases from the response.\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Refusals and Strong Uncertainty\n",
    "\n",
    "For which prompt(s) did the AI refuse to answer or express strong uncertainty? List the prompt ID numbers and categories.\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Variation in Confidence\n",
    "\n",
    "Did the AI use similar language for all prompts, or did confidence levels vary across different categories? Give specific examples.\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Predicting Overconfidence\n",
    "\n",
    "PREDICTION: Looking at the 8 responses you collected, for which prompts do you think the AI's self-assessment will be accurate? Which ones do you suspect might show overconfidence (confident tone but actually wrong)?\n",
    "\n",
    "*(Answer on your handout)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q4-Q7** on your lab handout\n",
    "2. **Remember your group code:** (write it down again!)\n",
    "3. **Continue to Module 2** where you'll verify the actual accuracy of each response\n",
    "4. **Use the same group code** in Module 2\n",
    "\n",
    "In Module 2, you'll discover whether the AI's confidence matched reality!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}