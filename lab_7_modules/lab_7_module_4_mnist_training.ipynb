{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7, Module 4: Training a CNN on MNIST\n",
    "\n",
    "**Estimated time:** 15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: From Pretrained to Training Your Own**\n",
    "\n",
    "In Modules 1-3, you:\n",
    "- Applied hand-designed filters (Sobel, blur, sharpen)\n",
    "- Explored a pretrained CNN (MobileNetV2)\n",
    "- Learned about hierarchical feature extraction\n",
    "\n",
    "**Now it's time to see the magic happen:**\n",
    "\n",
    "You'll **train your own CNN from scratch**‚Äîwatching it learn to:\n",
    "1. Detect edges in handwritten digits\n",
    "2. Combine edges into shapes\n",
    "3. Recognize digits 0-9\n",
    "\n",
    "**Best part?** This takes only **2-3 minutes** on a regular CPU!\n",
    "\n",
    "### **Why This Matters**\n",
    "\n",
    "Training a CNN demonstrates:\n",
    "- **CNNs aren't magic**‚Äîthey're just optimization via gradient descent\n",
    "- **Learned filters** emerge naturally (similar to Sobel, but optimized for digits)\n",
    "- **Fast training** shows CNNs are practical for real applications\n",
    "- **High accuracy** (>98%) shows hierarchical features work!\n",
    "\n",
    "### **Connection to Lab 4**\n",
    "\n",
    "Remember Lab 4, where you trained neural networks on Iris and Breast Cancer datasets?\n",
    "\n",
    "| Aspect | Lab 4 (Dense Networks) | Lab 7 (CNNs) |\n",
    "|--------|------------------------|---------------|\n",
    "| **Data type** | Tabular (features in columns) | Images (28√ó28 pixels) |\n",
    "| **Architecture** | Fully-connected layers | Convolutional + dense layers |\n",
    "| **Training** | Gradient descent + backprop | Same! |\n",
    "| **Loss function** | Cross-entropy | Same! |\n",
    "| **Optimizer** | Adam | Same! |\n",
    "\n",
    "**Key insight:** CNNs use the same training process as dense networks‚Äîjust a different architecture!\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **About the MNIST Dataset**\n",
    "\n",
    "**MNIST** = Modified National Institute of Standards and Technology\n",
    "\n",
    "### **Dataset Details:**\n",
    "- **60,000 training images** (handwritten digits 0-9)\n",
    "- **10,000 test images**\n",
    "- **28√ó28 pixels**, grayscale (1 channel)\n",
    "- **10 classes** (digits 0, 1, 2, ..., 9)\n",
    "\n",
    "### **Why MNIST?**\n",
    "- Classic benchmark dataset (since 1998)\n",
    "- Small enough to train quickly (2-3 minutes on CPU)\n",
    "- Complex enough to need a CNN (>98% accuracy is hard without convolution)\n",
    "- Real-world application: Check reading, postal mail sorting\n",
    "\n",
    "### **Historical Context:**\n",
    "- 1998: LeNet-5 (first CNN) achieved 99.2% on MNIST\n",
    "- 2012: Dropout improved to 99.5%\n",
    "- Today: State-of-the-art reaches 99.8% (only 20 errors out of 10,000!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• **Load and Explore MNIST Data**\n",
    "\n",
    "Let's load the dataset and see what handwritten digits look like!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset (built into Keras)\n",
    "print(\"Loading MNIST dataset...\\n\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"‚úÖ Dataset loaded!\\n\")\n",
    "print(f\"Training set: {x_train.shape[0]} images\")\n",
    "print(f\"Test set: {x_test.shape[0]} images\")\n",
    "print(f\"Image shape: {x_train.shape[1]} √ó {x_train.shape[2]} pixels\")\n",
    "print(f\"\\nClass distribution (training):\")\n",
    "for digit in range(10):\n",
    "    count = np.sum(y_train == digit)\n",
    "    print(f\"  Digit {digit}: {count} images ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for digit in range(10):\n",
    "    # Find first example of this digit\n",
    "    idx = np.where(y_train == digit)[0][0]\n",
    "    \n",
    "    # Display\n",
    "    axes[digit].imshow(x_train[idx], cmap='gray')\n",
    "    axes[digit].set_title(f'Digit: {digit}', fontsize=12, fontweight='bold')\n",
    "    axes[digit].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample MNIST Digits (One Example per Class)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Handwriting varies widely!\")\n",
    "print(\"  - Different stroke widths\")\n",
    "print(\"  - Different slants and orientations\")\n",
    "print(\"  - Different styles (loopy vs. straight)\")\n",
    "print(\"\\nThis is why we need a CNN‚Äîsimple rules won't work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è **Preprocess the Data**\n",
    "\n",
    "Before training, we need to:\n",
    "1. **Reshape** images to add channel dimension (28, 28, 1)\n",
    "2. **Normalize** pixel values to [0, 1] (currently [0, 255])\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to add channel dimension\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print(\"‚úÖ Data preprocessed!\\n\")\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"  (samples, height, width, channels) = {x_train.shape}\")\n",
    "print(f\"\\nTest data shape: {x_test.shape}\")\n",
    "print(f\"\\nPixel value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è **Build the CNN Architecture**\n",
    "\n",
    "We'll create a simple but effective CNN:\n",
    "\n",
    "### **Architecture:**\n",
    "```\n",
    "Input: (28, 28, 1)\n",
    "    ‚Üì\n",
    "Conv2D: 32 filters, 3√ó3, ReLU\n",
    "    ‚Üì\n",
    "MaxPooling: 2√ó2\n",
    "    ‚Üì\n",
    "Conv2D: 64 filters, 3√ó3, ReLU\n",
    "    ‚Üì\n",
    "MaxPooling: 2√ó2\n",
    "    ‚Üì\n",
    "Flatten\n",
    "    ‚Üì\n",
    "Dense: 128 units, ReLU\n",
    "    ‚Üì\n",
    "Dense: 10 units, Softmax\n",
    "    ‚Üì\n",
    "Output: Class probabilities (0-9)\n",
    "```\n",
    "\n",
    "### **What Each Layer Does:**\n",
    "- **Conv2D:** Applies convolution filters to detect patterns\n",
    "- **ReLU:** Activation function (introduces nonlinearity)\n",
    "- **MaxPooling:** Downsamples by taking maximum value in 2√ó2 window\n",
    "- **Flatten:** Converts 2D feature maps to 1D vector\n",
    "- **Dense:** Fully-connected layer (like Lab 4!)\n",
    "- **Softmax:** Converts outputs to probabilities (sum to 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    \n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(128, activation='relu', name='dense1'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "print(\"üìã Model Architecture:\\n\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nüìä Total parameters: {total_params:,}\")\n",
    "print(\"\\n(Compare to a fully-connected network: millions of parameters!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q20 (Prediction)**\n",
    "\n",
    "### **Q20. Before training, predict: What accuracy do you expect on MNIST?**\n",
    "\n",
    "*Consider these options:*\n",
    "- **10%** (random guessing: 1 out of 10 classes)\n",
    "- **50%** (better than random, but not great)\n",
    "- **90%** (very good)\n",
    "- **99%** (near-perfect)\n",
    "\n",
    "*What do you think is realistic for a simple CNN after just 3-5 epochs of training?*\n",
    "\n",
    "**Record your prediction in the Answer Sheet BEFORE continuing!**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Compile the Model**\n",
    "\n",
    "We need to specify:\n",
    "1. **Optimizer:** Adam (adaptive learning rate)\n",
    "2. **Loss function:** Sparse categorical cross-entropy (for multi-class classification)\n",
    "3. **Metrics:** Accuracy (% of correct predictions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model compiled and ready to train!\")\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(\"  - Optimizer: Adam\")\n",
    "print(\"  - Loss: Sparse categorical cross-entropy\")\n",
    "print(\"  - Metric: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ **Train the Model!**\n",
    "\n",
    "Time to train! This will take **2-3 minutes** on CPU.\n",
    "\n",
    "Watch the accuracy improve with each epoch!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "print(\"This will take ~2-3 minutes on CPU.\")\n",
    "print(\"Watch the accuracy improve with each epoch!\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', marker='o', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  ‚úì Accuracy increased each epoch\")\n",
    "print(\"  ‚úì Loss decreased each epoch\")\n",
    "print(\"  ‚úì Validation accuracy close to training accuracy (no overfitting!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **Evaluate on Test Set**\n",
    "\n",
    "Now let's see how well the model performs on completely unseen data!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nOut of {len(y_test)} test images:\")\n",
    "print(f\"  ‚úì Correct: {int(test_acc * len(y_test))}\")\n",
    "print(f\"  ‚úó Incorrect: {int((1-test_acc) * len(y_test))}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí° Context:\")\n",
    "print(\"  - Random guessing: 10% accuracy\")\n",
    "print(\"  - Simple dense network: ~85% accuracy\")\n",
    "print(f\"  - Our simple CNN: {test_acc*100:.2f}% accuracy\")\n",
    "print(\"  - State-of-the-art: 99.8% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q21 (Observation)**\n",
    "\n",
    "### **Q21. After training for 3 epochs, what test accuracy did you achieve? Was this higher or lower than your prediction from Q20?**\n",
    "\n",
    "*Look at the test accuracy above. Were you surprised by the result? Why or why not?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **Confusion Matrix**\n",
    "\n",
    "Remember confusion matrices from Lab 4? Let's see which digits are confused with each other!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_probs = model.predict(x_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Digit', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Digit', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix: MNIST Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HOW TO READ THIS MATRIX:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n- Diagonal (dark blue): Correct predictions\")\n",
    "print(\"- Off-diagonal (lighter): Misclassifications\\n\")\n",
    "print(\"Example: If row 5, column 3 has value 8, that means:\")\n",
    "print(\"  ‚Üí 8 images of digit 5 were incorrectly classified as digit 3\\n\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most common confusions\n",
    "print(\"\\nüìä MOST COMMON CONFUSIONS:\\n\")\n",
    "\n",
    "# Set diagonal to 0 to ignore correct predictions\n",
    "cm_off_diag = cm.copy()\n",
    "np.fill_diagonal(cm_off_diag, 0)\n",
    "\n",
    "# Find top 5 confusions\n",
    "top_confusions = []\n",
    "for true_digit in range(10):\n",
    "    for pred_digit in range(10):\n",
    "        if true_digit != pred_digit and cm[true_digit, pred_digit] > 0:\n",
    "            top_confusions.append((cm[true_digit, pred_digit], true_digit, pred_digit))\n",
    "\n",
    "top_confusions.sort(reverse=True)\n",
    "\n",
    "for i, (count, true_digit, pred_digit) in enumerate(top_confusions[:5], 1):\n",
    "    print(f\"{i}. {count} times: Digit '{true_digit}' misclassified as '{pred_digit}'\")\n",
    "\n",
    "print(\"\\nüí° Why might these confusions happen?\")\n",
    "print(\"  - Similar shapes (e.g., 4 and 9, 3 and 8, 5 and 6)\")\n",
    "print(\"  - Handwriting variations\")\n",
    "print(\"  - Ambiguous examples (even humans would struggle!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q22 (Analysis)**\n",
    "\n",
    "### **Q22. Looking at the confusion matrix, which digits are most commonly confused with each other? Why might this be?**\n",
    "\n",
    "*Hint: Look at the off-diagonal cells with the highest values. Do the confused digits look similar? Think about their shapes.*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ **Visualize Learned Filters**\n",
    "\n",
    "Remember Module 2, where you saw pretrained filters from MobileNetV2?\n",
    "\n",
    "**Now let's see what YOUR CNN learned!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned filters from first convolutional layer\n",
    "conv1_weights = model.get_layer('conv1').get_weights()[0]  # Shape: (3, 3, 1, 32)\n",
    "\n",
    "print(f\"First convolutional layer learned {conv1_weights.shape[-1]} filters\")\n",
    "print(f\"Each filter is {conv1_weights.shape[0]}√ó{conv1_weights.shape[1]} pixels\\n\")\n",
    "\n",
    "# Visualize all 32 filters\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(32):\n",
    "    filter_img = conv1_weights[:, :, 0, i]  # Extract filter i\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    vmin, vmax = filter_img.min(), filter_img.max()\n",
    "    \n",
    "    axes[i].imshow(filter_img, cmap='RdBu', vmin=vmin, vmax=vmax)\n",
    "    axes[i].set_title(f'Filter {i+1}', fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Learned Filters from First Convolutional Layer (32 filters, 3√ó3 each)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT TO LOOK FOR:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n- Edge detectors (similar to Sobel filters from Module 1!)\")\n",
    "print(\"- Diagonal patterns\")\n",
    "print(\"- Corner detectors\")\n",
    "print(\"- Gradient patterns\\n\")\n",
    "print(\"Notice: The network learned these filters automatically through\")\n",
    "print(\"gradient descent‚Äîno human design required!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q23 (Observation)**\n",
    "\n",
    "### **Q23. Examine the learned filters from the first convolutional layer. Do they look like edge detectors (similar to Sobel filters from Module 1)?**\n",
    "\n",
    "*Hint: Compare these learned filters to the Sobel vertical and horizontal edge detectors you saw in Module 1. What patterns do you recognize?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Connection to Lab 4: Dense Networks vs. CNNs**\n",
    "\n",
    "Let's compare what you did in Lab 4 to what you just did now!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Lab 4 vs. Lab 7 Comparison**\n",
    "\n",
    "| Aspect | Lab 4 (Breast Cancer) | Lab 7 (MNIST) |\n",
    "|--------|-----------------------|---------------|\n",
    "| **Data type** | Tabular (30 features) | Images (28√ó28 pixels) |\n",
    "| **Input shape** | (30,) | (28, 28, 1) |\n",
    "| **Architecture** | Dense ‚Üí Dense ‚Üí Output | Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí Dense ‚Üí Output |\n",
    "| **Parameters** | ~4,000 | ~100,000 |\n",
    "| **Training time** | <1 minute | ~2-3 minutes |\n",
    "| **Training method** | Gradient descent | Same! |\n",
    "| **Loss function** | Binary cross-entropy | Sparse categorical cross-entropy |\n",
    "| **Optimizer** | Adam | Adam |\n",
    "| **Visualization** | ROC curve | Confusion matrix |\n",
    "| **Key concept** | Hidden layers learn representations | Conv layers learn spatial features |\n",
    "\n",
    "### **Key Similarities:**\n",
    "- Both use gradient descent + backpropagation\n",
    "- Both use Adam optimizer\n",
    "- Both use cross-entropy loss\n",
    "- Both achieved high accuracy (>95%)\n",
    "\n",
    "### **Key Differences:**\n",
    "- **Data structure:** Tabular vs. spatial (images)\n",
    "- **Architecture:** Fully-connected vs. convolutional\n",
    "- **Parameters:** Dense layers have many more parameters for images\n",
    "- **Inductive bias:** CNNs assume local spatial patterns matter\n",
    "\n",
    "### **When to Use Each:**\n",
    "- **Dense networks (Lab 4):** Tabular data, sensor readings, feature vectors\n",
    "- **CNNs (Lab 7):** Images, videos, spatial data, anything with local patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Question Q24 (Synthesis)**\n",
    "\n",
    "### **Q24. Compare this CNN training to the Breast Cancer classifier from Lab 4. What's similar? What's different?**\n",
    "\n",
    "*Think about: training process, architecture, data type, accuracy, time to train, etc.*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **Analyze Misclassified Examples**\n",
    "\n",
    "Let's look at examples the model got wrong. Sometimes these are genuinely ambiguous!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "incorrect_indices = np.where(y_pred != y_test)[0]\n",
    "\n",
    "print(f\"Found {len(incorrect_indices)} misclassified examples out of {len(y_test)}\")\n",
    "print(f\"Error rate: {len(incorrect_indices)/len(y_test)*100:.2f}%\\n\")\n",
    "\n",
    "# Show first 12 misclassified examples\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(12, len(incorrect_indices))):\n",
    "    idx = incorrect_indices[i]\n",
    "    \n",
    "    # Get image, true label, predicted label\n",
    "    img = x_test[idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred[idx]\n",
    "    confidence = y_pred_probs[idx][pred_label] * 100\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'True: {true_label}, Predicted: {pred_label}\\nConfidence: {confidence:.1f}%',\n",
    "                     fontsize=10, color='red', fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Misclassified Examples (What the Model Got Wrong)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° OBSERVATIONS:\")\n",
    "print(\"  - Some examples are genuinely ambiguous (even humans might struggle!)\")\n",
    "print(\"  - Poor handwriting quality\")\n",
    "print(\"  - Unusual writing styles\")\n",
    "print(\"  - Digits that look similar (4 vs. 9, 3 vs. 8, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù **Question Q25 (Critical Thinking)**\n",
    "\n",
    "### **Q25. Find 2-3 misclassified examples above. Can you understand why the model got them wrong? Are they ambiguous even to you?**\n",
    "\n",
    "*Look at the images carefully. Record:*\n",
    "1. True label and predicted label\n",
    "2. Why you think the model made the mistake\n",
    "3. Whether you would have classified it correctly\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## üåâ **Bridge to Lab 8: From Analysis to Synthesis**\n",
    "\n",
    "### **What You've Learned in Lab 7:**\n",
    "- CNNs **extract features** from images (edges ‚Üí textures ‚Üí shapes ‚Üí objects)\n",
    "- Hierarchical architecture builds complex patterns from simple ones\n",
    "- Learned filters emerge automatically through training\n",
    "- **Analysis task:** Given an image, identify what's in it\n",
    "\n",
    "### **What's Coming in Lab 8: Diffusion Models**\n",
    "- **Synthesis task:** Given a description (or noise), **generate** an image\n",
    "- Diffusion models **create images from noise**\n",
    "- Similar architecture (U-Net uses convolution!), opposite direction\n",
    "- Powers DALL-E, Stable Diffusion, Midjourney\n",
    "\n",
    "### **The Connection:**\n",
    "```\n",
    "Lab 7 (CNNs): Image ‚Üí Features ‚Üí Classification\n",
    "              (Analysis: \"What is this?\")\n",
    "\n",
    "Lab 8 (Diffusion): Noise ‚Üí Features ‚Üí Image\n",
    "                   (Synthesis: \"Create this!\")\n",
    "```\n",
    "\n",
    "**Both use convolutional architectures!**\n",
    "- CNNs: Downsampling (image ‚Üí features)\n",
    "- Diffusion: Upsampling (noise ‚Üí image)\n",
    "- U-Net: Both together (encoder-decoder architecture)\n",
    "\n",
    "### **Multimodal AI:**\n",
    "- **Lab 5:** Text embeddings (sentences ‚Üí vectors)\n",
    "- **Lab 7:** Image embeddings (images ‚Üí vectors)\n",
    "- **Lab 8:** Text ‚Üí Image generation (\"A dog wearing a hat\" ‚Üí actual image!)\n",
    "\n",
    "**You're building toward understanding modern AI systems like GPT-4 with vision, DALL-E, and beyond!**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Module 4 Complete!\n",
    "\n",
    "You now understand:\n",
    "- **How to build and train a CNN from scratch**\n",
    "- **What CNNs learn** (automatic feature extraction via gradient descent)\n",
    "- **How to evaluate CNN performance** (accuracy, confusion matrix)\n",
    "- **What learned filters look like** (edge detectors, patterns)\n",
    "- **Connection to Lab 4** (same training process, different architecture)\n",
    "- **When CNNs fail** (ambiguous examples, similar-looking digits)\n",
    "\n",
    "**Key insight:**\n",
    "> CNNs learn hierarchical feature extractors automatically‚Äîno human filter design required. The same training process from Lab 4 (gradient descent) produces sophisticated pattern detectors for images!\n",
    "\n",
    "**Congratulations!** You've completed all 5 modules of Lab 7!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö **Lab 7 Complete: Review**\n",
    "\n",
    "### **What You've Accomplished:**\n",
    "\n",
    "**Module 0:** Learned what convolution is (sliding window + multiply-and-add)\n",
    "\n",
    "**Module 1:** Applied classic filters to real images (Sobel, blur, sharpen)\n",
    "\n",
    "**Module 2:** Visualized feature maps from pretrained CNN (MobileNetV2)\n",
    "\n",
    "**Module 3:** Understood hierarchical feature extraction principles\n",
    "\n",
    "**Module 4:** Trained your own CNN from scratch on MNIST (this module!)\n",
    "\n",
    "### **Core Concepts Mastered:**\n",
    "- Convolution operation\n",
    "- Filters and feature maps\n",
    "- Hierarchical learning (edges ‚Üí textures ‚Üí shapes ‚Üí objects)\n",
    "- Parameter sharing and translation invariance\n",
    "- CNN training pipeline\n",
    "- Model evaluation and error analysis\n",
    "\n",
    "### **Connections Made:**\n",
    "- Lab 3: Activation functions (ReLU after convolution)\n",
    "- Lab 4: Hidden layers (conv layers are spatially-structured hidden layers)\n",
    "- Lab 5: Embeddings (CNN final layers create image embeddings)\n",
    "- Lab 6: Saliency (feature maps show WHAT, saliency shows WHERE)\n",
    "\n",
    "### **What's Next:**\n",
    "Lab 8 will explore **diffusion models**‚Äîthe technology behind DALL-E, Stable Diffusion, and Midjourney. You'll learn how to **generate images from text descriptions**, completing your understanding of multimodal AI!\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Great work!**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
