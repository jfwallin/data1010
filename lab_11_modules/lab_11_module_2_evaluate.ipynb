{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 - Module 2: Evaluate (Stage 2)\n",
    "\n",
    "**Time:** ~20 minutes\n",
    "\n",
    "## Stage 2: Systematic Evaluation with Rubrics\n",
    "\n",
    "In Module 1, you gave a quick \"first impression\" rating. Now you'll evaluate more systematically using **explicit criteria**.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Apply explicit rubrics to evaluate content quality\n",
    "- Compare human evaluation vs. AI self-evaluation\n",
    "- Discover where AI's self-assessment is accurate or inaccurate\n",
    "- Understand that \"good\" requires clear, measurable standards\n",
    "\n",
    "### What You'll Do\n",
    "\n",
    "1. Select a rubric that fits your content type\n",
    "2. Score the AI's output (human judgment) using the rubric\n",
    "3. Ask the AI to score its OWN work using the same rubric\n",
    "4. Compare: Where do you agree? Where do you disagree?\n",
    "5. Identify the weakest areas for revision in Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML, Markdown\n",
    "\n",
    "print(\"‚úì Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scenario and Module 1 data\n",
    "group_code = int(input(\"Enter your group code: \"))\n",
    "\n",
    "try:\n",
    "    # Load scenario\n",
    "    with open(f'lab11_group_{group_code}_scenario.json', 'r') as f:\n",
    "        scenario = json.load(f)\n",
    "    \n",
    "    # Load Module 1 data\n",
    "    with open(f'lab11_group_{group_code}_module1.json', 'r') as f:\n",
    "        module1_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úì Loaded data for Group {group_code}\")\n",
    "    print(f\"\\nPrompt Family: {scenario['family']}\")\n",
    "    print(f\"AI Model Used: {module1_data['model_used']}\")\n",
    "    print(f\"First Impression Rating: {module1_data['first_impression_rating']}/5\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ùå ERROR: Could not find required files for group {group_code}.\")\n",
    "    print(\"Please run Module 0 and Module 1 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Your AI's Output\n",
    "\n",
    "Here's what the AI generated in Module 1. You'll be evaluating this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"AI OUTPUT TO EVALUATE\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(module1_data['ai_output'])\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Select a Rubric\n",
    "\n",
    "Choose the rubric that best fits your content type. The **recommended rubric** for your scenario is shown, but you can choose a different one if it fits better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three rubrics\n",
    "RUBRICS = {\n",
    "    'General Communication': {\n",
    "        'description': 'Best for: Science Explainers, Museum Panels, Educational Analogies, Infographic Structures',\n",
    "        'criteria': [\n",
    "            {\n",
    "                'name': 'Clarity',\n",
    "                'levels': {\n",
    "                    1: 'Confusing, jargon-heavy, hard to follow',\n",
    "                    3: 'Mostly clear with some unclear parts',\n",
    "                    5: 'Crystal clear, no confusion, well-explained'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Audience Fit',\n",
    "                'levels': {\n",
    "                    1: 'Wrong level/tone for target audience',\n",
    "                    3: 'Generally appropriate for audience',\n",
    "                    5: 'Perfectly tailored to audience needs'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Structure',\n",
    "                'levels': {\n",
    "                    1: 'Disorganized, hard to follow flow',\n",
    "                    3: 'Logical but could be tighter',\n",
    "                    5: 'Exceptionally well-organized, smooth flow'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Specificity',\n",
    "                'levels': {\n",
    "                    1: 'Vague, generic statements',\n",
    "                    3: 'Some specifics mixed with generic content',\n",
    "                    5: 'Rich in concrete details and examples'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Accuracy',\n",
    "                'levels': {\n",
    "                    1: 'Factual errors present',\n",
    "                    3: 'Mostly accurate with minor issues',\n",
    "                    5: 'Fully accurate and defensible'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'Persuasion/Campaign': {\n",
    "        'description': 'Best for: PSAs, Product Pitches, Debate Positions',\n",
    "        'criteria': [\n",
    "            {\n",
    "                'name': 'Message Focus',\n",
    "                'levels': {\n",
    "                    1: 'Unclear or multiple conflicting messages',\n",
    "                    3: 'Clear message but could be sharper',\n",
    "                    5: 'Single, powerful, memorable message'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Evidence/Support',\n",
    "                'levels': {\n",
    "                    1: 'No evidence or weak anecdotes only',\n",
    "                    3: 'Some credible support provided',\n",
    "                    5: 'Strong, credible, well-integrated evidence'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Call to Action',\n",
    "                'levels': {\n",
    "                    1: 'Missing or extremely vague',\n",
    "                    3: 'Present but not compelling',\n",
    "                    5: 'Clear, specific, highly motivating'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Tone Fit',\n",
    "                'levels': {\n",
    "                    1: 'Mismatched tone for purpose/audience',\n",
    "                    3: 'Appropriate but not distinctive',\n",
    "                    5: 'Perfectly calibrated, engaging tone'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Ethics',\n",
    "                'levels': {\n",
    "                    1: 'Misleading or manipulative',\n",
    "                    3: 'Honest with some weak points',\n",
    "                    5: 'Transparent, balanced, trustworthy'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'Creative/Narrative': {\n",
    "        'description': 'Best for: Short Narratives, Creative Content',\n",
    "        'criteria': [\n",
    "            {\n",
    "                'name': 'Engagement',\n",
    "                'levels': {\n",
    "                    1: 'Boring, loses attention quickly',\n",
    "                    3: 'Intermittently interesting',\n",
    "                    5: 'Captivating throughout'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Coherence',\n",
    "                'levels': {\n",
    "                    1: 'Confusing, disjointed narrative',\n",
    "                    3: 'Logical but rough transitions',\n",
    "                    5: 'Seamlessly flows, well-integrated'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Creativity',\n",
    "                'levels': {\n",
    "                    1: 'Clich√©d, completely predictable',\n",
    "                    3: 'Some original elements',\n",
    "                    5: 'Fresh, unexpected, memorable'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Constraint Adherence',\n",
    "                'levels': {\n",
    "                    1: 'Ignores key requirements',\n",
    "                    3: 'Meets most constraints',\n",
    "                    5: 'Perfectly fulfills all constraints'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display rubric information\n",
    "print(f\"\\nRECOMMENDED RUBRIC FOR YOUR SCENARIO:\")\n",
    "print(f\"  {scenario['default_rubric']}\")\n",
    "print(f\"\\nALL AVAILABLE RUBRICS:\")\n",
    "print()\n",
    "for rubric_name, rubric_info in RUBRICS.items():\n",
    "    print(f\"‚Ä¢ {rubric_name}\")\n",
    "    print(f\"  {rubric_info['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubric selection widget\n",
    "rubric_selector = widgets.Dropdown(\n",
    "    options=list(RUBRICS.keys()),\n",
    "    value=scenario['default_rubric'],\n",
    "    description='Select Rubric:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "rubric_justification = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Why did you choose this rubric? How does it fit your content?',\n",
    "    description='Justification:',\n",
    "    layout=widgets.Layout(width='100%', height='80px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"SELECT YOUR RUBRIC\")\n",
    "print(\"=\"*70)\n",
    "display(rubric_selector)\n",
    "display(rubric_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Display Your Selected Rubric\n",
    "\n",
    "Run this cell to see the full rubric with all criteria and scoring levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rubric(rubric_name):\n",
    "    rubric = RUBRICS[rubric_name]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"{rubric_name.upper()} RUBRIC\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{rubric['description']}\")\n",
    "    print()\n",
    "    \n",
    "    for i, criterion in enumerate(rubric['criteria'], 1):\n",
    "        print(f\"{i}. {criterion['name']}\")\n",
    "        print(f\"   1 (Weak):      {criterion['levels'][1]}\")\n",
    "        print(f\"   3 (Good):      {criterion['levels'][3]}\")\n",
    "        print(f\"   5 (Excellent): {criterion['levels'][5]}\")\n",
    "        print()\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Button to display rubric\n",
    "display_button = widgets.Button(\n",
    "    description='üìã Display Rubric',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "rubric_output = widgets.Output()\n",
    "\n",
    "def on_display_clicked(b):\n",
    "    with rubric_output:\n",
    "        clear_output()\n",
    "        display_rubric(rubric_selector.value)\n",
    "\n",
    "display_button.on_click(on_display_clicked)\n",
    "\n",
    "display(display_button)\n",
    "display(rubric_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Human Evaluation (Your Scores)\n",
    "\n",
    "Now score the AI's output using the rubric. Use your best judgment.\n",
    "\n",
    "**Scoring Scale:** 1, 2, 3, 4, or 5 for each criterion\n",
    "- **1-2:** Closer to \"Weak\" description\n",
    "- **3:** Matches \"Good\" description\n",
    "- **4-5:** Closer to \"Excellent\" description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scoring widgets dynamically based on selected rubric\n",
    "def create_human_scoring_widgets():\n",
    "    selected_rubric = RUBRICS[rubric_selector.value]\n",
    "    human_scores = {}\n",
    "    human_justifications = {}\n",
    "    \n",
    "    print(\"HUMAN EVALUATION (Your Scores)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Score each criterion from 1-5 based on the rubric.\")\n",
    "    print()\n",
    "    \n",
    "    for criterion in selected_rubric['criteria']:\n",
    "        name = criterion['name']\n",
    "        \n",
    "        score_widget = widgets.Dropdown(\n",
    "            options=[1, 2, 3, 4, 5],\n",
    "            description=f\"{name}:\",\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        )\n",
    "        \n",
    "        justification_widget = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder=f'Why this score for {name}? Be specific.',\n",
    "            layout=widgets.Layout(width='100%', height='60px')\n",
    "        )\n",
    "        \n",
    "        human_scores[name] = score_widget\n",
    "        human_justifications[name] = justification_widget\n",
    "        \n",
    "        display(score_widget)\n",
    "        display(justification_widget)\n",
    "        print()\n",
    "    \n",
    "    return human_scores, human_justifications\n",
    "\n",
    "# Button to create scoring interface\n",
    "create_scoring_button = widgets.Button(\n",
    "    description='‚úèÔ∏è Start Scoring',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "scoring_output = widgets.Output()\n",
    "human_scores_dict = {}\n",
    "human_justifications_dict = {}\n",
    "\n",
    "def on_create_scoring_clicked(b):\n",
    "    global human_scores_dict, human_justifications_dict\n",
    "    with scoring_output:\n",
    "        clear_output()\n",
    "        human_scores_dict, human_justifications_dict = create_human_scoring_widgets()\n",
    "\n",
    "create_scoring_button.on_click(on_create_scoring_clicked)\n",
    "\n",
    "print(\"Click 'Start Scoring' after you've displayed and reviewed the rubric:\")\n",
    "display(create_scoring_button)\n",
    "display(scoring_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: AI Self-Evaluation\n",
    "\n",
    "Now go back to your AI system and ask it to evaluate its OWN work using the same rubric.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Go back to the AI conversation from Module 1 (or start a new one and paste the AI's output)\n",
    "\n",
    "2. Copy the **prompt below** (it includes the rubric and asks AI to self-evaluate)\n",
    "\n",
    "3. Paste it into the AI and get its response\n",
    "\n",
    "4. Record the AI's self-scores and reasoning below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate self-evaluation prompt\n",
    "def generate_self_eval_prompt():\n",
    "    selected_rubric = RUBRICS[rubric_selector.value]\n",
    "    \n",
    "    prompt = f\"\"\"I need you to evaluate the quality of your previous response using this rubric:\n",
    "\n",
    "**{rubric_selector.value} Rubric:**\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, criterion in enumerate(selected_rubric['criteria'], 1):\n",
    "        prompt += f\"{i}. **{criterion['name']}**\\n\"\n",
    "        prompt += f\"   - Score 1 (Weak): {criterion['levels'][1]}\\n\"\n",
    "        prompt += f\"   - Score 3 (Good): {criterion['levels'][3]}\\n\"\n",
    "        prompt += f\"   - Score 5 (Excellent): {criterion['levels'][5]}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    \n",
    "    prompt += \"\"\"Please:\n",
    "1. Score your response on EACH criterion (1-5 scale)\n",
    "2. Briefly explain your reasoning for each score (1-2 sentences)\n",
    "3. Identify the TWO WEAKEST areas (lowest scores)\n",
    "4. For each weak area, suggest a specific improvement\n",
    "\n",
    "Format your response clearly with the criterion name, score, and reasoning.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Button to generate and display prompt\n",
    "generate_prompt_button = widgets.Button(\n",
    "    description='üìù Generate Prompt',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "prompt_output = widgets.Output()\n",
    "\n",
    "def on_generate_prompt_clicked(b):\n",
    "    with prompt_output:\n",
    "        clear_output()\n",
    "        prompt = generate_self_eval_prompt()\n",
    "        print(\"=\"*70)\n",
    "        print(\"COPY THIS PROMPT TO YOUR AI\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        print(prompt)\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "\n",
    "generate_prompt_button.on_click(on_generate_prompt_clicked)\n",
    "\n",
    "print(\"\\nGenerate the self-evaluation prompt to give to your AI:\")\n",
    "display(generate_prompt_button)\n",
    "display(prompt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Record AI's Self-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget to record AI's complete self-evaluation\n",
    "ai_self_eval_full = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder=\"Paste the AI's COMPLETE self-evaluation response here...\",\n",
    "    description='AI Self-Eval:',\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "ai_weaknesses_identified = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder=\"What 2 weaknesses did the AI identify? What improvements did it suggest?\",\n",
    "    description='AI Weaknesses:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"RECORD AI'S SELF-EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "display(ai_self_eval_full)\n",
    "display(ai_weaknesses_identified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract AI's Scores\n",
    "\n",
    "Manually enter the scores the AI gave itself for each criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI score entry widgets\n",
    "def create_ai_score_widgets():\n",
    "    selected_rubric = RUBRICS[rubric_selector.value]\n",
    "    ai_scores = {}\n",
    "    \n",
    "    print(\"AI'S SELF-SCORES (Enter what AI scored itself)\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    for criterion in selected_rubric['criteria']:\n",
    "        name = criterion['name']\n",
    "        \n",
    "        score_widget = widgets.Dropdown(\n",
    "            options=[1, 2, 3, 4, 5],\n",
    "            description=f\"{name}:\",\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        )\n",
    "        \n",
    "        ai_scores[name] = score_widget\n",
    "        display(score_widget)\n",
    "    \n",
    "    return ai_scores\n",
    "\n",
    "# Button to create AI score entry\n",
    "create_ai_scores_button = widgets.Button(\n",
    "    description='ü§ñ Enter AI Scores',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "ai_scores_output = widgets.Output()\n",
    "ai_scores_dict = {}\n",
    "\n",
    "def on_create_ai_scores_clicked(b):\n",
    "    global ai_scores_dict\n",
    "    with ai_scores_output:\n",
    "        clear_output()\n",
    "        ai_scores_dict = create_ai_score_widgets()\n",
    "\n",
    "create_ai_scores_button.on_click(on_create_ai_scores_clicked)\n",
    "\n",
    "display(create_ai_scores_button)\n",
    "display(ai_scores_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Evaluations\n",
    "\n",
    "Now let's see where you and the AI agree and disagree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "compare_button = widgets.Button(\n",
    "    description='üìä Compare Scores',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "compare_output = widgets.Output()\n",
    "\n",
    "def on_compare_clicked(b):\n",
    "    with compare_output:\n",
    "        clear_output()\n",
    "        \n",
    "        if not human_scores_dict or not ai_scores_dict:\n",
    "            print(\"‚ùå Please complete both human scoring and AI score entry first.\")\n",
    "            return\n",
    "        \n",
    "        # Create comparison table\n",
    "        selected_rubric = RUBRICS[rubric_selector.value]\n",
    "        \n",
    "        comparison_data = []\n",
    "        total_human = 0\n",
    "        total_ai = 0\n",
    "        disagreements = []\n",
    "        \n",
    "        for criterion in selected_rubric['criteria']:\n",
    "            name = criterion['name']\n",
    "            human_score = human_scores_dict[name].value\n",
    "            ai_score = ai_scores_dict[name].value\n",
    "            diff = human_score - ai_score\n",
    "            \n",
    "            total_human += human_score\n",
    "            total_ai += ai_score\n",
    "            \n",
    "            agreement = \"‚úì Match\" if diff == 0 else (\"‚ö†Ô∏è Human higher\" if diff > 0 else \"‚ö†Ô∏è AI higher\")\n",
    "            \n",
    "            if abs(diff) >= 2:\n",
    "                disagreements.append((name, human_score, ai_score, diff))\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Criterion': name,\n",
    "                'Human Score': human_score,\n",
    "                'AI Score': ai_score,\n",
    "                'Difference': diff,\n",
    "                'Agreement': agreement\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"HUMAN VS. AI EVALUATION COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        print(df.to_string(index=False))\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total Human Score: {total_human}/{len(selected_rubric['criteria']) * 5}\")\n",
    "        print(f\"Total AI Score:    {total_ai}/{len(selected_rubric['criteria']) * 5}\")\n",
    "        print(f\"Overall Difference: {total_human - total_ai}\")\n",
    "        print()\n",
    "        \n",
    "        if disagreements:\n",
    "            print(\"SIGNIFICANT DISAGREEMENTS (¬±2 or more):\")\n",
    "            for name, h_score, a_score, diff in disagreements:\n",
    "                if diff > 0:\n",
    "                    print(f\"  ‚Ä¢ {name}: You scored {h_score}, AI scored {a_score} (AI underrated itself by {diff})\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ {name}: You scored {h_score}, AI scored {a_score} (AI overrated itself by {abs(diff)})\")\n",
    "        else:\n",
    "            print(\"No major disagreements (all within ¬±1 point)\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "\n",
    "compare_button.on_click(on_compare_clicked)\n",
    "\n",
    "display(compare_button)\n",
    "display(compare_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Reflection on Disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_analysis = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Where did you most disagree with AI? Why do you think this disagreement occurred?',\n",
    "    description='Analysis:',\n",
    "    layout=widgets.Layout(width='100%', height='120px'),\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "print(\"\\nANALYZE DISAGREEMENTS\")\n",
    "print(\"=\"*70)\n",
    "display(disagreement_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save button\n",
    "save_module2_button = widgets.Button(\n",
    "    description='üíæ Save Module 2',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "save_output = widgets.Output()\n",
    "\n",
    "def on_save_module2_clicked(b):\n",
    "    with save_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Validate\n",
    "        if not rubric_justification.value:\n",
    "            print(\"‚ùå Please provide justification for rubric choice\")\n",
    "            return\n",
    "        if not human_scores_dict or not ai_scores_dict:\n",
    "            print(\"‚ùå Please complete both human and AI scoring\")\n",
    "            return\n",
    "        if not ai_self_eval_full.value:\n",
    "            print(\"‚ùå Please record AI's full self-evaluation\")\n",
    "            return\n",
    "        \n",
    "        # Compile data\n",
    "        selected_rubric = RUBRICS[rubric_selector.value]\n",
    "        \n",
    "        human_scores_data = {name: widget.value for name, widget in human_scores_dict.items()}\n",
    "        human_justifications_data = {name: widget.value for name, widget in human_justifications_dict.items()}\n",
    "        ai_scores_data = {name: widget.value for name, widget in ai_scores_dict.items()}\n",
    "        \n",
    "        # Find two weakest criteria (lowest human scores)\n",
    "        sorted_criteria = sorted(human_scores_data.items(), key=lambda x: x[1])\n",
    "        weakest_criteria = [sorted_criteria[0][0], sorted_criteria[1][0]]\n",
    "        \n",
    "        module2_data = {\n",
    "            'group_code': group_code,\n",
    "            'selected_rubric': rubric_selector.value,\n",
    "            'rubric_justification': rubric_justification.value,\n",
    "            'human_scores': human_scores_data,\n",
    "            'human_justifications': human_justifications_data,\n",
    "            'ai_self_evaluation_full': ai_self_eval_full.value,\n",
    "            'ai_scores': ai_scores_data,\n",
    "            'ai_weaknesses_identified': ai_weaknesses_identified.value,\n",
    "            'disagreement_analysis': disagreement_analysis.value,\n",
    "            'weakest_criteria': weakest_criteria\n",
    "        }\n",
    "        \n",
    "        # Save\n",
    "        with open(f'lab11_group_{group_code}_module2.json', 'w') as f:\n",
    "            json.dump(module2_data, f, indent=2)\n",
    "        \n",
    "        print(\"‚úì Module 2 data saved successfully!\")\n",
    "        print(f\"\\nWeakest criteria identified for revision:\")\n",
    "        print(f\"  1. {weakest_criteria[0]} (score: {human_scores_data[weakest_criteria[0]]})\")\n",
    "        print(f\"  2. {weakest_criteria[1]} (score: {human_scores_data[weakest_criteria[1]]})\")\n",
    "        print(f\"\\nYou're ready for Module 3 (Revise & Compare)!\")\n",
    "\n",
    "save_module2_button.on_click(on_save_module2_clicked)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVE YOUR WORK\")\n",
    "print(\"=\"*70)\n",
    "display(save_module2_button)\n",
    "display(save_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2 Questions\n",
    "\n",
    "Answer these on your Lab 11 Answer Sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Rubric Selection\n",
    "\n",
    "Which rubric did you choose? Why is it the best fit for your content type?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Human Evaluation\n",
    "\n",
    "Record your scores for each criterion. Include brief justification for each score.\n",
    "\n",
    "*(Answer on your answer sheet - use the comparison table)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: AI Self-Scores\n",
    "\n",
    "What scores did the AI give itself? Record the AI's reasoning for each score.\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: AI-Identified Weaknesses\n",
    "\n",
    "Which 2 weaknesses did the AI identify in its own work? What improvements did it suggest?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Biggest Disagreement\n",
    "\n",
    "Where did you MOST disagree with the AI's self-evaluation? Why do you think this disagreement occurred? Was the AI overconfident or underconfident?\n",
    "\n",
    "*(Answer on your answer sheet)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Accomplished\n",
    "\n",
    "‚úì Selected a rubric that fits your content type\n",
    "\n",
    "‚úì Systematically scored the AI's output (human judgment)\n",
    "\n",
    "‚úì Asked the AI to self-evaluate using the same rubric\n",
    "\n",
    "‚úì Compared human vs. AI scores\n",
    "\n",
    "‚úì Identified areas of agreement and disagreement\n",
    "\n",
    "‚úì Found the 2 weakest criteria for revision\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "You've likely discovered that **AI self-evaluation is unreliable**. AI may:\n",
    "- Be **overconfident** (score itself higher than it deserves)\n",
    "- Be **underconfident** (score itself lower than deserved)\n",
    "- Miss obvious flaws that humans catch\n",
    "- Focus on the wrong aspects of quality\n",
    "\n",
    "This is why **human judgment remains essential** in evaluation!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 3 (Revise & Compare)**, you will:\n",
    "- Ask the AI to revise its work, targeting the 2 weakest criteria\n",
    "- Score the revised version\n",
    "- Compare: Did it improve? Did new problems appear?\n",
    "- Test whether AI is better at revision than self-evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
