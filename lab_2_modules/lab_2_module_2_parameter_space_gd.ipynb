{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Module 2: Gradient Descent on Parameter Space (Line Fitting)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Apply gradient descent to 2D parameter optimization\n",
    "- Visualize GD paths on MSE contour plots\n",
    "- Compare GD navigation to manual search from Lab 1\n",
    "- Understand how learning rate affects convergence in 2D\n",
    "\n",
    "**Time:** ~20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**IMPORTANT:** Enter the same group code from Lab 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Lab 1\n",
    "\n",
    "In **Lab 1 Modules 2-3**, you:\n",
    "- Manually adjusted (m, b) sliders to fit a line\n",
    "- Explored parameter space by submitting guesses\n",
    "- Only saw MSE values (no visual of data)\n",
    "- Navigated by trial and error\n",
    "\n",
    "**Today:** Gradient descent will navigate the same parameter space automatically!\n",
    "\n",
    "### Key Difference:\n",
    "- **Lab 1:** You chose where to go next based on MSE feedback\n",
    "- **Lab 2:** GD computes gradients and moves systematically downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Generate Same Data as Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ipywidgets import FloatText, Button, Output, VBox, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "group_code = int(input(\"Enter your group code: \"))\n",
    "np.random.seed(group_code)\n",
    "\n",
    "# Generate same line fitting data as Lab 1\n",
    "true_m = np.random.uniform(-3, 3)\n",
    "true_b = np.random.uniform(-5, 5)\n",
    "x_data = np.linspace(-5, 5, 25)\n",
    "noise = np.random.normal(0, 1.0, size=len(x_data))\n",
    "y_data = true_m * x_data + true_b + noise\n",
    "\n",
    "print(\"✓ Same line fitting data loaded from Lab 1\")\n",
    "print(f\"\\nTrue parameters: m = {true_m:.3f}, b = {true_b:.3f}\")\n",
    "print(\"(Revealed for learning purposes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define MSE Function and Gradients\n",
    "\n",
    "The Mean Squared Error (MSE) measures how well a line fits the data:\n",
    "\n",
    "```\n",
    "MSE(m, b) = (1/N) × Σ(y_i - (m × x_i + b))²\n",
    "```\n",
    "\n",
    "Gradient descent will minimize this function by adjusting (m, b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(params):\n",
    "    \"\"\"\n",
    "    Compute MSE for line parameters [m, b].\n",
    "    \n",
    "    Args:\n",
    "        params: numpy array [m, b]\n",
    "    \n",
    "    Returns:\n",
    "        MSE value (scalar)\n",
    "    \"\"\"\n",
    "    m, b = params\n",
    "    y_pred = m * x_data + b\n",
    "    return np.mean((y_data - y_pred) ** 2)\n",
    "\n",
    "def compute_gradient_mse(params, h=1e-5):\n",
    "    \"\"\"\n",
    "    Compute numerical gradient of MSE with respect to [m, b].\n",
    "    \n",
    "    Returns:\n",
    "        [gradient_m, gradient_b]\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(params)\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        params_forward = params.copy()\n",
    "        params_backward = params.copy()\n",
    "        \n",
    "        params_forward[i] += h\n",
    "        params_backward[i] -= h\n",
    "        \n",
    "        grad[i] = (compute_mse(params_forward) - compute_mse(params_backward)) / (2 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def gd_step_2d(params, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step on (m, b).\n",
    "    \n",
    "    Returns:\n",
    "        Updated [m, b]\n",
    "    \"\"\"\n",
    "    grad = compute_gradient_mse(params)\n",
    "    return params - learning_rate * grad\n",
    "\n",
    "print(\"✓ MSE and gradient functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute MSE Landscape (for visualization)\n",
    "\n",
    "We'll precompute the MSE over a grid so we can show contour plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for contour plot\n",
    "m_vals = np.linspace(-5, 5, 60)\n",
    "b_vals = np.linspace(-5, 5, 60)\n",
    "M_grid, B_grid = np.meshgrid(m_vals, b_vals)\n",
    "MSE_grid = np.zeros_like(M_grid)\n",
    "\n",
    "for i in range(M_grid.shape[0]):\n",
    "    for j in range(M_grid.shape[1]):\n",
    "        MSE_grid[i, j] = compute_mse(np.array([M_grid[i, j], B_grid[i, j]]))\n",
    "\n",
    "# Find grid minimum\n",
    "flat_idx = np.argmin(MSE_grid)\n",
    "i_min, j_min = np.unravel_index(flat_idx, MSE_grid.shape)\n",
    "m_min_grid = M_grid[i_min, j_min]\n",
    "b_min_grid = B_grid[i_min, j_min]\n",
    "mse_min_grid = MSE_grid[i_min, j_min]\n",
    "\n",
    "print(\"✓ MSE landscape computed\")\n",
    "print(f\"Grid minimum: m = {m_min_grid:.3f}, b = {b_min_grid:.3f}, MSE = {mse_min_grid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Questions (Answer BEFORE running GD)\n",
    "\n",
    "**Q6 (PREDICTION):** \n",
    "\n",
    "Think about your Lab 1 experience:\n",
    "- What was your best (m, b) from Lab 1 Module 3?\n",
    "- If you start GD from that point, will it find an even lower MSE?\n",
    "- Starting from (0, 0), predict: Will the path be straight or curved? Why?\n",
    "\n",
    "Write your predictions on the answer sheet before continuing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Gradient Descent\n",
    "\n",
    "**Instructions:**\n",
    "1. Enter starting values for m and b (try (0, 0) first)\n",
    "2. Choose a learning rate (start with 0.1)\n",
    "3. Click \"Run 1 Step\" to see a single GD update\n",
    "4. Click \"Run 20 Steps\" to see longer path\n",
    "5. Click \"Show Full Landscape\" to reveal MSE contours\n",
    "6. Click \"Reset\" to try new starting point or learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State for interactive GD\n",
    "gd_state_2d = {\n",
    "    'history': [],\n",
    "    'running': False,\n",
    "    'show_landscape': False\n",
    "}\n",
    "\n",
    "# Widgets\n",
    "m0_input = FloatText(description=\"Starting m:\", value=0.0, step=0.5)\n",
    "b0_input = FloatText(description=\"Starting b:\", value=0.0, step=0.5)\n",
    "lr_input = FloatText(description=\"Learning rate:\", value=0.1, step=0.01)\n",
    "step_button = Button(description=\"Run 1 Step\", button_style='info')\n",
    "multi_step_button = Button(description=\"Run 20 Steps\", button_style='success')\n",
    "reveal_button = Button(description=\"Show Full Landscape\", button_style='primary')\n",
    "reset_button = Button(description=\"Reset\", button_style='warning')\n",
    "output_area = Output()\n",
    "\n",
    "def initialize_gd_2d():\n",
    "    \"\"\"Initialize or reset GD state\"\"\"\n",
    "    m_start = m0_input.value\n",
    "    b_start = b0_input.value\n",
    "    params_start = np.array([m_start, b_start])\n",
    "    \n",
    "    gd_state_2d['history'] = [{\n",
    "        'step': 0,\n",
    "        'm': m_start,\n",
    "        'b': b_start,\n",
    "        'MSE': compute_mse(params_start),\n",
    "        'grad_m': 0.0,\n",
    "        'grad_b': 0.0\n",
    "    }]\n",
    "    gd_state_2d['running'] = True\n",
    "\n",
    "def run_gd_steps_2d(n_steps):\n",
    "    \"\"\"Run n steps of gradient descent\"\"\"\n",
    "    if not gd_state_2d['running']:\n",
    "        initialize_gd_2d()\n",
    "    \n",
    "    lr = lr_input.value\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        current = gd_state_2d['history'][-1]\n",
    "        params_current = np.array([current['m'], current['b']])\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad = compute_gradient_mse(params_current)\n",
    "        \n",
    "        # Update parameters\n",
    "        params_new = params_current - lr * grad\n",
    "        m_new, b_new = params_new\n",
    "        \n",
    "        gd_state_2d['history'].append({\n",
    "            'step': current['step'] + 1,\n",
    "            'm': m_new,\n",
    "            'b': b_new,\n",
    "            'MSE': compute_mse(params_new),\n",
    "            'grad_m': grad[0],\n",
    "            'grad_b': grad[1]\n",
    "        })\n",
    "\n",
    "def plot_gd_2d():\n",
    "    \"\"\"Plot GD path on parameter space\"\"\"\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        \n",
    "        if not gd_state_2d['history']:\n",
    "            print(\"Click 'Run 1 Step' or 'Run 20 Steps' to start!\")\n",
    "            return\n",
    "        \n",
    "        # Extract history\n",
    "        df = pd.DataFrame(gd_state_2d['history'])\n",
    "        m_hist = df['m'].values\n",
    "        b_hist = df['b'].values\n",
    "        mse_hist = df['MSE'].values\n",
    "        \n",
    "        # Create figure\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), dpi=100)\n",
    "        \n",
    "        # Left plot: Parameter space with optional landscape\n",
    "        if gd_state_2d['show_landscape']:\n",
    "            # Show full MSE contours\n",
    "            contour = ax1.contourf(M_grid, B_grid, MSE_grid, levels=25, cmap='viridis', alpha=0.6)\n",
    "            plt.colorbar(contour, ax=ax1, label='MSE')\n",
    "            \n",
    "            # Mark grid minimum\n",
    "            ax1.scatter([m_min_grid], [b_min_grid], c='white', marker='o', \n",
    "                       s=200, edgecolors='black', linewidths=2, zorder=10, label='Grid minimum')\n",
    "        \n",
    "        # Plot GD path\n",
    "        ax1.plot(m_hist, b_hist, 'r-', linewidth=2, alpha=0.8, label='GD path')\n",
    "        \n",
    "        # Mark start and end\n",
    "        ax1.scatter([m_hist[0]], [b_hist[0]], c='green', marker='o', \n",
    "                   s=200, edgecolors='black', linewidths=2, zorder=5, label='Start')\n",
    "        ax1.scatter([m_hist[-1]], [b_hist[-1]], c='red', marker='*', \n",
    "                   s=300, edgecolors='black', linewidths=2, zorder=5, label='Current')\n",
    "        \n",
    "        # Add arrows\n",
    "        arrow_freq = max(1, len(m_hist) // 8)\n",
    "        for i in range(0, len(m_hist) - 1, arrow_freq):\n",
    "            ax1.annotate('', xy=(m_hist[i+1], b_hist[i+1]), \n",
    "                        xytext=(m_hist[i], b_hist[i]),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red', lw=2, alpha=0.6))\n",
    "        \n",
    "        ax1.set_xlabel('m (slope)', fontsize=11)\n",
    "        ax1.set_ylabel('b (intercept)', fontsize=11)\n",
    "        ax1.set_title('GD Path in Parameter Space', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(fontsize=9, loc='best')\n",
    "        ax1.set_xlim(-5, 5)\n",
    "        ax1.set_ylim(-5, 5)\n",
    "        \n",
    "        # Right plot: MSE over iterations\n",
    "        ax2.plot(df['step'], mse_hist, 'b-o', linewidth=2, markersize=5)\n",
    "        ax2.axhline(y=mse_min_grid, color='green', linestyle='--', \n",
    "                   linewidth=2, alpha=0.5, label='Grid minimum MSE')\n",
    "        ax2.set_xlabel('Step', fontsize=11)\n",
    "        ax2.set_ylabel('MSE', fontsize=11)\n",
    "        ax2.set_title('MSE Convergence', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend(fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary stats\n",
    "        print(f\"\\nSteps taken: {len(gd_state_2d['history']) - 1}\")\n",
    "        print(f\"Start: m = {m_hist[0]:.4f}, b = {b_hist[0]:.4f}, MSE = {mse_hist[0]:.4f}\")\n",
    "        print(f\"Current: m = {m_hist[-1]:.4f}, b = {b_hist[-1]:.4f}, MSE = {mse_hist[-1]:.4f}\")\n",
    "        print(f\"MSE improvement: {mse_hist[0] - mse_hist[-1]:.4f}\")\n",
    "        print(f\"\\nGrid minimum: m = {m_min_grid:.4f}, b = {b_min_grid:.4f}, MSE = {mse_min_grid:.4f}\")\n",
    "        print(f\"Distance from minimum: Δm = {abs(m_hist[-1] - m_min_grid):.4f}, Δb = {abs(b_hist[-1] - b_min_grid):.4f}\")\n",
    "        \n",
    "        # Table of recent steps\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Recent Steps:\")\n",
    "        df_display = df[['step', 'm', 'b', 'MSE', 'grad_m', 'grad_b']].tail(10).copy()\n",
    "        for col in ['m', 'b', 'MSE', 'grad_m', 'grad_b']:\n",
    "            df_display[col] = df_display[col].apply(lambda v: f\"{v:.4f}\")\n",
    "        display(df_display)\n",
    "\n",
    "def on_step_click(b):\n",
    "    run_gd_steps_2d(1)\n",
    "    plot_gd_2d()\n",
    "\n",
    "def on_multi_step_click(b):\n",
    "    run_gd_steps_2d(20)\n",
    "    plot_gd_2d()\n",
    "\n",
    "def on_reveal_click(b):\n",
    "    gd_state_2d['show_landscape'] = True\n",
    "    plot_gd_2d()\n",
    "\n",
    "def on_reset_click(b):\n",
    "    gd_state_2d['history'] = []\n",
    "    gd_state_2d['running'] = False\n",
    "    gd_state_2d['show_landscape'] = False\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(\"Reset! Adjust starting point and learning rate, then click 'Run 1 Step'.\")\n",
    "\n",
    "step_button.on_click(on_step_click)\n",
    "multi_step_button.on_click(on_multi_step_click)\n",
    "reveal_button.on_click(on_reveal_click)\n",
    "reset_button.on_click(on_reset_click)\n",
    "\n",
    "print(\"Interactive Gradient Descent on Parameter Space\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Set starting (m, b) and learning rate\")\n",
    "print(\"2. Click 'Run 1 Step' to see one GD update\")\n",
    "print(\"3. Click 'Run 20 Steps' to see longer path\")\n",
    "print(\"4. Click 'Show Full Landscape' to reveal MSE contours\")\n",
    "print(\"5. Try different starting points and learning rates\")\n",
    "print(\"\\nSuggested experiments:\")\n",
    "print(\"  - Start at (0, 0) with LR = 0.1\")\n",
    "print(\"  - Try your Lab 1 best guess as starting point\")\n",
    "print(\"  - Try LR = 0.01 (too small), LR = 0.1 (good), LR = 0.5 (risky)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "display(VBox([\n",
    "    HBox([m0_input, b0_input, lr_input]),\n",
    "    HBox([step_button, multi_step_button, reveal_button, reset_button]),\n",
    "    output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Systematic Comparison: Three Learning Rates\n",
    "\n",
    "Let's compare three learning rates starting from the same point (0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare three learning rates\n",
    "start_params = np.array([0.0, 0.0])\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "max_steps = 50\n",
    "colors_lr = ['blue', 'green', 'red']\n",
    "\n",
    "results_2d = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    history_params = [start_params.copy()]\n",
    "    history_mse = [compute_mse(start_params)]\n",
    "    \n",
    "    params_current = start_params.copy()\n",
    "    for step in range(max_steps):\n",
    "        grad = compute_gradient_mse(params_current)\n",
    "        params_current = params_current - lr * grad\n",
    "        \n",
    "        history_params.append(params_current.copy())\n",
    "        history_mse.append(compute_mse(params_current))\n",
    "    \n",
    "    results_2d[lr] = {\n",
    "        'params': np.array(history_params),\n",
    "        'mse': np.array(history_mse)\n",
    "    }\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), dpi=100)\n",
    "\n",
    "# Left: Parameter space with contours\n",
    "contour = ax1.contourf(M_grid, B_grid, MSE_grid, levels=25, cmap='viridis', alpha=0.4)\n",
    "plt.colorbar(contour, ax=ax1, label='MSE')\n",
    "\n",
    "# Plot paths for each LR\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    params_hist = results_2d[lr]['params']\n",
    "    ax1.plot(params_hist[:, 0], params_hist[:, 1], 'o-', \n",
    "            color=colors_lr[i], label=f'LR = {lr}', linewidth=2, markersize=4, alpha=0.8)\n",
    "\n",
    "# Mark start and minimum\n",
    "ax1.scatter([start_params[0]], [start_params[1]], c='black', marker='o', \n",
    "           s=200, edgecolors='white', linewidths=2, zorder=10, label='Start')\n",
    "ax1.scatter([m_min_grid], [b_min_grid], c='white', marker='*', \n",
    "           s=300, edgecolors='black', linewidths=2, zorder=10, label='Minimum')\n",
    "\n",
    "ax1.set_xlabel('m (slope)', fontsize=11)\n",
    "ax1.set_ylabel('b (intercept)', fontsize=11)\n",
    "ax1.set_title('GD Paths for Different Learning Rates', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9, loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-5, 5)\n",
    "ax1.set_ylim(-5, 5)\n",
    "\n",
    "# Right: MSE convergence\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    mse_hist = results_2d[lr]['mse']\n",
    "    ax2.plot(range(len(mse_hist)), mse_hist, 'o-',\n",
    "            color=colors_lr[i], label=f'LR = {lr}', linewidth=2, markersize=4, alpha=0.8)\n",
    "\n",
    "ax2.axhline(y=mse_min_grid, color='black', linestyle='--', \n",
    "           linewidth=2, alpha=0.3, label='Minimum MSE')\n",
    "ax2.set_xlabel('Step', fontsize=11)\n",
    "ax2.set_ylabel('MSE', fontsize=11)\n",
    "ax2.set_title('Convergence Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')  # Log scale to see differences better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"=\"*80)\n",
    "summary_data = []\n",
    "for lr in learning_rates:\n",
    "    final_params = results_2d[lr]['params'][-1]\n",
    "    final_mse = results_2d[lr]['mse'][-1]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Final m': f\"{final_params[0]:.4f}\",\n",
    "        'Final b': f\"{final_params[1]:.4f}\",\n",
    "        'Final MSE': f\"{final_mse:.6f}\",\n",
    "        'Error from min': f\"{final_mse - mse_min_grid:.6f}\"\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(summary_data))\n",
    "\n",
    "print(f\"\\nGrid minimum: m = {m_min_grid:.4f}, b = {b_min_grid:.4f}, MSE = {mse_min_grid:.6f}\")\n",
    "print(f\"True parameters: m = {true_m:.4f}, b = {true_b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize the Fitted Line\n",
    "\n",
    "Let's see what the actual line looks like for the GD solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best result (LR = 0.1)\n",
    "best_lr = 0.1\n",
    "final_params = results_2d[best_lr]['params'][-1]\n",
    "m_gd, b_gd = final_params\n",
    "\n",
    "# Plot data with fitted line\n",
    "plt.figure(figsize=(10, 6), dpi=100)\n",
    "plt.scatter(x_data, y_data, s=80, alpha=0.6, edgecolors='black', label='Data')\n",
    "plt.plot(x_data, m_gd * x_data + b_gd, 'r-', linewidth=3, \n",
    "        label=f'GD fit: y = {m_gd:.3f}x + {b_gd:.3f}')\n",
    "plt.plot(x_data, true_m * x_data + true_b, 'g--', linewidth=2, alpha=0.5,\n",
    "        label=f'True line: y = {true_m:.3f}x + {true_b:.3f}')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Gradient Descent Line Fit', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"GD found: m = {m_gd:.4f}, b = {b_gd:.4f}\")\n",
    "print(f\"True params: m = {true_m:.4f}, b = {true_b:.4f}\")\n",
    "print(f\"Error: Δm = {abs(m_gd - true_m):.4f}, Δb = {abs(b_gd - true_b):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Your Answer Sheet\n",
    "\n",
    "**Q7.** Describe the shape of the GD path on the MSE contour plot:\n",
    "- Is it straight or curved? Why?\n",
    "- Does the path always move perpendicular to contour lines?\n",
    "- What happens to step size as GD approaches the minimum?\n",
    "\n",
    "**Q8.** Compare GD to your manual exploration in Lab 1:\n",
    "- Which was more efficient at finding the minimum?\n",
    "- How many \"guesses\" did you make in Lab 1 vs. GD steps here?\n",
    "- What advantage does GD have over random/systematic search?\n",
    "\n",
    "**Q9.** Based on the learning rate comparison:\n",
    "- Which LR converged fastest to near-minimum?\n",
    "- What happens with LR = 0.01? (Too slow?)\n",
    "- What happens with LR = 0.5? (Oscillation? Divergence?)\n",
    "- How would you choose a good LR for a new problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q6, Q7, Q8, Q9** on your answer sheet\n",
    "2. **Return to the LMS** and continue to Module 3\n",
    "3. In Module 3, you'll explore learning rate effects in more depth!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
