{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6, Module 0: What Is Saliency?\n",
    "\n",
    "**Estimated time:** 5 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: The \"Why\" Behind AI Decisions**\n",
    "\n",
    "Imagine this scenario:\n",
    "\n",
    "- **A doctor** reviews an X-ray and diagnoses pneumonia. When asked, they can point to the specific cloudy regions that indicate infection.\n",
    "- **A loan officer** denies a mortgage application. By law, they must explain which factors (income, credit score, debt ratio) led to the decision.\n",
    "- **An AI system** analyzes the same X-ray and predicts pneumonia with 95% confidence. But when asked \"why?\", it can't answer.\n",
    "\n",
    "**This is a problem.**\n",
    "\n",
    "When AI systems make important decisions‚Äîmedical diagnoses, loan approvals, criminal sentencing recommendations, hiring decisions‚Äîwe need to understand **what they're looking at** and **what drives their predictions**.\n",
    "\n",
    "That's where **saliency maps** come in.\n",
    "\n",
    "---\n",
    "\n",
    "# üìò **What Is a Saliency Map?**\n",
    "\n",
    "A **saliency map** is a visualization that shows which parts of an input were most important to a model's decision.\n",
    "\n",
    "Think of it as **highlighting the parts the model \"looked at\" when making a prediction**.\n",
    "\n",
    "### **Examples Across Different Input Types:**\n",
    "\n",
    "| Input Type | What Saliency Reveals | Example |\n",
    "|------------|------------------------|----------|\n",
    "| **Text** | Which words drove the sentiment? | In \"The movie was **excellent** and the acting was **terrible**\", which words matter most for positive/negative prediction? |\n",
    "| **Images** | Which pixels identified the object? | When classifying a dog photo, does the model focus on the ears, the snout, or the background? |\n",
    "| **Tabular Data** | Which features influenced the decision? | For loan approval, does the model rely on income, credit score, or... zip code? |\n",
    "\n",
    "**The key insight:** Saliency maps let us **see inside the black box** and understand what features the model relies on.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **How Do Saliency Maps Work? (Intuition)**\n",
    "\n",
    "The core idea is simple:\n",
    "\n",
    "> **If I change or remove a part of the input, how much does the prediction change?**\n",
    "\n",
    "### **Method 1: Masking/Perturbation** (Module 1 & 3)\n",
    "- Remove one word at a time from a sentence\n",
    "- See how much the sentiment score drops\n",
    "- Words that cause big drops are \"important\"\n",
    "\n",
    "Example:\n",
    "- Original: \"The movie was **excellent**\" ‚Üí 95% positive\n",
    "- Remove \"excellent\": \"The movie was\" ‚Üí 50% positive\n",
    "- **Importance of \"excellent\"**: 45%\n",
    "\n",
    "### **Method 2: Gradients** (Module 2)\n",
    "- For image models, we can compute **how much each pixel needs to change** to increase the class score\n",
    "- Pixels with high gradients are \"important\"\n",
    "- This creates a heatmap showing where the model focuses\n",
    "\n",
    "**Don't worry about the math!** The key idea is:\n",
    "> Saliency measures **sensitivity**: small changes to important parts ‚Üí big changes to predictions\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **What Saliency Maps Are NOT**\n",
    "\n",
    "Before we dive in, it's critical to understand the **limitations** of saliency maps:\n",
    "\n",
    "### **1. Saliency ‚â† Causality**\n",
    "Just because a feature is important doesn't mean it *causes* the outcome.\n",
    "\n",
    "**Example:** A hiring model might find \"years of experience\" salient, but it doesn't mean experience *causes* good job performance‚Äîit might just be correlated.\n",
    "\n",
    "### **2. Saliency ‚â† Complete Explanation**\n",
    "Saliency shows **what** the model uses, but not **how** it combines features or **why** it learned to focus on them.\n",
    "\n",
    "**Example:** Knowing a medical AI focuses on \"cloudy regions\" in an X-ray doesn't tell you if it understands lung anatomy or just memorized patterns.\n",
    "\n",
    "### **3. Saliency Can Reveal Spurious Correlations**\n",
    "Sometimes models focus on **the wrong things**‚Äîartifacts, watermarks, or biased proxies.\n",
    "\n",
    "**Example:** An image classifier trained on hospital X-rays might focus on the **hospital logo** in the corner rather than actual anatomy! Saliency maps would reveal this problem.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Why Saliency Matters: Ethics & Responsible AI**\n",
    "\n",
    "Explainability isn't just a technical curiosity‚Äîit's an **ethical and legal requirement** in many domains.\n",
    "\n",
    "### **Real-World Stakes:**\n",
    "\n",
    "#### **Medical Diagnosis**\n",
    "- **Problem:** An AI predicts skin cancer from photos\n",
    "- **Saliency reveals:** The model focuses on rulers/measuring tapes (often in malignant lesion photos from dermatology clinics)\n",
    "- **Consequence:** The model learned a spurious correlation, not actual cancer indicators\n",
    "- **Action:** Retrain with diverse images, remove artifacts\n",
    "\n",
    "#### **Criminal Justice**\n",
    "- **Problem:** An AI predicts recidivism risk for sentencing\n",
    "- **Saliency reveals:** Zip code and neighborhood are highly salient\n",
    "- **Consequence:** The model encodes systemic bias (redlining, over-policing)\n",
    "- **Action:** Remove proxy features, audit for fairness\n",
    "\n",
    "#### **Hiring Systems**\n",
    "- **Problem:** An AI screens resumes\n",
    "- **Saliency reveals:** Names, college names, and graduation years drive decisions\n",
    "- **Consequence:** Potential discrimination by ethnicity, age, or socioeconomic status\n",
    "- **Action:** Remove identifying information, focus on skills\n",
    "\n",
    "### **Legal Requirements**\n",
    "- **GDPR (Europe):** Right to explanation for automated decisions\n",
    "- **Equal Credit Opportunity Act (US):** Lenders must explain loan denials\n",
    "- **Emerging AI regulations:** Many jurisdictions are requiring explainability for high-stakes AI\n",
    "\n",
    "**The bottom line:** If you can't explain what your model is doing, you probably shouldn't deploy it in production.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **The Interpretability-Performance Tradeoff**\n",
    "\n",
    "There's often a tension between **how well a model performs** and **how well we can understand it**:\n",
    "\n",
    "| Model Type | Performance | Interpretability |\n",
    "|------------|-------------|------------------|\n",
    "| **Linear regression** | ‚≠ê‚≠ê (low) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (very high) |\n",
    "| **Decision tree** | ‚≠ê‚≠ê‚≠ê (medium) | ‚≠ê‚≠ê‚≠ê‚≠ê (high) |\n",
    "| **Random forest** | ‚≠ê‚≠ê‚≠ê‚≠ê (high) | ‚≠ê‚≠ê‚≠ê (medium) |\n",
    "| **Neural network** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (very high) | ‚≠ê‚≠ê (low) |\n",
    "| **Large language model (GPT-4)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (very high) | ‚≠ê (very low) |\n",
    "\n",
    "### **The Dilemma**\n",
    "- Simple models (linear regression, decision trees) are easy to understand but often don't perform well\n",
    "- Complex models (neural networks, transformers) perform amazingly but are \"black boxes\"\n",
    "\n",
    "### **Saliency as a Bridge**\n",
    "Saliency methods let us:\n",
    "- Use powerful models when we need high performance\n",
    "- Still get **some** insight into their decision-making\n",
    "- Detect when they're using problematic features\n",
    "- Build trust and meet regulatory requirements\n",
    "\n",
    "**It's not perfect understanding, but it's better than nothing.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Connection to Previous Labs**\n",
    "\n",
    "Saliency builds on everything you've learned so far:\n",
    "\n",
    "### **Lab 1: Models and Parameters**\n",
    "- You learned models make predictions using parameters\n",
    "- **Lab 6:** Saliency reveals **which inputs affect those predictions most**\n",
    "\n",
    "### **Lab 2: Gradient Descent**\n",
    "- You learned how gradients point in the direction of steepest change\n",
    "- **Lab 6:** Saliency uses gradients to find **which input changes affect the output most**\n",
    "\n",
    "### **Lab 3: Activation Functions**\n",
    "- You learned how nonlinearities transform input space\n",
    "- **Lab 6:** Saliency shows **which parts of the transformed space drive decisions**\n",
    "\n",
    "### **Lab 4: Hidden Layers**\n",
    "- You learned hidden layers create internal representations\n",
    "- **Lab 6:** Saliency reveals **what aspects of the input those representations capture**\n",
    "\n",
    "### **Lab 5: Embeddings**\n",
    "- You learned how words and sentences become vectors\n",
    "- **Lab 6:** Saliency shows **which words in those embeddings drive predictions**\n",
    "\n",
    "**Today, you're completing the picture:** Not just \"how does the model work?\" but \"what does the model actually look at?\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **What You'll Do in This Lab**\n",
    "\n",
    "Today's lab has **5 modules**:\n",
    "\n",
    "| Module | Focus | Time |\n",
    "|--------|-------|------|\n",
    "| 0 (this one) | Conceptual foundation | 5 min |\n",
    "| 1 | Text saliency via word masking | 15 min |\n",
    "| 2 | Image saliency via gradient visualization | 20 min |\n",
    "| 3 | Tabular saliency via feature perturbation | 10 min |\n",
    "| 4 | Ethics & explainability in practice | 10 min |\n",
    "\n",
    "**Total: ~60 minutes**\n",
    "\n",
    "By the end, you'll understand:\n",
    "- How to compute saliency for different data types\n",
    "- What saliency can and cannot tell you\n",
    "- Why explainability is crucial for responsible AI\n",
    "- How to spot when models focus on the wrong features\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Questions (Q1-Q3)**\n",
    "\n",
    "Before moving on to the hands-on modules, let's reflect on these conceptual questions. Record your answers in the **Answer Sheet**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q1. In your own words, what is a saliency map and why would we want one?**\n",
    "\n",
    "*Think about: What problem does saliency solve? Why isn't just knowing a model's accuracy enough?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. Give an example of a real-world application where model explainability is ethically important.**\n",
    "\n",
    "*Think about: Where are the stakes high? Where might bias or spurious correlations cause harm?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. Why might a saliency map not tell the whole story about how a model makes decisions?**\n",
    "\n",
    "*Think about: What are the limitations we discussed? What can saliency show, and what can't it show?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Module 0 Complete!\n",
    "\n",
    "You now understand:\n",
    "- **What saliency maps are** (visualizations of input importance)\n",
    "- **Why they matter** (ethics, debugging, trust, legal requirements)\n",
    "- **Their limitations** (not causality, not complete explanations)\n",
    "- **The interpretability tradeoff** (simple models vs. powerful models)\n",
    "\n",
    "**Ready for hands-on work?**\n",
    "\n",
    "Move on to **Module 1: Text Saliency with Word Masking**, where you'll train a sentiment classifier and see which words drive its predictions!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
