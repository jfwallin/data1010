{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Module 3: Building a Perceptron\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the two-step perceptron process: weighted sum â†’ activation\n",
    "- Interactively adjust weights and bias to classify data\n",
    "- See how the perceptron creates a decision boundary\n",
    "- Connect perceptrons to the neural networks you'll learn about later\n",
    "\n",
    "**Time:** ~20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**From Modules 1-2:** You learned that activation functions warp space and have different properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Previous Modules\n",
    "\n",
    "**Module 0:** Some patterns (XOR, circles) can't be separated by straight lines\n",
    "\n",
    "**Module 1:** Activation functions warp space, creating curved boundaries from straight rules\n",
    "\n",
    "**Module 2:** Different activations have different properties (range, smoothness, saturation)\n",
    "\n",
    "**Today:** We'll build a complete **perceptron** by combining:\n",
    "1. A weighted sum: `z = wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚ + b`\n",
    "2. An activation function: `output = activation(z)`\n",
    "\n",
    "This is the fundamental building block of neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Two-Step Process\n",
    "\n",
    "A **perceptron** is just two simple steps:\n",
    "\n",
    "### Step 1: Weighted Sum\n",
    "Combine the inputs with **weights** and add a **bias**:\n",
    "\n",
    "```\n",
    "z = wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚ + b\n",
    "```\n",
    "\n",
    "- **wâ‚, wâ‚‚** = weights (how much each input matters)\n",
    "- **b** = bias (shifts the decision boundary)\n",
    "- **z** = \"pre-activation\" (the value before applying activation)\n",
    "\n",
    "This is just like the line equation from Lab 1! `z` measures \"which side of the line\" a point is on.\n",
    "\n",
    "### Step 2: Activation\n",
    "Apply an activation function to the weighted sum:\n",
    "\n",
    "```\n",
    "output = activation(z)\n",
    "```\n",
    "\n",
    "This introduces **nonlinearity** and often squashes the output to a useful range (like 0 to 1).\n",
    "\n",
    "---\n",
    "\n",
    "### The Complete Perceptron:\n",
    "```\n",
    "output = activation(wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚ + b)\n",
    "```\n",
    "\n",
    "That's it! Two steps, one powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Setup: Create Multiple Datasets\n\nLet's create several linearly separable datasets with different configurations. Each dataset has two distinct classes that can be separated by a straight line, but they require different perceptron parameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom ipywidgets import FloatSlider, Dropdown, interact, VBox, HTML\nfrom IPython.display import display\n\n# Activation functions\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef relu(z):\n    return np.maximum(0, z)\n\ndef step(z):\n    return (z > 0).astype(float)\n\n# Create multiple datasets with different seeds\ndef create_dataset(dataset_num):\n    \"\"\"\n    Generate linearly separable datasets with different configurations.\n    \n    Dataset 1: Diagonal separation (upper-right vs lower-left)\n    Dataset 2: Vertical separation (left vs right)\n    Dataset 3: Horizontal separation (top vs bottom)\n    Dataset 4: Diagonal opposite (upper-left vs lower-right)\n    Dataset 5: Tight clusters with small gap\n    \"\"\"\n    np.random.seed(42 + dataset_num)\n    n_points = 50\n    \n    if dataset_num == 1:\n        # Diagonal: upper-right vs lower-left\n        X_class0 = np.random.randn(n_points, 2) * 0.5 + np.array([-1.5, -1.5])\n        X_class1 = np.random.randn(n_points, 2) * 0.5 + np.array([1.5, 1.5])\n        name = \"Diagonal (â†—)\"\n    elif dataset_num == 2:\n        # Vertical: left vs right\n        X_class0 = np.random.randn(n_points, 2) * 0.6 + np.array([-1.8, 0])\n        X_class1 = np.random.randn(n_points, 2) * 0.6 + np.array([1.8, 0])\n        name = \"Vertical (â†â†’)\"\n    elif dataset_num == 3:\n        # Horizontal: top vs bottom\n        X_class0 = np.random.randn(n_points, 2) * 0.6 + np.array([0, -1.8])\n        X_class1 = np.random.randn(n_points, 2) * 0.6 + np.array([0, 1.8])\n        name = \"Horizontal (â†‘â†“)\"\n    elif dataset_num == 4:\n        # Diagonal opposite: upper-left vs lower-right\n        X_class0 = np.random.randn(n_points, 2) * 0.5 + np.array([-1.5, 1.5])\n        X_class1 = np.random.randn(n_points, 2) * 0.5 + np.array([1.5, -1.5])\n        name = \"Diagonal (â†–)\"\n    else:  # dataset_num == 5\n        # Tight clusters\n        X_class0 = np.random.randn(n_points, 2) * 0.4 + np.array([-1.0, -0.5])\n        X_class1 = np.random.randn(n_points, 2) * 0.4 + np.array([1.2, 0.8])\n        name = \"Tight Clusters\"\n    \n    X = np.vstack([X_class0, X_class1])\n    y = np.hstack([np.zeros(n_points), np.ones(n_points)])\n    \n    return X, y, name\n\n# Create all datasets\ndatasets = {}\nfor i in range(1, 6):\n    X, y, name = create_dataset(i)\n    datasets[i] = {'X': X, 'y': y, 'name': name}\n\nprint(\"âœ“ Five datasets created!\")\nprint(\"\\nEach dataset has:\")\nprint(\"  â€¢ 50 blue points (Class 0)\")\nprint(\"  â€¢ 50 red points (Class 1)\")\nprint(\"  â€¢ Different orientations and separations\")\nprint(\"\\nYour goal: Find perceptron parameters that work for each dataset!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Preview All Five Datasets\n\nLet's see all five datasets side-by-side. Notice how they have different orientations and require different decision boundaries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preview all datasets\nfig, axes = plt.subplots(2, 3, figsize=(16, 10), dpi=100)\naxes = axes.flatten()\n\nfor i in range(1, 6):\n    ax = axes[i-1]\n    data = datasets[i]\n    X, y, name = data['X'], data['y'], data['name']\n    \n    ax.scatter(X[y == 0, 0], X[y == 0, 1], \n              c='blue', s=60, alpha=0.6, label='Class 0', edgecolors='k', linewidths=1)\n    ax.scatter(X[y == 1, 0], X[y == 1, 1], \n              c='red', s=60, alpha=0.6, label='Class 1', edgecolors='k', linewidths=1)\n    \n    ax.set_xlabel('xâ‚', fontsize=11)\n    ax.set_ylabel('xâ‚‚', fontsize=11)\n    ax.set_title(f'Dataset {i}: {name}', fontsize=12, fontweight='bold')\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect('equal')\n    ax.set_xlim(-4, 4)\n    ax.set_ylim(-4, 4)\n\n# Hide the 6th subplot\naxes[5].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"  â€¢ Dataset 1: Classes separated diagonally (â†—)\")\nprint(\"  â€¢ Dataset 2: Classes separated vertically (best separated by xâ‚)\")\nprint(\"  â€¢ Dataset 3: Classes separated horizontally (best separated by xâ‚‚)\")\nprint(\"  â€¢ Dataset 4: Classes separated diagonally (â†–) - opposite of Dataset 1\")\nprint(\"  â€¢ Dataset 5: Tighter clusters - requires more precise parameters\")\nprint(\"\\nAll are linearly separable - a perceptron can classify each one perfectly!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Interactive Perceptron Builder\n\n**Your task:** Choose a dataset, then adjust the weights (wâ‚, wâ‚‚), bias (b), and activation function to classify the data perfectly.\n\n**How it works:**\n1. For each point (xâ‚, xâ‚‚), compute: `z = wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚ + b`\n2. Apply activation: `output = activation(z)`\n3. Classify: If output > threshold (0.5 for sigmoid/tanh, 0 for others), predict Class 1\n\nThe visualization shows:\n- **Background color:** The perceptron's predictions across the entire space\n- **Decision boundary:** The green line where the perceptron switches from Class 0 to Class 1\n- **Data points:** Correctly classified points have solid borders, incorrect ones are marked with yellow X's\n\n**Challenge:** Try to get 100% accuracy on all five datasets!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def perceptron_classifier(dataset_num, w1, w2, b, activation_name):\n    \"\"\"\n    Visualize perceptron decision boundary and classification.\n    \"\"\"\n    # Get selected dataset\n    data = datasets[dataset_num]\n    X, y_true, name = data['X'], data['y'], data['name']\n    \n    # Select activation\n    activations = {\n        'Sigmoid': sigmoid,\n        'Tanh': tanh,\n        'ReLU': relu,\n        'Step': step\n    }\n    activation = activations[activation_name]\n    \n    # Create grid for decision boundary visualization\n    x1_grid = np.linspace(-4, 4, 200)\n    x2_grid = np.linspace(-4, 4, 200)\n    X1_grid, X2_grid = np.meshgrid(x1_grid, x2_grid)\n    grid_points = np.column_stack([X1_grid.ravel(), X2_grid.ravel()])\n    \n    # Step 1: Weighted sum for all grid points\n    z_grid = w1 * grid_points[:, 0] + w2 * grid_points[:, 1] + b\n    \n    # Step 2: Activation\n    output_grid = activation(z_grid)\n    \n    # Classify based on threshold\n    if activation_name in ['Sigmoid', 'Tanh']:\n        threshold = 0.5 if activation_name == 'Sigmoid' else 0.0\n    else:\n        threshold = 0.5\n    \n    predictions_grid = (output_grid > threshold).astype(int).reshape(X1_grid.shape)\n    \n    # Classify data points\n    z_data = w1 * X[:, 0] + w2 * X[:, 1] + b\n    output_data = activation(z_data)\n    predictions_data = (output_data > threshold).astype(int)\n    \n    # Compute accuracy\n    accuracy = np.mean(predictions_data == y_true) * 100\n    \n    # Identify misclassified points\n    correct = predictions_data == y_true\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 10), dpi=100)\n    \n    # Plot decision regions\n    ax.contourf(X1_grid, X2_grid, predictions_grid, \n               levels=[-0.5, 0.5, 1.5], colors=['lightblue', 'lightcoral'], alpha=0.3)\n    \n    # Plot decision boundary\n    ax.contour(X1_grid, X2_grid, predictions_grid, \n              levels=[0.5], colors=['green'], linewidths=3, linestyles='solid')\n    \n    # Plot correctly classified points\n    ax.scatter(X[correct & (y_true == 0), 0], X[correct & (y_true == 0), 1],\n              c='blue', s=100, alpha=0.7, edgecolors='black', linewidths=1.5, label='Class 0 (correct)')\n    ax.scatter(X[correct & (y_true == 1), 0], X[correct & (y_true == 1), 1],\n              c='red', s=100, alpha=0.7, edgecolors='black', linewidths=1.5, label='Class 1 (correct)')\n    \n    # Plot misclassified points with thick red borders\n    if not correct.all():\n        ax.scatter(X[~correct, 0], X[~correct, 1],\n                  c='yellow', s=150, alpha=0.9, edgecolors='red', linewidths=4, \n                  label='Misclassified', marker='X')\n    \n    ax.set_xlabel('xâ‚', fontsize=13)\n    ax.set_ylabel('xâ‚‚', fontsize=13)\n    ax.set_title(f'Dataset {dataset_num}: {name} | {activation_name} Activation\\nAccuracy: {accuracy:.1f}%', \n                fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10, loc='upper left')\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect('equal')\n    ax.set_xlim(-4, 4)\n    ax.set_ylim(-4, 4)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Feedback\n    print(f\"\\nDataset {dataset_num}: {name}\")\n    print(\"=\"*70)\n    print(f\"  Weighted sum: z = {w1:.2f}Ã—xâ‚ + {w2:.2f}Ã—xâ‚‚ + {b:.2f}\")\n    print(f\"  Activation: {activation_name}(z)\")\n    print(f\"  Accuracy: {accuracy:.1f}% ({int(accuracy * len(y_true) / 100)}/{len(y_true)} correct)\")\n    \n    if accuracy == 100:\n        print(\"\\nğŸ‰ Perfect classification! Try another dataset to keep practicing!\")\n    elif accuracy >= 95:\n        print(\"\\nğŸ‘ Excellent! Just a tiny adjustment needed for perfection.\")\n    elif accuracy >= 85:\n        print(\"\\nğŸ˜Š Good progress! Fine-tune the parameters to improve the boundary.\")\n    elif accuracy >= 70:\n        print(\"\\nğŸ’¡ Getting there! Adjust weights to change the slope, bias to shift position.\")\n    else:\n        print(\"\\nğŸ’¡ Tip:\")\n        print(\"   â€¢ For Dataset 2 (vertical), try large |wâ‚| and small wâ‚‚\")\n        print(\"   â€¢ For Dataset 3 (horizontal), try large |wâ‚‚| and small wâ‚\")\n        print(\"   â€¢ For diagonals, try similar |wâ‚| and |wâ‚‚| values\")\n\n# Interactive widget\nprint(\"Interactive Perceptron Builder\")\nprint(\"=\"*70)\nprint(\"Select a dataset and adjust parameters to classify it perfectly.\\n\")\n\ninteract(\n    perceptron_classifier,\n    dataset_num=Dropdown(\n        options=[\n            (f'Dataset 1: {datasets[1][\"name\"]}', 1),\n            (f'Dataset 2: {datasets[2][\"name\"]}', 2),\n            (f'Dataset 3: {datasets[3][\"name\"]}', 3),\n            (f'Dataset 4: {datasets[4][\"name\"]}', 4),\n            (f'Dataset 5: {datasets[5][\"name\"]}', 5)\n        ],\n        value=1,\n        description='Dataset:'\n    ),\n    w1=FloatSlider(min=-3, max=3, step=0.1, value=1.0, description='Weight wâ‚:', continuous_update=False),\n    w2=FloatSlider(min=-3, max=3, step=0.1, value=1.0, description='Weight wâ‚‚:', continuous_update=False),\n    b=FloatSlider(min=-5, max=5, step=0.1, value=0.0, description='Bias b:', continuous_update=False),\n    activation_name=Dropdown(\n        options=['Sigmoid', 'Tanh', 'ReLU', 'Step'],\n        value='Sigmoid',\n        description='Activation:'\n    )\n);"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Understanding the Parameters\n\n### Weights (wâ‚, wâ‚‚):\n- Control the **orientation** (slope/angle) of the decision boundary\n- Larger weight â†’ that input dimension matters more\n- If wâ‚ = wâ‚‚, the boundary has a 45Â° angle\n- If wâ‚ â‰« wâ‚‚, the boundary is mostly vertical (xâ‚ matters more)\n- If wâ‚‚ â‰« wâ‚, the boundary is mostly horizontal (xâ‚‚ matters more)\n\n### Bias (b):\n- Controls the **position** of the decision boundary\n- Positive bias â†’ shifts boundary toward negative direction\n- Negative bias â†’ shifts boundary toward positive direction\n- Without bias (b=0), the boundary must pass through the origin\n\n### Activation Function:\n- For these linearly separable problems, **all activations should work equally well!**\n- The activation determines the output range (0-1 for sigmoid, -1 to 1 for tanh, etc.)\n- But the decision boundary location is determined by the weights and bias\n\n### Tips for Each Dataset:\n- **Dataset 1 (Diagonal â†—):** Need positive wâ‚ and wâ‚‚ (similar magnitudes)\n- **Dataset 2 (Vertical):** Need large |wâ‚|, small wâ‚‚\n- **Dataset 3 (Horizontal):** Need large |wâ‚‚|, small wâ‚\n- **Dataset 4 (Diagonal â†–):** Need opposite signs for wâ‚ and wâ‚‚\n- **Dataset 5 (Tight):** Requires precise adjustment of all three parameters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Connection to Neural Networks\n",
    "\n",
    "You just built a **single perceptron** (one artificial neuron)!\n",
    "\n",
    "A **neural network** is just:\n",
    "- **Many perceptrons** arranged in layers\n",
    "- Each perceptron has its own weights and bias\n",
    "- Outputs from one layer become inputs to the next layer\n",
    "\n",
    "### Single Perceptron (what you built):\n",
    "```\n",
    "Input layer     Perceptron                Output\n",
    "    xâ‚  â”€â”€â”€â”€â”\n",
    "            â”œâ”€â”€â†’ [z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + b] â”€â”€â†’ [Ïƒ(z)] â”€â”€â†’ output\n",
    "    xâ‚‚  â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Multi-Layer Neural Network:\n",
    "```\n",
    "Input layer   Hidden layer      Output layer\n",
    "    xâ‚  â”€â”€â”€â”€â”\n",
    "            â”œâ”€â”€â†’ [perceptron 1]â”€â”€â”\n",
    "    xâ‚‚  â”€â”€â”€â”€â”¤                    â”œâ”€â”€â†’ [perceptron 4] â”€â”€â†’ output\n",
    "            â”œâ”€â”€â†’ [perceptron 2]â”€â”€â”¤\n",
    "            â”‚                    â”‚\n",
    "            â””â”€â”€â†’ [perceptron 3]â”€â”€â”˜\n",
    "```\n",
    "\n",
    "More layers and more perceptrons = more complex patterns can be learned!\n",
    "\n",
    "**That's why neural networks are powerful:** They stack many simple perceptrons to create complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Questions for Your Answer Sheet\n\n**Q17.** Write down the two steps of a perceptron in your own words.\n\n**Q18.** What do the weights (wâ‚, wâ‚‚) control about the decision boundary?\n\n**Q19.** What does the bias (b) control about the decision boundary?\n\n**Q20.** Compare Dataset 2 (vertical separation) and Dataset 3 (horizontal separation). For Dataset 2, which weight (wâ‚ or wâ‚‚) needed to be larger to get good classification? For Dataset 3, which weight needed to be larger? Explain why this makes sense.\n\n**Q21.** For these linearly separable datasets, did the choice of activation function (Sigmoid vs Tanh vs ReLU vs Step) significantly change whether you could classify the data correctly? Why or why not?\n\n**Q22.** In Section 6, we saw that neural networks are made of many perceptrons in layers. Why might having multiple layers of perceptrons allow neural networks to solve problems that a single perceptron cannot (like XOR or circles from Module 0)?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. **Answer Q17-Q22** on your answer sheet\n2. **Return to the LMS** and continue to Module 4\n3. In Module 4, you'll test the perceptron on harder problems (XOR and circles) to see its limits!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}