{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Module 0: Setup and The Update Rule\n",
    "\n",
    "**Run this module first!**\n",
    "\n",
    "This module:\n",
    "1. Loads your group's parameters from Lab 1\n",
    "2. Introduces the universal update rule\n",
    "3. Shows how gradient descent automates what you did in Lab 1\n",
    "\n",
    "**Time:** ~5 minutes (Prelab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. From Lab 1 to Lab 2\n",
    "\n",
    "In Lab 1, you **manually searched** for optimal parameters:\n",
    "- Adjusted sliders to find the best line fit\n",
    "- Chose (x, y) values to explore landscapes\n",
    "- Used \"warmer/colder\" feedback to guide your choices\n",
    "\n",
    "**Today:** You'll learn how **gradient descent** automates this process.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> **Gradient descent is the automated version of what you did manually in Lab 1.**\n",
    "\n",
    "Instead of guessing which direction to move, gradient descent:\n",
    "- Computes the **local slope** (gradient)\n",
    "- Moves **downhill** automatically\n",
    "- Uses a simple update rule: `new = old + change`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Group Code\n",
    "\n",
    "Enter the **same group code** you used in Lab 1.\n",
    "\n",
    "This will regenerate the same landscapes so you can compare manual search (Lab 1) vs. gradient descent (Lab 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "group_code = int(input(\"Enter your group code from Lab 1: \"))\n",
    "\n",
    "np.random.seed(group_code)\n",
    "print(f\"Random seed set using group code: {group_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Same Parameters as Lab 1\n",
    "\n",
    "This code regenerates the exact same landscapes you explored in Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all parameters for this group (same as Lab 1)\n",
    "np.random.seed(group_code)\n",
    "\n",
    "# Line fitting parameters\n",
    "true_m = np.random.uniform(-3, 3)\n",
    "true_b = np.random.uniform(-5, 5)\n",
    "\n",
    "# Hidden function parameters (parabola: f(x) = a(x-b)^2 + c)\n",
    "hidden_a = np.random.uniform(0.5, 2.0)\n",
    "hidden_b = np.random.uniform(-4, 4)\n",
    "hidden_c = np.random.uniform(-10, 10)\n",
    "\n",
    "# Mountain landscape parameters\n",
    "num_peaks = np.random.randint(3, 6)\n",
    "\n",
    "# Store all parameters\n",
    "group_data = {\n",
    "    \"group_code\": group_code,\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"parameters\": {\n",
    "        \"line_slope\": float(true_m),\n",
    "        \"line_intercept\": float(true_b),\n",
    "        \"hidden_func_a\": float(hidden_a),\n",
    "        \"hidden_func_b\": float(hidden_b),\n",
    "        \"hidden_func_c\": float(hidden_c),\n",
    "        \"num_mountain_peaks\": int(num_peaks)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "filename = f\"lab2_group_{group_code}_params.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(group_data, f, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Parameters Generated!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Group Code: {group_code}\")\n",
    "print(f\"Parameters file: {filename}\")\n",
    "print()\n",
    "print(\"These are the SAME landscapes you explored in Lab 1.\")\n",
    "print(\"Now you'll see how gradient descent navigates them automatically!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Universal Update Rule\n",
    "\n",
    "Gradient descent uses one simple rule to update parameters:\n",
    "\n",
    "### The Rule:\n",
    "\n",
    "```\n",
    "new_value = old_value + change\n",
    "```\n",
    "\n",
    "Where:\n",
    "```\n",
    "change = -learning_rate × gradient\n",
    "```\n",
    "\n",
    "- **gradient** = local slope (direction of steepest ascent)\n",
    "- **learning_rate** = how big a step to take\n",
    "- **negative sign** = go downhill (opposite of gradient)\n",
    "\n",
    "### Why This Works:\n",
    "\n",
    "- **Gradient** tells us which direction is uphill\n",
    "- We want to go **downhill** (minimize error)\n",
    "- So we move in the **negative gradient** direction\n",
    "- **Learning rate** controls how far we step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Update Rule\n",
    "\n",
    "Let's see how the update rule works on a simple curve.\n",
    "\n",
    "Consider a parabola: `f(x) = (x - 2)²`\n",
    "\n",
    "The minimum is at x = 2. Let's see how gradient descent finds it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple parabola centered at x=2\n",
    "def simple_parabola(x):\n",
    "    return (x - 2)**2\n",
    "\n",
    "# Plot the function\n",
    "x_vals = np.linspace(-1, 5, 300)\n",
    "y_vals = [simple_parabola(x) for x in x_vals]\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = (x-2)²')\n",
    "plt.axvline(x=2, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Minimum at x=2')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Simple Parabola: f(x) = (x-2)²', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The minimum of f(x) = (x-2)² is at x = 2, where f(2) = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Slope and Movement Direction\n",
    "\n",
    "The **gradient** is just the slope at a point.\n",
    "\n",
    "Let's mark three points and see their slopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute numerical gradient (slope) at three points\n",
    "def compute_slope(x, func, h=1e-5):\n",
    "    \"\"\"Compute slope using central difference\"\"\"\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)\n",
    "\n",
    "# Three test points\n",
    "test_points = [0.0, 2.0, 4.0]\n",
    "test_colors = ['red', 'green', 'blue']\n",
    "test_labels = ['Left of minimum (x=0)', 'At minimum (x=2)', 'Right of minimum (x=4)']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, alpha=0.3, label='f(x) = (x-2)²')\n",
    "\n",
    "print(\"Gradient Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, x in enumerate(test_points):\n",
    "    y = simple_parabola(x)\n",
    "    slope = compute_slope(x, simple_parabola)\n",
    "    \n",
    "    # Plot point\n",
    "    plt.scatter(x, y, c=test_colors[i], s=200, edgecolors='black', \n",
    "               linewidths=2, zorder=5, label=test_labels[i])\n",
    "    \n",
    "    # Print analysis\n",
    "    print(f\"\\nPoint: x = {x:.1f}\")\n",
    "    print(f\"  f(x) = {y:.2f}\")\n",
    "    print(f\"  Gradient (slope) = {slope:.2f}\")\n",
    "    \n",
    "    if slope > 0.1:\n",
    "        direction = \"LEFT (downhill)\"\n",
    "        print(f\"  → Slope is POSITIVE → Function increasing → Move {direction}\")\n",
    "    elif slope < -0.1:\n",
    "        direction = \"RIGHT (downhill)\"\n",
    "        print(f\"  → Slope is NEGATIVE → Function decreasing → Move {direction}\")\n",
    "    else:\n",
    "        print(f\"  → Slope is ZERO → At the minimum!\")\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gradient Direction Analysis', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive: Explore Learning Rate\n",
    "\n",
    "Now let's see how **learning rate** affects the step size.\n",
    "\n",
    "Starting from x = 0, we'll take one gradient descent step with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point\n",
    "x_start = 0.0\n",
    "f_start = simple_parabola(x_start)\n",
    "grad_start = compute_slope(x_start, simple_parabola)\n",
    "\n",
    "print(\"Starting Point:\")\n",
    "print(f\"  x = {x_start}\")\n",
    "print(f\"  f(x) = {f_start:.2f}\")\n",
    "print(f\"  gradient = {grad_start:.2f}\")\n",
    "print()\n",
    "\n",
    "# Try different learning rates\n",
    "learning_rates = [0.05, 0.2, 0.5, 1.0]\n",
    "\n",
    "print(\"One Step of Gradient Descent:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Apply update rule: new = old - learning_rate × gradient\n",
    "    change = -lr * grad_start\n",
    "    x_new = x_start + change\n",
    "    f_new = simple_parabola(x_new)\n",
    "    \n",
    "    print(f\"\\nLearning Rate = {lr}:\")\n",
    "    print(f\"  change = -({lr}) × ({grad_start:.2f}) = {change:.2f}\")\n",
    "    print(f\"  x_new = {x_start} + {change:.2f} = {x_new:.2f}\")\n",
    "    print(f\"  f(x_new) = {f_new:.2f}\")\n",
    "    print(f\"  Improvement: {f_start:.2f} → {f_new:.2f} (reduced by {f_start - f_new:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Learning Rate Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the steps\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, alpha=0.3, label='f(x) = (x-2)²')\n",
    "\n",
    "# Mark starting point\n",
    "plt.scatter(x_start, f_start, c='red', s=300, marker='o', \n",
    "           edgecolors='black', linewidths=2, zorder=5, label='Start')\n",
    "\n",
    "# Show each step\n",
    "colors_lr = ['orange', 'yellow', 'lightgreen', 'lightblue']\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    change = -lr * grad_start\n",
    "    x_new = x_start + change\n",
    "    f_new = simple_parabola(x_new)\n",
    "    \n",
    "    plt.scatter(x_new, f_new, c=colors_lr[i], s=200, marker='*',\n",
    "               edgecolors='black', linewidths=1.5, zorder=5, \n",
    "               label=f'LR={lr}: x={x_new:.2f}')\n",
    "    \n",
    "    # Arrow showing movement\n",
    "    plt.annotate('', xy=(x_new, f_new), xytext=(x_start, f_start),\n",
    "                arrowprops=dict(arrowstyle='->', color=colors_lr[i], lw=2))\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Effect of Learning Rate on Step Size', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"  • Larger learning rate → Bigger step\")\n",
    "print(\"  • Smaller learning rate → Smaller step\")\n",
    "print(\"  • All steps move in the SAME direction (toward minimum)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Your Answer Sheet\n",
    "\n",
    "**Q1.** If the gradient (slope) at a point is **positive**, which direction does gradient descent move? Why?\n",
    "\n",
    "**Q2.** What happens to the step size if:\n",
    "- (a) The learning rate is very large (e.g., 10.0)?\n",
    "- (b) The slope (gradient magnitude) is very large?\n",
    "- (c) Both learning rate and slope are small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Answer Q1 and Q2** on your answer sheet\n",
    "2. **Return to the LMS** and continue to Module 1\n",
    "3. **Remember your group code** for the next modules!\n",
    "\n",
    "In the next module, you'll apply gradient descent to the hidden parabola from Lab 1 and see how it automates the search you did manually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
