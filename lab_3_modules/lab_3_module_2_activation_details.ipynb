{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 \u2013 Module 2: Activation Functions in Detail\n",
    "\n",
    "**Time:** ~5 minutes\n",
    "\n",
    "---\n",
    "\n",
    "In Module 1 you saw how activation functions bend space. Now let\u2019s zoom in on **how they behave with different inputs** \u2014 especially very large ones \u2014 and why that matters for a model that needs to *learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Run this cell to load three activation functions: **Sigmoid**, **ReLU**, and **Step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FloatSlider, interact\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def step(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "print('Activation functions loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test All Three on the Same Input\n",
    "\n",
    "Drag the slider to feed the **same number** into Sigmoid, ReLU, and Step.  \n",
    "Pay special attention to what happens at **very large positive values** (like 100) and **very large negative values** (like \u2013100).\n",
    "\n",
    "Questions to think about while exploring:\n",
    "- Which function\u2019s output keeps growing when the input gets bigger?\n",
    "- Which function\u2019s output flattens out no matter how big the input gets?\n",
    "- Which function gives you the least information about the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_activations(input_value):\n",
    "    x = np.array([input_value])\n",
    "    results = {\n",
    "        'Sigmoid': sigmoid(x)[0],\n",
    "        'ReLU':    relu(x)[0],\n",
    "        'Step':    step(x)[0],\n",
    "    }\n",
    "\n",
    "    fig, (ax_graph, ax_bar) = plt.subplots(1, 2, figsize=(14, 5), dpi=100,\n",
    "                                            gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "    # --- Left: function curves with current input marked ---\n",
    "    t = np.linspace(-6, 6, 300)\n",
    "    for func, name, color in [(sigmoid, 'Sigmoid', '#7b2d8e'),\n",
    "                               (relu,    'ReLU',    '#2d8e4e'),\n",
    "                               (step,    'Step',    '#c0392b')]:\n",
    "        ax_graph.plot(t, func(t), color=color, lw=2.5, label=name)\n",
    "    # Marker for current input (clamped to visible range on x-axis)\n",
    "    x_vis = np.clip(input_value, -6, 6)\n",
    "    for func, color in [(sigmoid, '#7b2d8e'), (relu, '#2d8e4e'), (step, '#c0392b')]:\n",
    "        ax_graph.plot(x_vis, func(np.array([input_value]))[0], 'o', color=color,\n",
    "                      markersize=10, markeredgecolor='k', zorder=5)\n",
    "    ax_graph.axhline(0, color='k', lw=0.5); ax_graph.axvline(0, color='k', lw=0.5)\n",
    "    ax_graph.set_xlabel('Input', fontsize=12); ax_graph.set_ylabel('Output', fontsize=12)\n",
    "    ax_graph.set_title(f'Activation curves  (input = {input_value:.1f})', fontsize=13, fontweight='bold')\n",
    "    ax_graph.legend(fontsize=11); ax_graph.grid(True, alpha=0.3)\n",
    "    ax_graph.set_xlim(-6, 6); ax_graph.set_ylim(-0.5, 6.5)\n",
    "\n",
    "    # --- Right: bar chart of outputs ---\n",
    "    names  = list(results.keys())\n",
    "    values = list(results.values())\n",
    "    colors = ['#7b2d8e', '#2d8e4e', '#c0392b']\n",
    "    bars = ax_bar.bar(names, values, color=colors, alpha=0.8, edgecolor='k', lw=1.5)\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax_bar.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.15,\n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    ax_bar.set_ylabel('Output', fontsize=12)\n",
    "    ax_bar.set_title('Output values', fontsize=13, fontweight='bold')\n",
    "    ax_bar.set_ylim(-0.3, max(6, max(values) + 1))\n",
    "    ax_bar.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Contextual hint\n",
    "    if input_value > 5:\n",
    "        print('Notice: Sigmoid is stuck near 1.000 while ReLU keeps climbing.')\n",
    "        print('That \"stuck\" behavior is called SATURATION.')\n",
    "    elif input_value < -5:\n",
    "        print('Notice: Sigmoid is stuck near 0.000, ReLU is exactly 0, Step is exactly 0.')\n",
    "    elif abs(input_value) < 1:\n",
    "        print('Near zero all three functions give noticeably different outputs.')\n",
    "\n",
    "interact(\n",
    "    compare_activations,\n",
    "    input_value=FloatSlider(min=-10, max=10, step=0.5, value=0.0,\n",
    "                            description='Input:', continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saturation \u2014 When Learning Gets Stuck\n",
    "\n",
    "**Saturation** means the output barely changes even as the input keeps growing \u2014 like squeezing a sponge that\u2019s already dry.\n",
    "\n",
    "Look at the Sigmoid curve at the extremes: whether the input is 5 or 500, the output is essentially 1.000. The function has *flattened out*.\n",
    "\n",
    "Why does this matter?\n",
    "\n",
    "When a model is **learning**, it adjusts its numbers a little at a time and checks whether the output improved. If the output barely budges no matter what you change, the model has no signal to follow \u2014 it\u2019s stuck.\n",
    "\n",
    "**ReLU avoids this** for positive inputs: the output keeps growing proportionally, so the model always gets useful feedback about which direction to adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Step Function \u2014 Simple but Rigid\n",
    "\n",
    "Step is the simplest activation: input negative \u2192 **0**, input positive \u2192 **1**. Like a light switch.\n",
    "\n",
    "That sounds appealing \u2014 simple is usually good. But there\u2019s a serious problem:\n",
    "\n",
    "- If the input goes from 0.01 to 0.02, the output stays exactly 1. No change.\n",
    "- If the input goes from \u20130.01 to \u20130.02, the output stays exactly 0. No change.\n",
    "- The *only* place the output changes is at exactly 0, and there it jumps all at once.\n",
    "\n",
    "A learning system needs **gradual feedback** \u2014 small input changes should produce small output changes so the model knows it\u2019s heading in the right direction. Step doesn\u2019t provide that; it\u2019s either 0 or 1 with nothing in between.\n",
    "\n",
    "That\u2019s why modern neural networks use **smooth** activations like Sigmoid or ReLU instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\u2011Sheet Questions (Q7 \u2013 Q9)\n",
    "\n",
    "**Q7.** Test a very large positive input (like 100) on Sigmoid and then on ReLU. What does each one output? Which one keeps changing, and which one flattens out?\n",
    "\n",
    "**Q8.** When a function \u201csaturates,\u201d its output barely changes even as the input keeps growing \u2014 like squeezing a sponge that\u2019s already dry. Why would that be a problem for a model that\u2019s trying to learn and adjust itself?\n",
    "\n",
    "**Q9.** The Step function is the simplest of all \u2014 just on or off, like a light switch. If simple is usually good, why isn\u2019t Step the obvious choice for a learning system? What does it lose by being so rigid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next:** Continue to **Module 3** to build a *perceptron* \u2014 the single building block of every neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}