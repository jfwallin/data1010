{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8, Module 2: Training a Toy Denoiser\n",
    "\n",
    "**Estimated time:** 18-22 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Opening: From Destruction to Reconstruction**\n",
    "\n",
    "In Module 1, you learned the \"easy\" part: **adding noise** to destroy information.\n",
    "\n",
    "Now comes the exciting part: **training a model to reverse this process**!\n",
    "\n",
    "In this module, you'll:\n",
    "- Build a **simplified U-Net** architecture\n",
    "- Train it to **predict the noise** added to MNIST digits\n",
    "- Watch it learn to denoise images in just **2-3 minutes**!\n",
    "- Visualize learned filters (edge detectors emerge automatically)\n",
    "\n",
    "**Key insight:** The model doesn't try to directly predict the clean imageâ€”it predicts the **noise**, then we subtract it!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š **About the Training Task**\n",
    "\n",
    "### **What We're Training:**\n",
    "A **denoising model** that takes two inputs:\n",
    "1. **Noisy image** (x_t at some timestep t)\n",
    "2. **Timestep** (t from 0 to 199)\n",
    "\n",
    "And produces one output:\n",
    "- **Predicted noise** (ÎµÌ‚ that was added)\n",
    "\n",
    "### **Training Process:**\n",
    "```\n",
    "1. Pick random clean image from MNIST\n",
    "2. Pick random timestep t\n",
    "3. Add noise according to forward diffusion formula\n",
    "4. Feed noisy image + timestep to model\n",
    "5. Model predicts the noise\n",
    "6. Compare predicted noise to actual noise (MSE loss)\n",
    "7. Update weights via gradient descent\n",
    "8. Repeat thousands of times!\n",
    "```\n",
    "\n",
    "### **Connection to Lab 7:**\n",
    "Remember training the CNN on MNIST in Lab 7, Module 4?\n",
    "\n",
    "| Aspect | Lab 7 (CNN) | Lab 8 (Denoiser) |\n",
    "|--------|-------------|------------------|\n",
    "| **Task** | Classify digit (0-9) | Predict noise |\n",
    "| **Input** | Clean image | Noisy image + timestep |\n",
    "| **Output** | Class probabilities | Noise image |\n",
    "| **Loss** | Cross-entropy | MSE (mean squared error) |\n",
    "| **Training** | Gradient descent | Same! |\n",
    "| **Architecture** | Encoder (downsample) | U-Net (down + up) |\n",
    "\n",
    "**Key similarity:** Same training processâ€”just different architecture and task!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ **Setup: Import Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "print(f\"\\nðŸ–¥ï¸  GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"   âœ“ Training will use GPU (T4) - expect 2-3 minutes\")\n",
    "else:\n",
    "    print(\"   âš  No GPU detected - training will be slower (~5-10 minutes)\")\n",
    "    print(\"   ðŸ’¡ In Colab: Runtime â†’ Change runtime type â†’ T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¥ **Load and Prepare MNIST Data**\n",
    "\n",
    "We'll use a **subset** of MNIST to keep training fast:\n",
    "- Default: **Digits 0-3** (~24,000 images)\n",
    "- Adjustable: Try 2, 4, 6, or all 10 digit classes!\n",
    "\n",
    "We'll also **downsample to 16Ã—16** to make training faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION: Adjust this to control dataset size!\n",
    "NUM_CLASSES = 4  # Options: 2, 4, 6, or 10 (default: 4 for balance of speed and variety)\n",
    "\n",
    "print(f\"Loading MNIST dataset with {NUM_CLASSES} digit classes (0 to {NUM_CLASSES-1})...\\n\")\n",
    "\n",
    "# Load MNIST\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Filter to selected classes\n",
    "mask = y_train_full < NUM_CLASSES\n",
    "x_train = x_train_full[mask]\n",
    "y_train = y_train_full[mask]\n",
    "\n",
    "print(f\"âœ… Filtered to {len(x_train)} training images\")\n",
    "print(f\"   Classes: {np.unique(y_train)}\")\n",
    "\n",
    "# Downsample to 16x16 using bilinear interpolation\n",
    "x_train_resized = tf.image.resize(x_train[..., np.newaxis], [16, 16]).numpy()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "x_train_resized = x_train_resized.astype('float32') / 255.0\n",
    "\n",
    "print(f\"âœ… Downsampled to 16Ã—16 pixels\")\n",
    "print(f\"   Final shape: {x_train_resized.shape}\")\n",
    "print(f\"   Pixel range: [{x_train_resized.min():.2f}, {x_train_resized.max():.2f}]\")\n",
    "\n",
    "# Display sample from each class\n",
    "fig, axes = plt.subplots(1, NUM_CLASSES, figsize=(NUM_CLASSES*2, 2))\n",
    "if NUM_CLASSES == 1:\n",
    "    axes = [axes]\n",
    "    \n",
    "for i in range(NUM_CLASSES):\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(x_train_resized[idx, :, :, 0], cmap='gray')\n",
    "    axes[i].set_title(f'Digit {i}', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Sample 16Ã—16 Digits (Classes 0-{NUM_CLASSES-1})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Tip: Want to experiment? Change NUM_CLASSES above to 2, 6, or 10!\")\n",
    "print(f\"   - 2 classes: ~12k images, trains in ~1-2 min\")\n",
    "print(f\"   - 4 classes: ~24k images, trains in ~2-3 min (recommended)\")\n",
    "print(f\"   - 6 classes: ~36k images, trains in ~3-4 min\")\n",
    "print(f\"   - 10 classes: ~60k images, trains in ~5-6 min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ **Setup Diffusion Schedule (From Module 1)**\n",
    "\n",
    "We'll use the same noise schedule from Module 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion hyperparameters\n",
    "NUM_TIMESTEPS = 200\n",
    "BETA_START = 0.0001\n",
    "BETA_END = 0.02\n",
    "\n",
    "# Create linear beta schedule\n",
    "betas = np.linspace(BETA_START, BETA_END, NUM_TIMESTEPS).astype('float32')\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = np.cumprod(alphas).astype('float32')\n",
    "\n",
    "# Convert to TensorFlow constants for faster computation\n",
    "alphas_cumprod_tf = tf.constant(alphas_cumprod, dtype=tf.float32)\n",
    "\n",
    "print(\"âœ… Diffusion schedule created!\")\n",
    "print(f\"   Timesteps: {NUM_TIMESTEPS}\")\n",
    "print(f\"   Beta range: [{BETA_START}, {BETA_END}]\")\n",
    "print(f\"   Alpha_bar at t=0: {alphas_cumprod[0]:.4f}\")\n",
    "print(f\"   Alpha_bar at t=100: {alphas_cumprod[99]:.4f}\")\n",
    "print(f\"   Alpha_bar at t=199: {alphas_cumprod[199]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ **Build Simplified U-Net Architecture**\n",
    "\n",
    "### **What is a U-Net?**\n",
    "\n",
    "A U-Net has two parts:\n",
    "1. **Encoder (downsampling):** Extract features, reduce spatial size\n",
    "2. **Decoder (upsampling):** Reconstruct spatial details, increase size\n",
    "3. **Skip connections:** Copy features from encoder to decoder\n",
    "\n",
    "**Why U-Net for diffusion?**\n",
    "- Encoder: Understands what's in the noisy image\n",
    "- Decoder: Reconstructs the noise pattern\n",
    "- Skip connections: Preserve spatial details\n",
    "\n",
    "### **Our Simplified Architecture:**\n",
    "```\n",
    "Input: 16Ã—16 noisy image + timestep embedding\n",
    "    â†“\n",
    "Encoder Block 1: Conv(32) â†’ Pool â†’ 8Ã—8\n",
    "    â†“\n",
    "Encoder Block 2: Conv(64) â†’ Pool â†’ 4Ã—4\n",
    "    â†“\n",
    "Bottleneck: Conv(128) â†’ 4Ã—4\n",
    "    â†“\n",
    "Decoder Block 1: Upsample â†’ Concat(skip) â†’ Conv(64) â†’ 8Ã—8\n",
    "    â†“\n",
    "Decoder Block 2: Upsample â†’ Concat(skip) â†’ Conv(32) â†’ 16Ã—16\n",
    "    â†“\n",
    "Output: Conv(1) â†’ 16Ã—16 predicted noise\n",
    "```\n",
    "\n",
    "### **Timestep Embedding:**\n",
    "We need to tell the model \"what timestep is this?\" because:\n",
    "- At t=0: Almost no noise (predict tiny corrections)\n",
    "- At t=100: 50% noise (predict moderate corrections)\n",
    "- At t=199: Almost all noise (predict large-scale structure)\n",
    "\n",
    "We'll use **sinusoidal embeddings** (like position encodings in Transformers).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim=32):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    \n",
    "    Args:\n",
    "        timesteps: Tensor of timestep indices [batch_size]\n",
    "        embedding_dim: Dimensionality of embeddings\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: Tensor of shape [batch_size, embedding_dim]\n",
    "    \"\"\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = np.log(10000) / (half_dim - 1)\n",
    "    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
    "    emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n",
    "    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
    "    return emb\n",
    "\n",
    "def build_unet_denoiser(image_size=16, channels=1, timestep_dim=32):\n",
    "    \"\"\"\n",
    "    Build a simplified U-Net for noise prediction.\n",
    "    \n",
    "    Args:\n",
    "        image_size: Image size (16x16)\n",
    "        channels: Number of channels (1 for grayscale)\n",
    "        timestep_dim: Dimension of timestep embeddings\n",
    "    \n",
    "    Returns:\n",
    "        model: Keras model that takes (noisy_image, timestep) and outputs predicted noise\n",
    "    \"\"\"\n",
    "    # Inputs\n",
    "    noisy_image_input = layers.Input(shape=(image_size, image_size, channels), name='noisy_image')\n",
    "    timestep_input = layers.Input(shape=(), dtype=tf.int32, name='timestep')\n",
    "    \n",
    "    # Timestep embedding\n",
    "    t_emb = layers.Lambda(lambda t: get_timestep_embedding(t, timestep_dim))(timestep_input)\n",
    "    t_emb = layers.Dense(64, activation='relu')(t_emb)\n",
    "    t_emb = layers.Dense(64, activation='relu')(t_emb)\n",
    "    \n",
    "    # Add timestep to image (broadcast across spatial dimensions)\n",
    "    t_emb_reshaped = layers.Reshape((1, 1, 64))(t_emb)\n",
    "    t_emb_broadcast = layers.Lambda(lambda x: tf.tile(x, [1, image_size, image_size, 1]))(t_emb_reshaped)\n",
    "    \n",
    "    # Concatenate image and timestep\n",
    "    x = layers.Concatenate()([noisy_image_input, t_emb_broadcast])\n",
    "    \n",
    "    # Encoder Block 1: 16x16 â†’ 8x8\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    skip1 = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2)(skip1)\n",
    "    \n",
    "    # Encoder Block 2: 8x8 â†’ 4x4\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    skip2 = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2)(skip2)\n",
    "    \n",
    "    # Bottleneck: 4x4\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Decoder Block 1: 4x4 â†’ 8x8\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Concatenate()([x, skip2])  # Skip connection\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Decoder Block 2: 8x8 â†’ 16x16\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Concatenate()([x, skip1])  # Skip connection\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Output: Predicted noise (same size as input image)\n",
    "    noise_output = layers.Conv2D(channels, 3, padding='same', name='noise_pred')(x)\n",
    "    \n",
    "    # Build model\n",
    "    model = keras.Model(inputs=[noisy_image_input, timestep_input], outputs=noise_output, name='unet_denoiser')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "print(\"Building U-Net denoiser...\\n\")\n",
    "model = build_unet_denoiser(image_size=16, channels=1, timestep_dim=32)\n",
    "\n",
    "# Print summary\n",
    "print(\"ðŸ“‹ Model Architecture:\\n\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nðŸ“Š Total parameters: {total_params:,}\")\n",
    "print(\"\\nðŸ’¡ This is a simplified U-Net!\")\n",
    "print(\"   Real diffusion models (Stable Diffusion) have billions of parameters.\")\n",
    "print(\"   But the core idea is the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ **Create Training Dataset**\n",
    "\n",
    "We need to create training pairs:\n",
    "- Input: (noisy image, timestep)\n",
    "- Target: actual noise that was added\n",
    "\n",
    "We'll create these **on-the-fly** during training using a custom data generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_batch(images, batch_size, alphas_cumprod_tf, num_timesteps):\n",
    "    \"\"\"\n",
    "    Create a training batch by:\n",
    "    1. Sample random images\n",
    "    2. Sample random timesteps\n",
    "    3. Add noise according to forward diffusion\n",
    "    4. Return (noisy_images, timesteps), noise_targets\n",
    "    \"\"\"\n",
    "    # Sample random images\n",
    "    indices = np.random.randint(0, len(images), batch_size)\n",
    "    batch_images = images[indices]\n",
    "    \n",
    "    # Sample random timesteps\n",
    "    timesteps = np.random.randint(0, num_timesteps, batch_size)\n",
    "    \n",
    "    # Get alpha_bar for these timesteps\n",
    "    alpha_bar_t = tf.gather(alphas_cumprod_tf, timesteps)\n",
    "    \n",
    "    # Generate random noise\n",
    "    noise = tf.random.normal(shape=batch_images.shape)\n",
    "    \n",
    "    # Apply forward diffusion\n",
    "    sqrt_alpha_bar_t = tf.sqrt(alpha_bar_t)[:, None, None, None]\n",
    "    sqrt_one_minus_alpha_bar_t = tf.sqrt(1.0 - alpha_bar_t)[:, None, None, None]\n",
    "    \n",
    "    noisy_images = sqrt_alpha_bar_t * batch_images + sqrt_one_minus_alpha_bar_t * noise\n",
    "    \n",
    "    return (noisy_images, timesteps), noise\n",
    "\n",
    "# Test the batch generator\n",
    "test_batch_inputs, test_batch_targets = create_training_batch(\n",
    "    x_train_resized, batch_size=4, \n",
    "    alphas_cumprod_tf=alphas_cumprod_tf,\n",
    "    num_timesteps=NUM_TIMESTEPS\n",
    ")\n",
    "\n",
    "print(\"âœ… Training batch generator created!\")\n",
    "print(f\"\\nExample batch:\")\n",
    "print(f\"  Noisy images shape: {test_batch_inputs[0].shape}\")\n",
    "print(f\"  Timesteps shape: {test_batch_inputs[1].shape}\")\n",
    "print(f\"  Noise targets shape: {test_batch_targets.shape}\")\n",
    "print(f\"  Example timesteps: {test_batch_inputs[1].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ **Question Q10 (Prediction)**\n",
    "\n",
    "### **Q10. Before training, predict: How long will training take with your current NUM_CLASSES setting?**\n",
    "\n",
    "*Consider:*\n",
    "- **30 seconds** (very fast)\n",
    "- **2-3 minutes** (fast)\n",
    "- **10-15 minutes** (moderate)\n",
    "- **30+ minutes** (slow)\n",
    "\n",
    "*With GPU (T4), what do you expect?*\n",
    "\n",
    "**Record your prediction in the Answer Sheet BEFORE continuing!**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ **Compile and Train the Model!**\n",
    "\n",
    "Let's train! We'll use:\n",
    "- **Loss:** MSE (mean squared error) between predicted and actual noise\n",
    "- **Optimizer:** Adam with learning rate 1e-3\n",
    "- **Epochs:** 8 (sufficient for convergence)\n",
    "- **Batch size:** 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled!\")\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(\"  - Optimizer: Adam (lr=1e-3)\")\n",
    "print(\"  - Loss: MSE (mean squared error)\")\n",
    "print(f\"  - Epochs: 8\")\n",
    "print(f\"  - Batch size: 128\")\n",
    "print(f\"  - Training samples: {len(x_train_resized):,}\")\n",
    "print(f\"  - Steps per epoch: {len(x_train_resized) // 128}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with custom batch generation\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 128\n",
    "STEPS_PER_EPOCH = len(x_train_resized) // BATCH_SIZE\n",
    "\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "print(f\"Expected time: ~2-3 minutes with GPU, ~5-10 minutes without\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Track training history\n",
    "history = {'loss': []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # Generate training batch\n",
    "        (noisy_images, timesteps), noise_targets = create_training_batch(\n",
    "            x_train_resized, BATCH_SIZE, alphas_cumprod_tf, NUM_TIMESTEPS\n",
    "        )\n",
    "        \n",
    "        # Train on batch\n",
    "        loss = model.train_on_batch([noisy_images, timesteps], noise_targets)\n",
    "        epoch_losses.append(loss)\n",
    "    \n",
    "    # Record epoch loss\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    history['loss'].append(epoch_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.6f} - Time: {elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"âœ… Training complete in {total_time:.1f} seconds ({total_time/60:.1f} minutes)!\")\n",
    "print(f\"\\nFinal loss: {history['loss'][-1]:.6f}\")\n",
    "print(f\"Target loss: ~0.05 (lower is better)\")\n",
    "\n",
    "if history['loss'][-1] < 0.1:\n",
    "    print(\"\\nâœ“ Good convergence! Model learned to predict noise.\")\n",
    "elif history['loss'][-1] < 0.2:\n",
    "    print(\"\\nâœ“ Reasonable convergence. Model is learning.\")\n",
    "else:\n",
    "    print(\"\\nâš  Loss is high. Model may need more training.\")\n",
    "    print(\"   Try increasing EPOCHS or checking GPU availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š **Visualize Training Progress**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['loss'], marker='o', linewidth=2, markersize=8, color='blue')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss: Learning to Predict Noise', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.05, color='green', linestyle='--', linewidth=1, label='Target loss (~0.05)')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Key observations:\")\n",
    "print(f\"  - Initial loss: {history['loss'][0]:.4f} (random predictions)\")\n",
    "print(f\"  - Final loss: {history['loss'][-1]:.4f} (learned predictions)\")\n",
    "print(f\"  - Improvement: {(1 - history['loss'][-1]/history['loss'][0])*100:.1f}% reduction\")\n",
    "print(\"\\nâœ“ Loss decreased â†’ Model learned to predict noise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ **Questions Q11-Q12 (Observations)**\n",
    "\n",
    "### **Q11. What final loss did you achieve? Is it close to the target of ~0.05?**\n",
    "\n",
    "*Look at the final loss value above. Lower is better!*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q12. How long did training take? Was it faster or slower than your prediction from Q10?**\n",
    "\n",
    "*Look at the total training time. Did GPU help?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª **Test the Denoiser**\n",
    "\n",
    "Let's see if the model can actually remove noise!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a test image\n",
    "test_img = x_train_resized[0:1]  # Shape: (1, 16, 16, 1)\n",
    "\n",
    "# Test at different timesteps\n",
    "test_timesteps = [25, 50, 100, 150]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_timesteps), 5, figsize=(16, len(test_timesteps)*3))\n",
    "\n",
    "for i, t in enumerate(test_timesteps):\n",
    "    # Get alpha_bar for this timestep\n",
    "    alpha_bar_t = alphas_cumprod[t]\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.randn(*test_img.shape)\n",
    "    sqrt_alpha_bar_t = np.sqrt(alpha_bar_t)\n",
    "    sqrt_one_minus_alpha_bar_t = np.sqrt(1.0 - alpha_bar_t)\n",
    "    noisy = sqrt_alpha_bar_t * test_img + sqrt_one_minus_alpha_bar_t * noise\n",
    "    \n",
    "    # Predict noise\n",
    "    timestep_input = np.array([t])\n",
    "    predicted_noise = model.predict([noisy, timestep_input], verbose=0)\n",
    "    \n",
    "    # Denoise\n",
    "    denoised = (noisy - sqrt_one_minus_alpha_bar_t * predicted_noise) / sqrt_alpha_bar_t\n",
    "    \n",
    "    # Plot\n",
    "    axes[i, 0].imshow(test_img[0, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[i, 0].set_title('Original', fontsize=11, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(noisy[0, :, :, 0], cmap='gray')\n",
    "    axes[i, 1].set_title(f'Noisy (t={t})', fontsize=11, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(noise[0, :, :, 0], cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[i, 2].set_title('Actual Noise', fontsize=11, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[i, 3].imshow(predicted_noise[0, :, :, 0], cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[i, 3].set_title('Predicted Noise', fontsize=11, fontweight='bold')\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    axes[i, 4].imshow(np.clip(denoised[0, :, :, 0], 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[i, 4].set_title('Denoised', fontsize=11, fontweight='bold')\n",
    "    axes[i, 4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Denoising Performance at Different Timesteps', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Observations:\")\n",
    "print(\"  - At t=25 (low noise): Model should denoise well\")\n",
    "print(\"  - At t=100 (medium noise): Model should partially denoise\")\n",
    "print(\"  - At t=150+ (high noise): Harder to recover structure\")\n",
    "print(\"\\nâœ“ The model learned to predict noise at different noise levels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ **Questions Q13-Q15 (Analysis)**\n",
    "\n",
    "### **Q13. At t=100, does the denoiser successfully remove noise? How does the denoised image compare to the original?**\n",
    "\n",
    "*Look at the middle row. Is the digit recognizable after denoising?*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q14. Compare denoising performance at t=25 vs t=150. At which timestep does the model perform better? Why?**\n",
    "\n",
    "*Hint: Less noise = easier to denoise. More signal remains at lower timesteps.*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q15. How is this similar to Lab 7, Module 4 where you trained a CNN on MNIST? How is it different?**\n",
    "\n",
    "*Think about: training process, architecture, task, loss function*\n",
    "\n",
    "**Record your answer in the Answer Sheet.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¬ **Visualize Learned Filters**\n",
    "\n",
    "Just like in Lab 7, let's see what the first convolutional layer learned!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first conv layer weights\n",
    "first_conv_layer = None\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        first_conv_layer = layer\n",
    "        break\n",
    "\n",
    "if first_conv_layer is not None:\n",
    "    weights = first_conv_layer.get_weights()[0]  # Shape: (3, 3, input_channels, 32)\n",
    "    num_filters = min(32, weights.shape[-1])\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        filter_img = weights[:, :, 0, i]  # Get first input channel\n",
    "        axes[i].imshow(filter_img, cmap='RdBu')\n",
    "        axes[i].set_title(f'Filter {i+1}', fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Learned Filters: Do They Look Like Edge Detectors?', \n",
    "                 fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Look for:\")\n",
    "    print(\"  - Edge detectors (vertical, horizontal, diagonal)\")\n",
    "    print(\"  - Gradient patterns\")\n",
    "    print(\"  - Similar to Sobel filters from Lab 7, Module 1!\")\n",
    "    print(\"\\nâœ“ The model learned useful features automatically through training!\")\n",
    "else:\n",
    "    print(\"Could not find convolutional layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Module 2 Complete!\n",
    "\n",
    "You now understand:\n",
    "- **U-Net architecture** (encoder-decoder with skip connections)\n",
    "- **Training a denoiser** (predict noise, not the image directly)\n",
    "- **Timestep embeddings** (model needs to know \"how noisy is this?\")\n",
    "- **Fast training** (2-3 minutes on GPU for toy model)\n",
    "- **Learned features** (automatic edge detector emergence)\n",
    "- **Denoising performance** (better at low noise, harder at high noise)\n",
    "\n",
    "**Key insights:**\n",
    "1. **Same training process as Lab 4 and Lab 7** - gradient descent works!\n",
    "2. **Model predicts noise, not images** - indirect approach is easier to learn\n",
    "3. **Timestep matters** - model needs context about noise level\n",
    "4. **U-Net preserves spatial details** - skip connections help\n",
    "\n",
    "**Ready to generate images?**\n",
    "\n",
    "Move on to **Module 3: Multi-Step Denoising**, where you'll use this trained model to generate digits from pure noise!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
